{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfb73cb-cc5e-4a0d-92fc-5e28158b6f2d",
   "metadata": {},
   "source": [
    "# End-to-End Machine Learning CI/CD Pipeline\n",
    "\n",
    "This notebook defines and executes a fully automated end-to-end machine learning CI/CD pipeline using Amazon SageMaker Pipelines. The workflow covers data preprocessing, model training, evaluation, conditional quality gating, and controlled model registration.\n",
    "\n",
    "Raw input data is ingested and validated in a dedicated preprocessing step, where schema consistency is enforced, features are selected, and datasets are split into training, validation, and test sets. A managed XGBoost training job is then launched using the processed data, producing versioned model artifacts. The trained model is evaluated on a held-out test set, and classification performance metrics (with F1 score as the primary metric) are computed and persisted as structured evaluation artifacts.\n",
    "\n",
    "Model promotion is governed by an explicit quality gate: if the evaluated F1 score meets or exceeds a configurable threshold, the model is registered in a SageMaker Model Package Group and prepared for deployment; otherwise, the pipeline fails deterministically. All steps are parameterized to support reproducibility, environment portability, and controlled experimentation.\n",
    "\n",
    "Together, these components form a reproducible, auditable ML lifecycle that integrates model validation and governance directly into the CI/CD process, ensuring that only models meeting defined quality standards are eligible for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c35765-2a75-4022-935f-3466357d986b",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd70404-e6bd-4bca-963b-6225b20cce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"sagemaker.workflow.utilities\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cfb72c-3c48-4204-8a5e-baf625dff212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved AWS Account ID: 697347838118\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get AWS Account ID\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "    print(f\"Successfully retrieved AWS Account ID: {account_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cannot retrieve account information: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5574168-b8a3-41c6-9acd-daa4d44daa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "BASE_BUCKET_NAME = f\"yelp-aai540-group6-{account_id}\"\n",
    "default_bucket = BASE_BUCKET_NAME\n",
    "\n",
    "\n",
    "MODEL_PREFIX = \"models/\"\n",
    "MODEL_DIR = f\"{BASE_BUCKET_NAME}/{MODEL_PREFIX}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94278990-da0c-47bd-9a77-6c4c159091d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = f\"venuesignal-model-group-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad4a7be-cab3-4882-b74b-450c5910cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri = f\"s3://yelp-aai540-group6-{account_id}/feature-store/training-data/alldata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895a484-a574-43a7-b518-0901f4e5bf68",
   "metadata": {},
   "source": [
    "## Define Parameters to Parametrize Pipeline Execution\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "ParameterString - represents a str Python type\n",
    "ParameterInteger - represents an int Python type\n",
    "ParameterFloat - represents a float Python type\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow include:\n",
    "\n",
    "processing_instance_count - The instance count of the processing job.\n",
    "instance_type - The ml.* instance type of the training job.\n",
    "model_approval_status - The approval status to register with the trained model for CI/CD purposes (\"PendingManualApproval\" is the default).\n",
    "input_data - The S3 bucket URI location of the input data.\n",
    "mse_threshold - The Mean Squared Error (MSE) threshold used to verify the accuracy of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e52513-9d06-47ae-b3aa-dad7263ec1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "f1_threshold = ParameterFloat(name=\"F1Threshold\", default_value=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5a75b-ed44-4025-8852-03fc854eb192",
   "metadata": {},
   "source": [
    "## Define Preprocessing step\n",
    "\n",
    "This preprocessing script prepares raw review-level data for downstream XGBoost model training, validation, and testing within a SageMaker Pipeline. Its primary purpose is to enforce a stable schema, sanitize heterogeneous input data, and emit model-ready CSV artifacts in the format required by the XGBoost training container.\n",
    "\n",
    "### Input Handling and Schema Enforcement\n",
    "\n",
    "The script ingests a single CSV file (alldata.csv) containing all splits (train, validation, test, production). Because upstream data sources may inconsistently include headers, the script first detects whether a header row is present and conditionally assigns column names. This prevents schema drift and avoids hard failures caused by misaligned headers or unexpected string tokens in numeric fields.\n",
    "\n",
    "### Data Normalization and Type Coercion\n",
    "\n",
    "To ensure numerical stability, all columns expected to be numeric are explicitly coerced using pd.to_numeric(errors=\"coerce\"). Any invalid or non-parsable values are converted to NaN and subsequently imputed with zeros. This guarantees deterministic behavior during training and avoids runtime type errors in XGBoost.\n",
    "\n",
    "The target label (is_highly_rated) is normalized to a binary integer representation (0/1), ensuring compatibility with binary classification objectives.\n",
    "\n",
    "### Dataset Partitioning\n",
    "\n",
    "The dataset is partitioned into training, validation, test, and production subsets using the split column. Non-model fields (event_time, split) are removed after filtering to prevent data leakage and ensure consistent feature vectors across all splits.\n",
    "\n",
    "### Feature Selection and XGBoost Formatting\n",
    "\n",
    "A curated subset of features is selected for model training, focusing on business attributes, parking-related signals, and engagement metrics. The output format strictly follows XGBoost conventions: the label column appears first, followed by feature columns, with no header row.\n",
    "\n",
    "### Output Artifacts\n",
    "\n",
    "The script writes three CSV artifacts to their respective SageMaker processing directories:\n",
    "\n",
    "train/train.csv\n",
    "\n",
    "validation/validation.csv\n",
    "\n",
    "test/test.csv\n",
    "\n",
    "These artifacts are consumed directly by downstream training and evaluation steps, enabling reproducible and automated CI/CD execution of the machine learning pipeline.\n",
    "\n",
    "### Role in the CI/CD Pipeline\n",
    "\n",
    "This preprocessing step acts as a contract boundary between raw data ingestion and model training. By aggressively validating schema, coercing types, and standardizing outputs, it ensures that downstream steps operate on clean, predictable inputs, reducing pipeline brittleness and improving overall system reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d54152b-734b-4690-a3cf-109cca20f285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "feature_columns_names = [\n",
    "    \"review_id\",\n",
    "    \"business_id\",\n",
    "    \"user_id\",\n",
    "    \"mentions_parking\",\n",
    "    \"parking_positive\",\n",
    "    \"parking_negative\",\n",
    "    \"parking_type_lot\",\n",
    "    \"parking_type_street\",\n",
    "    \"parking_type_garage\",\n",
    "    \"parking_type_valet\",\n",
    "    \"parking_free\",\n",
    "    \"parking_paid\",\n",
    "    \"enhanced_parking_score\",\n",
    "    \"business_stars\",\n",
    "    \"business_review_count\",\n",
    "    \"avg_review_stars\",\n",
    "    \"std_review_stars\",\n",
    "    \"total_reviews\",\n",
    "    \"avg_engagement\",\n",
    "    \"pct_highly_rated\",\n",
    "    \"has_parking_data\",\n",
    "    \"parking_sentiment\",\n",
    "    \"review_stars\",\n",
    "    \"useful\",\n",
    "    \"funny\",\n",
    "    \"cool\",\n",
    "    \"engagement_score\",\n",
    "    \"is_engaged\",\n",
    "    \"review_year\",\n",
    "    \"review_month\",\n",
    "    \"review_quarter\",\n",
    "    \"is_restaurant\",\n",
    "    \"price_range_numeric\",\n",
    "    \"event_time\",\n",
    "    \"split\",\n",
    "]\n",
    "\n",
    "label_column = \"is_highly_rated\"\n",
    "all_columns = feature_columns_names + [label_column]\n",
    "\n",
    "xgb_features = [\n",
    "    # Business features\n",
    "    \"avg_review_stars\",\n",
    "    \"std_review_stars\",\n",
    "    \"business_review_count\",\n",
    "    \"pct_highly_rated\",\n",
    "    # Parking features\n",
    "    \"enhanced_parking_score\",\n",
    "    \"parking_positive\",\n",
    "    \"parking_negative\",\n",
    "    \"parking_sentiment\",\n",
    "    \"has_parking_data\",\n",
    "    # Review engagement\n",
    "    \"avg_engagement\",\n",
    "    # Business attributes\n",
    "    \"is_restaurant\",\n",
    "    \"price_range_numeric\",\n",
    "]\n",
    "\n",
    "# Columns that must be numeric\n",
    "numeric_cols = [\n",
    "    \"mentions_parking\",\n",
    "    \"parking_positive\",\n",
    "    \"parking_negative\",\n",
    "    \"parking_type_lot\",\n",
    "    \"parking_type_street\",\n",
    "    \"parking_type_garage\",\n",
    "    \"parking_type_valet\",\n",
    "    \"parking_free\",\n",
    "    \"parking_paid\",\n",
    "    \"enhanced_parking_score\",\n",
    "    \"business_stars\",\n",
    "    \"business_review_count\",\n",
    "    \"avg_review_stars\",\n",
    "    \"std_review_stars\",\n",
    "    \"total_reviews\",\n",
    "    \"avg_engagement\",\n",
    "    \"pct_highly_rated\",\n",
    "    \"has_parking_data\",\n",
    "    \"parking_sentiment\",\n",
    "    \"review_stars\",\n",
    "    \"useful\",\n",
    "    \"funny\",\n",
    "    \"cool\",\n",
    "    \"engagement_score\",\n",
    "    \"is_engaged\",\n",
    "    \"review_year\",\n",
    "    \"review_month\",\n",
    "    \"review_quarter\",\n",
    "    \"is_restaurant\",\n",
    "    \"price_range_numeric\",\n",
    "    label_column,\n",
    "]\n",
    "\n",
    "\n",
    "def prepare_xgb_data(df: pd.DataFrame, features, target: str = \"is_highly_rated\") -> pd.DataFrame:\n",
    "    \"\"\"Prepare data in XGBoost format: target first, no header.\"\"\"\n",
    "    X = df[features].fillna(0)\n",
    "    y = df[target]\n",
    "    return pd.concat([y, X], axis=1)\n",
    "\n",
    "\n",
    "def _detect_header(csv_path: str, expected_first_col: str = \"review_id\") -> bool:\n",
    "    \"\"\"Heuristic: if first cell equals the expected first column name, treat as header.\"\"\"\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_line = f.readline().strip()\n",
    "\n",
    "    if not first_line:\n",
    "        return False\n",
    "\n",
    "    first_cell = first_line.split(\",\")[0].strip().strip('\"').strip(\"'\")\n",
    "    return first_cell == expected_first_col\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    csv_path = f\"{base_dir}/input/alldata.csv\"\n",
    "\n",
    "    # 1) Read CSV without forcing dtypes at read-time\n",
    "    has_header = _detect_header(csv_path, expected_first_col=feature_columns_names[0])\n",
    "    if has_header:\n",
    "        df = pd.read_csv(csv_path, header=0)\n",
    "        # Ensure we have exactly the columns we expect, in the right order\n",
    "        df = df[all_columns]\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path, header=None, names=all_columns)\n",
    "\n",
    "    # 2) Normalize split + event_time to strings (split is used for filtering).\n",
    "    df[\"split\"] = df[\"split\"].astype(str).str.strip().str.lower()\n",
    "    df[\"event_time\"] = df[\"event_time\"].astype(str)\n",
    "\n",
    "    # 3) Coerce numeric columns safely; invalid tokens become NaN.\n",
    "    for c in numeric_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # 4) Impute missing numeric values (simple, deterministic).\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "    # 5) Enforce label to 0/1 (robust if it was float/str).\n",
    "    df[label_column] = (df[label_column] > 0).astype(np.int64)\n",
    "\n",
    "    # 6) Split datasets and drop non-model fields.\n",
    "    drop_cols = [\"event_time\", \"split\"]\n",
    "    train_df = df[df[\"split\"] == \"train\"].drop(columns=drop_cols)\n",
    "    validation_df = df[df[\"split\"] == \"validation\"].drop(columns=drop_cols)\n",
    "    test_df = df[df[\"split\"] == \"test\"].drop(columns=drop_cols)\n",
    "    production_df = df[df[\"split\"] == \"production\"].drop(columns=drop_cols)  # kept for parity\n",
    "\n",
    "    # 7) Prepare XGBoost input (target first, then features; no header).\n",
    "    train_xgb = prepare_xgb_data(train_df, xgb_features, target=label_column)\n",
    "    val_xgb = prepare_xgb_data(validation_df, xgb_features, target=label_column)\n",
    "    test_xgb = prepare_xgb_data(test_df, xgb_features, target=label_column)\n",
    "\n",
    "    # 8) Write outputs.\n",
    "    os.makedirs(f\"{base_dir}/train\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/validation\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/test\", exist_ok=True)\n",
    "\n",
    "    train_xgb.to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    val_xgb.to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
    "    test_xgb.to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa88e40-7630-46c3-889e-2c75650b2c78",
   "metadata": {},
   "source": [
    "## Processing Infrastructure Configuration\n",
    "\n",
    "This cell defines the processing infrastructure used to execute the preprocessing step within the SageMaker Pipeline. It instantiates an `SKLearnProcessor`, which provisions a managed, ephemeral compute environment for running scikit-learn–compatible processing scripts.\n",
    "\n",
    "### Processor Type and Framework\n",
    "The `SKLearnProcessor` is configured with a specific scikit-learn framework version (`1.2-1`), ensuring that the preprocessing environment is fully reproducible and consistent across pipeline executions. Pinning the framework version prevents unexpected behavior caused by upstream library changes.\n",
    "\n",
    "### Compute Configuration\n",
    "The processor runs on `ml.m5.xlarge` instances, a general-purpose instance type well-suited for data preprocessing workloads such as type coercion, feature selection, and dataset partitioning. The number of instances is parameterized via `processing_instance_count`, allowing the pipeline to scale horizontally without code changes.\n",
    "\n",
    "### Execution Context\n",
    "The processor is associated with:\n",
    "- an IAM role (`role`) that grants access to required AWS resources (e.g., S3),\n",
    "- a SageMaker Pipeline session (`pipeline_session`) that integrates the processing job into the broader CI/CD workflow,\n",
    "- and a descriptive base job name (`sklearn-venue-signal-process`) to support traceability and observability in the SageMaker console.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This processor serves as the execution environment for the preprocessing script that validates schema, cleans raw data, and produces model-ready artifacts. By isolating preprocessing in a managed processing step, the pipeline enforces a clean separation of concerns between data preparation, model training, and evaluation, improving reliability and debuggability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d059de50-b069-4425-bff4-f5bf887c9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-venue-signal-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5bec5-7a65-45fd-8538-5d39e3def08a",
   "metadata": {},
   "source": [
    "## Processing Step Definition\n",
    "\n",
    "This cell defines a SageMaker Pipeline processing step responsible for executing the preprocessing logic and producing model-ready datasets. It wires the previously configured `SKLearnProcessor` to concrete inputs, outputs, and execution code, and registers the step within the pipeline graph.\n",
    "\n",
    "### Input Configuration\n",
    "The processing step consumes raw input data via a `ProcessingInput`, which maps the pipeline parameter `input_data` (typically an S3 URI) to the container path `/opt/ml/processing/input`. This standardized directory structure allows the preprocessing script to reliably locate incoming data regardless of execution context.\n",
    "\n",
    "### Output Artifacts\n",
    "The step declares three `ProcessingOutput` channels:\n",
    "- `train` → `/opt/ml/processing/train`\n",
    "- `validation` → `/opt/ml/processing/validation`\n",
    "- `test` → `/opt/ml/processing/test`\n",
    "\n",
    "Each output corresponds to a dataset split generated by the preprocessing script. These outputs are automatically uploaded to S3 and made available to downstream pipeline steps, such as training and evaluation, without manual data movement.\n",
    "\n",
    "### Execution Logic\n",
    "The preprocessing logic is defined in `code/preprocessing.py`, which is executed inside the managed scikit-learn processing container. The script performs schema validation, type coercion, feature selection, dataset splitting, and formatting required by the XGBoost training step.\n",
    "\n",
    "### Pipeline Integration\n",
    "The `ProcessingStep` named `VenueSignal` encapsulates the full preprocessing operation as a first-class pipeline component. By explicitly defining inputs, outputs, and execution code, this step enables reproducible, traceable, and fully automated data preparation within the CI/CD pipeline.\n",
    "\n",
    "This step serves as the contract boundary between raw data ingestion and model training, ensuring downstream steps receive clean, validated, and consistently formatted data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6267e377-7c12-4f70-a691-d77b70c79176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"VenueSignal\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02c151-5531-44c5-9ff7-a7071206b3bb",
   "metadata": {},
   "source": [
    "## Model Training Configuration\n",
    "\n",
    "This cell defines the model training step within the SageMaker Pipeline using the managed XGBoost algorithm. It configures the training container, hyperparameters, and data inputs, and connects the step to the outputs of the preprocessing stage.\n",
    "\n",
    "### Training Image and Environment\n",
    "The XGBoost training container is retrieved dynamically using `sagemaker.image_uris.retrieve`, specifying:\n",
    "- the XGBoost framework,\n",
    "- version `1.7-1`,\n",
    "- Python 3 runtime,\n",
    "- and the target instance type.\n",
    "\n",
    "This ensures the training environment is fully managed, reproducible, and aligned with SageMaker’s optimized XGBoost implementation.\n",
    "\n",
    "### Estimator Configuration\n",
    "An `Estimator` is instantiated to define the training job:\n",
    "- `instance_type` and `instance_count` control the compute resources used for training,\n",
    "- `output_path` specifies the S3 location where trained model artifacts are stored,\n",
    "- `role` provides permissions to access required AWS resources,\n",
    "- `pipeline_session` integrates the estimator into the SageMaker Pipeline execution context.\n",
    "\n",
    "### Hyperparameter Specification\n",
    "The model is configured for binary classification using the `binary:logistic` objective. Key hyperparameters include:\n",
    "- tree depth, learning rate (`eta`), and subsampling parameters to control model complexity,\n",
    "- `eval_metric='auc'` to evaluate ranking performance during training,\n",
    "- `early_stopping_rounds` to prevent overfitting by halting training when validation performance stops improving.\n",
    "\n",
    "These hyperparameters are explicitly defined to ensure consistent and repeatable training behavior across pipeline runs.\n",
    "\n",
    "### Training Data Inputs\n",
    "The training step consumes two datasets:\n",
    "- a training dataset,\n",
    "- and a validation dataset.\n",
    "\n",
    "Both datasets are provided as CSV files produced by the preprocessing step and referenced directly via their S3 output URIs. This explicit dependency ensures that training always operates on the most recent, validated preprocessing outputs.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This training configuration encapsulates the full model-fitting process as a deterministic pipeline step. By sourcing inputs from the preprocessing stage and emitting versioned model artifacts to S3, it enables automated retraining, evaluation, and downstream deployment as part of a CI/CD workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fbb5c4-e23b-4f65-97c3-07f1630ec9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "xgb_output_path = f\"s3://{MODEL_DIR}xgboost-output\"\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.7-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=xgb_output_path,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    max_depth=6,\n",
    "    eta=0.3,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "train_args = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8164fae-6b20-4bb7-9445-9d39e8ed383b",
   "metadata": {},
   "source": [
    "## Training Step Definition \n",
    "\n",
    "This cell registers the model training operation as a first-class step within the SageMaker Pipeline. It wraps the previously configured XGBoost estimator and its training arguments into a `TrainingStep`, enabling the training job to participate in the pipeline’s dependency graph and execution flow.\n",
    "\n",
    "### Step Construction\n",
    "The `TrainingStep` is instantiated with a descriptive name (`VenueSignalTrain`) and the `step_args` produced by the estimator’s `fit` call. These arguments fully specify the training configuration, including compute resources, hyperparameters, input datasets, and output artifact locations.\n",
    "\n",
    "### Pipeline Integration\n",
    "By defining training as a pipeline step, SageMaker can:\n",
    "- track the lineage between preprocessing outputs and trained model artifacts,\n",
    "- automatically resolve execution order based on data dependencies,\n",
    "- retry or re-execute training deterministically as part of CI/CD workflows.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This step encapsulates the core model-fitting logic and produces versioned model artifacts that can be consumed by downstream evaluation, registration, and deployment steps. Treating training as an explicit pipeline step ensures reproducibility, traceability, and seamless integration with automated model lifecycle management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c805d305-733a-4b6f-bcce-ac98296eb042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"VenueSignalTrain\",\n",
    "    step_args=train_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004f45b-73f8-4e29-9162-6fc8874ccf3c",
   "metadata": {},
   "source": [
    "## Model Evaluation Script\n",
    "\n",
    "This cell writes the evaluation script used by the pipeline to quantify model performance on the held-out test set and produce a machine-readable metrics artifact for downstream quality gating and reporting.\n",
    "\n",
    "### Model Artifact Loading\n",
    "The script reads the trained model from `/opt/ml/processing/model/model.tar.gz`, extracts its contents, and loads the model file (`xgboost-model`). It attempts to load the artifact as an XGBoost `Booster` (the standard format for SageMaker’s built-in XGBoost training output) and falls back to Python pickle loading only if the Booster load fails. This makes the evaluation step robust to different serialization formats.\n",
    "\n",
    "### Test Data Ingestion\n",
    "The script loads `/opt/ml/processing/test/test.csv`, which is expected to be in XGBoost-compatible format: the first column is the ground-truth label (`is_highly_rated`) and the remaining columns are model features. Labels are cast to integer (0/1), and features are converted into an `xgboost.DMatrix` for efficient prediction.\n",
    "\n",
    "### Prediction and Thresholding\n",
    "The model generates predicted probabilities for the positive class (consistent with `binary:logistic`). These probabilities are converted into class predictions using a fixed decision threshold of 0.8. This produces a deterministic mapping from probabilities to labels for metric computation.\n",
    "\n",
    "### Metric Computation\n",
    "The primary metric computed is F1 score, which summarizes the precision–recall tradeoff and is useful for imbalanced binary classification. Precision, recall, and accuracy are also computed as supporting diagnostics. If both classes are present in the test set, ROC-AUC is additionally computed using the predicted probabilities.\n",
    "\n",
    "### Metrics Artifact Output\n",
    "All computed metrics are written to `/opt/ml/processing/evaluation/evaluation.json` in a structured JSON format. This evaluation artifact is designed to be consumed by subsequent pipeline steps (e.g., a Condition step enforcing an F1 threshold) and to support reproducible, auditable model assessment within the CI/CD workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6457bd-b07d-4feb-9bf9-8ff5d339f42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "def load_xgb_model(model_filename: str):\n",
    "    \"\"\"Load XGBoost model (Booster preferred; pickle fallback).\"\"\"\n",
    "    try:\n",
    "        booster = xgboost.Booster()\n",
    "        booster.load_model(model_filename)\n",
    "        return booster\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        with open(model_filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Unable to load model '{model_filename}'. Expected XGBoost Booster or pickle. Error: {e}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Extract model artifact\n",
    "    model_tar_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_tar_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    model_file = \"xgboost-model\"\n",
    "    model = load_xgb_model(model_file)\n",
    "\n",
    "    # Load test data (label in column 0)\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    y_test = df.iloc[:, 0].to_numpy().astype(int)\n",
    "    X_test = df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "    dtest = xgboost.DMatrix(X_test)\n",
    "\n",
    "    # Predict probabilities (binary:logistic)\n",
    "    probs = model.predict(dtest)\n",
    "\n",
    "    # Convert to class predictions\n",
    "    y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"f1\": {\"value\": float(f1)},\n",
    "        \"precision\": {\"value\": float(precision)},\n",
    "        \"recall\": {\"value\": float(recall)},\n",
    "        \"accuracy\": {\"value\": float(accuracy)},\n",
    "    }\n",
    "\n",
    "    # AUC only valid if both classes present\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, probs)\n",
    "            metrics[\"auc\"] = {\"value\": float(auc)}\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    report_dict = {\n",
    "        \"classification_metrics\": metrics\n",
    "    }\n",
    "\n",
    "    # Write evaluation output\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234036c-6493-4f01-b1be-0c0afcb391f5",
   "metadata": {},
   "source": [
    "## Model Evaluation Infrastructure and Execution (ScriptProcessor)\n",
    "\n",
    "This cell defines the infrastructure and execution logic for the model evaluation step within the SageMaker Pipeline. It uses a `ScriptProcessor` to run a custom evaluation script against the trained model and the held-out test dataset, producing standardized performance metrics for downstream decision making.\n",
    "\n",
    "### Processor Configuration\n",
    "A `ScriptProcessor` is instantiated using the same XGBoost container image employed during training. This ensures that the evaluation environment matches the training runtime exactly, eliminating discrepancies caused by library or framework version mismatches.\n",
    "\n",
    "The processor is configured to:\n",
    "- run a Python entrypoint (`python3`),\n",
    "- execute on an `ml.m5.xlarge` instance,\n",
    "- use a single instance for deterministic evaluation,\n",
    "- and operate under the same IAM role and pipeline session as other pipeline steps.\n",
    "\n",
    "### Input Artifacts\n",
    "The evaluation step consumes two inputs:\n",
    "\n",
    "1. **Trained Model Artifact**  \n",
    "   The model produced by the training step is passed in via `step_train.properties.ModelArtifacts.S3ModelArtifacts` and mounted at `/opt/ml/processing/model`. This artifact contains the serialized XGBoost model (`model.tar.gz`) generated during training.\n",
    "\n",
    "2. **Test Dataset**  \n",
    "   The test split generated by the preprocessing step is passed in via the `test` processing output and mounted at `/opt/ml/processing/test`. This ensures evaluation is always performed on data that has been processed with the same logic as training.\n",
    "\n",
    "### Evaluation Execution\n",
    "The evaluation logic is defined in `code/evaluation.py`, which:\n",
    "- extracts and loads the trained model,\n",
    "- generates predictions on the test set,\n",
    "- computes classification metrics (including F1 score as the primary metric),\n",
    "- and writes a structured metrics report to disk.\n",
    "\n",
    "### Output Artifact\n",
    "The evaluation results are written to `/opt/ml/processing/evaluation` and exposed as a named processing output (`evaluation`). This directory contains a machine-readable `evaluation.json` file that can be consumed by downstream pipeline steps, such as a conditional quality gate or model registration logic.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This evaluation step formalizes model assessment as a first-class pipeline component. By explicitly tying evaluation to the outputs of both preprocessing and training, the pipeline ensures that model quality checks are reproducible, traceable, and fully automated as part of the CI/CD workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eed6342-4e49-4f85-802b-ea064c91e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-venue-signal-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluation.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94acf06-60c3-45ec-ad23-f159c77c9903",
   "metadata": {},
   "source": [
    "## Evaluation Step Registration and Metrics Exposure\n",
    "\n",
    "This cell registers the model evaluation operation as a first-class SageMaker Pipeline step and exposes the evaluation metrics as structured pipeline properties that can be referenced by downstream steps.\n",
    "\n",
    "### PropertyFile Definition\n",
    "A `PropertyFile` named `EvaluationReport` is defined to map a structured metrics artifact produced by the evaluation script. It specifies:\n",
    "- the logical name of the property file (`EvaluationReport`),\n",
    "- the processing output channel (`evaluation`),\n",
    "- and the relative path to the metrics file (`evaluation.json`).\n",
    "\n",
    "This configuration allows SageMaker Pipelines to parse the evaluation output and make individual metrics addressable via the pipeline’s property system.\n",
    "\n",
    "### Evaluation Step Construction\n",
    "The `ProcessingStep` named `VenueSignalEval` wraps the previously defined evaluation execution arguments (`eval_args`) and associates them with the declared `PropertyFile`. This formally registers the evaluation logic within the pipeline graph and binds its outputs to pipeline-accessible properties.\n",
    "\n",
    "### Metrics Accessibility\n",
    "By attaching the `PropertyFile` to the evaluation step, metrics such as F1 score, precision, recall, and AUC become accessible through expressions like:\n",
    "`step_eval.properties.PropertyFiles[\"EvaluationReport\"]`\n",
    "\n",
    "These properties can be used directly in conditional logic (e.g., quality gates), model registration decisions, or reporting steps without manual parsing or data movement.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This step serves as the bridge between raw evaluation output and automated decision-making. By converting evaluation results into structured pipeline properties, it enables deterministic, auditable model quality enforcement as part of the CI/CD workflow, such as approving, rejecting, or registering models based on predefined performance thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4a95c9-31e5-4746-a211-f219df276dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"VenueSignalEval\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6077e-d9f1-4b67-9dd7-1ae221fcf2b8",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "This cell defines a SageMaker `Model` object that packages the trained model artifact together with its inference container configuration. It represents a deployable model entity that can be registered, approved, or deployed by downstream pipeline steps.\n",
    "\n",
    "### Model Artifact Binding\n",
    "The `model_data` parameter references the S3 location of the trained model artifacts produced by the training step (`step_train.properties.ModelArtifacts.S3ModelArtifacts`). This creates a direct lineage between the training output and the deployable model definition, ensuring traceability and reproducibility.\n",
    "\n",
    "### Inference Environment\n",
    "The model is associated with a specific container image via `image_uri`. In this case, the same XGBoost image used for training is reused for inference, guaranteeing consistency between training and serving environments and avoiding runtime incompatibilities.\n",
    "\n",
    "### Execution Context\n",
    "The model is instantiated with:\n",
    "- an IAM role (`role`) that grants permissions required for deployment and inference,\n",
    "- and a SageMaker Pipeline session (`pipeline_session`) that integrates the model into the pipeline’s execution context.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This step formalizes the trained artifact as a SageMaker model object that can be consumed by subsequent steps such as model registration, conditional approval, or deployment. By separating model definition from training, the pipeline enables controlled promotion of models based on evaluation results and supports automated model lifecycle management within the CI/CD workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09dbf5f-84d6-4412-b95e-126129046f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220b128-22a2-4e3d-8309-9fddaef82044",
   "metadata": {},
   "source": [
    "## Model Creation Step\n",
    "\n",
    "This cell defines a SageMaker Pipeline `ModelStep` that materializes the trained model as a deployable inference resource. It converts the abstract `Model` definition into a concrete SageMaker model entity that can be used for real-time or batch inference.\n",
    "\n",
    "### Model Instantiation\n",
    "The `ModelStep` invokes the `create` method on the previously defined SageMaker `Model` object. This operation binds together the trained model artifacts, the inference container image, and the target compute configuration into a single deployable model instance.\n",
    "\n",
    "### Inference Compute Configuration\n",
    "The model is configured to run on an `ml.m5.large` instance, which provides a cost-efficient general-purpose environment suitable for low- to moderate-throughput inference workloads. An Elastic Inference accelerator (`ml.eia1.medium`) is attached to improve inference performance while minimizing cost relative to GPU-backed instances.\n",
    "\n",
    "### Pipeline Integration\n",
    "By registering model creation as a pipeline step, SageMaker ensures that model instantiation occurs only after successful completion of upstream steps such as training and evaluation. The step becomes part of the pipeline’s dependency graph, enabling deterministic execution, retry behavior, and lineage tracking.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This step serves as the transition point from model development to deployment readiness. It produces a deployable SageMaker model that can be conditionally approved, registered in a model registry, or attached to downstream deployment steps as part of an automated CI/CD workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15f8b712-53b1-4b23-8fce-097e9ba4caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"VenueSignalCreateModel\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fb72a-64c3-4fd4-9b3a-4267167389f5",
   "metadata": {},
   "source": [
    "## Model Metrics Attachment and Registration\n",
    "\n",
    "This cell defines the model registration logic for the SageMaker Pipeline. It attaches evaluation metrics to the trained model and registers the model in a SageMaker Model Package Group, enabling governance, versioning, and controlled promotion to deployment.\n",
    "\n",
    "### Model Metrics Definition\n",
    "A `ModelMetrics` object is created to associate quantitative evaluation results with the trained model. The metrics are sourced from the `evaluation.json` file produced by the evaluation processing step and stored in S3.\n",
    "\n",
    "The `MetricsSource` explicitly specifies:\n",
    "- the S3 URI of the evaluation artifact,\n",
    "- and the content type (`application/json`).\n",
    "\n",
    "By attaching metrics at registration time, SageMaker captures model performance characteristics as immutable metadata, supporting auditability and model comparison across versions.\n",
    "\n",
    "### Model Registration Configuration\n",
    "The model is registered using the `model.register()` method, which packages the trained model artifacts, inference container, and metadata into a versioned model package. Registration parameters define:\n",
    "- supported input (`content_types`) and output (`response_types`) formats,\n",
    "- compatible instance types for real-time inference and batch transform,\n",
    "- the target Model Package Group for version management,\n",
    "- the initial approval status (`model_approval_status`), which may be automated or manually controlled,\n",
    "- and the associated evaluation metrics.\n",
    "\n",
    "### Pipeline Integration\n",
    "The registration logic is wrapped in a `ModelStep` named `VenueSignalRegisterModel`, making model registration a first-class pipeline step. This ensures that model versions are registered deterministically as part of the pipeline execution and only after successful completion of upstream steps.\n",
    "\n",
    "### Role in the Pipeline\n",
    "This step formalizes the transition from a trained model artifact to a governed, versioned model package. By registering the model with attached performance metrics, the pipeline enables automated model lifecycle management, supports approval workflows, and enforces quality standards as part of a robust CI/CD process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f35087-dba3-4cc8-95ad-f03aa0cd98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "step_register = ModelStep(name=\"VenueSignalRegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c25385-6729-4567-ba00-3541cbdf4016",
   "metadata": {},
   "source": [
    "## Quality Gate Failure Step\n",
    "\n",
    "This cell defines a SageMaker Pipeline `FailStep` that explicitly terminates pipeline execution when the model fails to meet a predefined performance threshold. It is used as part of a conditional quality gate enforcing minimum acceptable model quality.\n",
    "\n",
    "### Failure Condition Messaging\n",
    "The `FailStep` is configured with a human-readable error message constructed using `Join`. This dynamically embeds the configured F1 threshold value into the failure message, producing a clear and informative explanation when the pipeline halts (e.g., indicating that the model’s F1 score fell below the required threshold).\n",
    "\n",
    "### Pipeline Control Flow\n",
    "This step is not executed unconditionally. Instead, it is typically referenced by a `ConditionStep` that evaluates the F1 score extracted from the evaluation report. If the condition fails (i.e., model performance does not meet the threshold), control is routed to this `FailStep`, immediately stopping pipeline execution.\n",
    "\n",
    "### Role in the Pipeline\n",
    "The `FailStep` enforces model quality as a first-class CI/CD concern. By explicitly failing the pipeline when performance requirements are not satisfied, it prevents underperforming models from being registered, approved, or deployed. This ensures that only models meeting defined standards progress through the pipeline, supporting robust governance and automated quality assurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e8a5c5-c4ad-491b-a7c2-5db975b42108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"VenueSignalF1Fail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to F1 <\", f1_threshold]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778c7c1-43b6-4f6a-a9b0-26a7663a1a80",
   "metadata": {},
   "source": [
    "## Model Quality Gate and Conditional Routing\n",
    "\n",
    "This cell defines the conditional logic that enforces a minimum model quality threshold within the SageMaker Pipeline. It evaluates the model’s F1 score produced during the evaluation step and deterministically routes execution based on whether the model meets the required performance standard.\n",
    "\n",
    "### Condition Definition\n",
    "A `ConditionLessThanOrEqualTo` condition is defined to compare:\n",
    "- the configured F1 performance threshold (`f1_threshold`),\n",
    "- against the actual F1 score extracted from the evaluation output.\n",
    "\n",
    "The F1 score is retrieved dynamically using `JsonGet`, which reads the value at\n",
    "`classification_metrics.f1.value` from the `evaluation.json` file exposed via the evaluation step’s `PropertyFile`. This allows the pipeline to reason over evaluation metrics without manual parsing or hard-coded values.\n",
    "\n",
    "### Conditional Execution\n",
    "The `ConditionStep` named `VenueSignalF1Cond` uses this condition to control pipeline flow:\n",
    "- **If the model meets or exceeds the F1 threshold**, execution proceeds to:\n",
    "  - model registration (`step_register`),\n",
    "  - and model creation (`step_create_model`).\n",
    "- **If the model fails to meet the threshold**, execution is routed to a failure step (`step_fail`), which terminates the pipeline with an explicit error message.\n",
    "\n",
    "### Pipeline Governance Role\n",
    "This conditional step serves as the formal quality gate in the CI/CD workflow. By enforcing performance requirements at pipeline runtime, it ensures that only models meeting predefined standards are registered or promoted. This approach enables automated, auditable model governance and prevents underperforming models from entering production systems.\n",
    "\n",
    "### Role in the End-to-End Pipeline\n",
    "The `ConditionStep` ties together evaluation, governance, and deployment readiness. It transforms raw evaluation metrics into actionable control flow, completing the automation loop from data ingestion through model validation and controlled promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1929a0f7-0327-4faf-93ca-280733352757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=f1_threshold,\n",
    "    right=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.f1.value\",\n",
    "    )\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"VenueSignalF1Cond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863302d-9536-436b-bd7a-a9b9252da473",
   "metadata": {},
   "source": [
    "## Pipeline Definition and Orchestration\n",
    "\n",
    "This cell defines the top-level SageMaker Pipeline that orchestrates the end-to-end machine learning workflow. It assembles all previously defined steps, parameters, and dependencies into a single, executable CI/CD pipeline.\n",
    "\n",
    "### Pipeline Configuration\n",
    "The pipeline is instantiated with a descriptive name (`VenueSignalPipeline`) to provide clear identification and traceability across executions. This name is used by SageMaker to version, visualize, and manage pipeline runs.\n",
    "\n",
    "### Parameterization\n",
    "The pipeline declares a set of runtime parameters that control its behavior without requiring code changes:\n",
    "- `processing_instance_count` to scale preprocessing resources,\n",
    "- `instance_type` to control training compute,\n",
    "- `model_approval_status` to manage governance and approval workflows,\n",
    "- `input_data` to specify the source dataset,\n",
    "- `f1_threshold` to enforce a minimum model quality requirement.\n",
    "\n",
    "By externalizing these values as parameters, the pipeline supports flexible configuration across environments (development, staging, production).\n",
    "\n",
    "### Step Composition\n",
    "The pipeline explicitly defines its execution graph via the ordered list of steps:\n",
    "- `step_process`: data preprocessing and feature preparation,\n",
    "- `step_train`: model training using XGBoost,\n",
    "- `step_eval`: model evaluation and metric generation,\n",
    "- `step_cond`: conditional quality gate controlling model registration and creation.\n",
    "\n",
    "Although listed sequentially, SageMaker resolves actual execution order based on data and property dependencies, ensuring deterministic and correct execution.\n",
    "\n",
    "### Role in the CI/CD Workflow\n",
    "This pipeline definition serves as the control plane for the entire ML lifecycle. It enables automated retraining, evaluation, governance, and promotion of models through a single, reproducible workflow. By encapsulating all stages into a SageMaker Pipeline, the system achieves traceability, repeatability, and enforceable quality standards as part of a robust ML CI/CD process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb8f7bd5-1bc5-43f9-afb3-dbe8d90d840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"VenueSignalPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        f1_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be0d2a-5dcf-45e2-86e3-c6707bc377c0",
   "metadata": {},
   "source": [
    "This cell retrieves and displays the compiled JSON definition of the SageMaker Pipeline. The definition represents the fully resolved pipeline graph, including all parameters, steps, dependencies, and conditional logic, as it will be executed by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f24b4a44-8224-4db7-b41b-9d46d5714562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://yelp-aai540-group6-697347838118/feature-store/training-data/alldata.csv'},\n",
       "  {'Name': 'F1Threshold', 'Type': 'Float', 'DefaultValue': 0.5}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'VenueSignal',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::697347838118:role/LabRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-697347838118/VenueSignalPipeline/code/2f369c0878b97e366da40c7ff4f47687cd5873bc2d2c269736073969c1dafaa6/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-697347838118',\n",
       "           'VenueSignalPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'VenueSignal',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-697347838118',\n",
       "           'VenueSignalPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'VenueSignal',\n",
       "           'output',\n",
       "           'validation']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-697347838118',\n",
       "           'VenueSignalPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'VenueSignal',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'VenueSignalTrain',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://yelp-aai540-group6-697347838118/models/xgboost-output'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 30,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'}},\n",
       "    'RoleArn': 'arn:aws:iam::697347838118:role/LabRole',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.VenueSignal.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.VenueSignal.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'validation'}],\n",
       "    'HyperParameters': {'objective': 'binary:logistic',\n",
       "     'num_round': '100',\n",
       "     'max_depth': '6',\n",
       "     'eta': '0.3',\n",
       "     'gamma': '0',\n",
       "     'min_child_weight': '1',\n",
       "     'subsample': '0.8',\n",
       "     'colsample_bytree': '0.8',\n",
       "     'eval_metric': 'auc',\n",
       "     'early_stopping_rounds': '10'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://yelp-aai540-group6-697347838118/models/xgboost-output',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://yelp-aai540-group6-697347838118/models/xgboost-output',\n",
       "     'DisableProfiler': False}}},\n",
       "  {'Name': 'VenueSignalEval',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluation.py']},\n",
       "    'RoleArn': 'arn:aws:iam::697347838118:role/LabRole',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.VenueSignalTrain.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.VenueSignal.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-697347838118/VenueSignalPipeline/code/e7fb719d684faa9ca30a53b4d711bd240cd7022171e5a541d6a670b4aff43561/evaluation.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-697347838118/script-venue-signal-eval-2026-02-21-22-30-49-170/output/evaluation',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'EvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'VenueSignalF1Cond',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo',\n",
       "      'LeftValue': {'Get': 'Parameters.F1Threshold'},\n",
       "      'RightValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.VenueSignalEval.PropertyFiles.EvaluationReport'},\n",
       "        'Path': 'classification_metrics.f1.value'}}}],\n",
       "    'IfSteps': [{'Name': 'VenueSignalRegisterModel-RegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'venuesignal-model-group-',\n",
       "       'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': 's3://sagemaker-us-east-1-697347838118/script-venue-signal-eval-2026-02-21-22-30-49-170/output/evaluation/evaluation.json'}},\n",
       "        'Bias': {},\n",
       "        'Explainability': {}},\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1',\n",
       "          'Environment': {},\n",
       "          'ModelDataUrl': {'Get': 'Steps.VenueSignalTrain.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "        'SupportedContentTypes': ['text/csv'],\n",
       "        'SupportedResponseMIMETypes': ['text/csv'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium',\n",
       "         'ml.m5.xlarge'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.m5.xlarge']},\n",
       "       'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
       "       'SkipModelValidation': 'None'}},\n",
       "     {'Name': 'VenueSignalCreateModel-CreateModel',\n",
       "      'Type': 'Model',\n",
       "      'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::697347838118:role/LabRole',\n",
       "       'PrimaryContainer': {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1',\n",
       "        'Environment': {},\n",
       "        'ModelDataUrl': {'Get': 'Steps.VenueSignalTrain.ModelArtifacts.S3ModelArtifacts'}}}}],\n",
       "    'ElseSteps': [{'Name': 'VenueSignalF1Fail',\n",
       "      'Type': 'Fail',\n",
       "      'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ',\n",
       "         'Values': ['Execution failed due to F1 <',\n",
       "          {'Get': 'Parameters.F1Threshold'}]}}}}]}}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ad874-fc02-4530-9e42-8418006ee104",
   "metadata": {},
   "source": [
    "## Pipeline Registration\n",
    "\n",
    "This cell creates or updates the SageMaker Pipeline definition in AWS using the specified IAM role, ensuring that the latest pipeline configuration is registered and ready for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ec78887-0d67-4f81-899d-eae63bdb5ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:697347838118:pipeline/VenueSignalPipeline',\n",
       " 'ResponseMetadata': {'RequestId': '1e7cdd6a-3bd0-4215-8bf9-c087c71f9de9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1e7cdd6a-3bd0-4215-8bf9-c087c71f9de9',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '109',\n",
       "   'date': 'Sat, 21 Feb 2026 22:31:17 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb920e2-7f8c-4190-b08f-cc4200988fe2",
   "metadata": {},
   "source": [
    "## Pipeline Execution\n",
    "\n",
    "This cell starts a new execution of the SageMaker Pipeline, triggering the end-to-end workflow including preprocessing, training, evaluation, and conditional model registration based on the configured parameters and quality gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03dc09a-f683-42c7-b273-a9a10d1a1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f247f-b4ed-42b3-9f8a-07af39b1b590",
   "metadata": {},
   "source": [
    "## Pipeline Execution Status\n",
    "\n",
    "This cell retrieves and displays metadata about the current pipeline execution, including its status, start time, and associated step details, allowing you to monitor progress and diagnose failures during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d1e68b4-ce5a-488d-bddb-40c7173f508a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:697347838118:pipeline/VenueSignalPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:697347838118:pipeline/VenueSignalPipeline/execution/mibq5rtjzpby',\n",
       " 'PipelineExecutionDisplayName': 'execution-1771711150457',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'CreationTime': datetime.datetime(2026, 2, 21, 21, 59, 10, 382000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2026, 2, 21, 21, 59, 10, 382000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:697347838118:user-profile/d-ohmizsi7kzns/default-1768357685526',\n",
       "  'UserProfileName': 'default-1768357685526',\n",
       "  'DomainId': 'd-ohmizsi7kzns',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::697347838118:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA2EXJLSCTAN3QBZ4ST:SageMaker'}},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:697347838118:user-profile/d-ohmizsi7kzns/default-1768357685526',\n",
       "  'UserProfileName': 'default-1768357685526',\n",
       "  'DomainId': 'd-ohmizsi7kzns',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::697347838118:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROA2EXJLSCTAN3QBZ4ST:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': 'ae75f999-473e-4895-bab1-63235e8bb381',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ae75f999-473e-4895-bab1-63235e8bb381',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1041',\n",
       "   'date': 'Sat, 21 Feb 2026 21:59:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1063e7-4056-4860-a25b-1018f6810a09",
   "metadata": {},
   "source": [
    "## Pipeline Execution Wait\n",
    "\n",
    "This cell blocks execution until the SageMaker Pipeline run has completed, ensuring that all steps finish before proceeding and making it easier to observe final outcomes or handle results synchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4199e4ef-e079-463c-9b2f-02558b520fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef80ec3-f47c-4329-ae73-b22df8330dfb",
   "metadata": {},
   "source": [
    "## Pipeline Step Listing\n",
    "\n",
    "This cell lists all steps executed as part of the current SageMaker Pipeline run, along with their statuses and metadata, allowing you to inspect progress, execution order, and identify any failed or skipped steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d567e7e-6fad-4eea-b15b-f754b579e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'VenueSignalCreateModel-CreateModel',\n",
       "  'StartTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 738000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 49, 327000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:model/pipelines-mibq5rtjzpby-VenueSignalCreateMod-5oCkUMXxse'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'VenueSignalRegisterModel-RegisterModel',\n",
       "  'StartTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 738000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 49, 298000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:model-package/venuesignal-model-group-697347838118/1'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'VenueSignalF1Cond',\n",
       "  'StartTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 262000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 392000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'Condition': {'Outcome': 'True'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'VenueSignalEval',\n",
       "  'StartTime': datetime.datetime(2026, 2, 21, 22, 4, 13, 696000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 46, 722000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:processing-job/pipelines-mibq5rtjzpby-VenueSignalEval-mI4vHSX47N'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'VenueSignalTrain',\n",
       "  'StartTime': datetime.datetime(2026, 2, 21, 22, 1, 45, 228000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2026, 2, 21, 22, 4, 13, 454000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:training-job/pipelines-mibq5rtjzpby-VenueSignalTrain-hOwI3V2EXu'}},\n",
       "  'AttemptCount': 1},\n",
       " {'StepName': 'VenueSignal',\n",
       "  'StartTime': datetime.datetime(2026, 2, 21, 21, 59, 11, 175000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2026, 2, 21, 22, 1, 44, 806000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:processing-job/pipelines-mibq5rtjzpby-VenueSignal-JSxpfk0SA8'}},\n",
       "  'AttemptCount': 1}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785eafd4-1405-41c4-a286-e01273eec512",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    %% Parameters block\n",
    "    P[Parameters\n",
    "    - proc_instance_count\n",
    "    - proc_instance_type\n",
    "    - training_instance_type\n",
    "    - model_approval_status\n",
    "    - input_data\n",
    "    - f1_threshold]\n",
    "\n",
    "    %% Main pipeline\n",
    "    subgraph PIPELINE[Pipeline]\n",
    "        A[Processing Step\\nfeature engineering]\n",
    "        B[Training Step\\nmodel training]\n",
    "        C[Processing Step\\nmodel evaluation]\n",
    "        D{Condition Step\\nmodel F1 threshold}\n",
    "    end\n",
    "\n",
    "    %% If branch\n",
    "    subgraph IFSTEPS[if steps]\n",
    "        E[Create Model Step\\nmodel]\n",
    "        F[Register Model\\nmodel package]\n",
    "    end\n",
    "\n",
    "    %% Else branch\n",
    "    subgraph ELSESTEPS[else steps]\n",
    "        H[Fail Step\\nexecution failed]\n",
    "    end\n",
    "\n",
    "    %% Flow connections\n",
    "    P --> A\n",
    "    A --> B --> C --> D\n",
    "\n",
    "    D -->|meets threshold| E\n",
    "    E --> F\n",
    "\n",
    "    D -->|fails threshold| H\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37733052-4398-47aa-8d85-a45038e0acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: VenueSignal\n",
      "  Status:   Succeeded\n",
      "  Duration: 153.6 seconds\n",
      "  ProcessingJob ARN: arn:aws:sagemaker:us-east-1:697347838118:processing-job/pipelines-mibq5rtjzpby-VenueSignal-JSxpfk0SA8\n",
      "----------------------------------------------------------------------\n",
      "Step: VenueSignalTrain\n",
      "  Status:   Succeeded\n",
      "  Duration: 148.2 seconds\n",
      "  TrainingJob ARN:   arn:aws:sagemaker:us-east-1:697347838118:training-job/pipelines-mibq5rtjzpby-VenueSignalTrain-hOwI3V2EXu\n",
      "----------------------------------------------------------------------\n",
      "Step: VenueSignalEval\n",
      "  Status:   Succeeded\n",
      "  Duration: 153.0 seconds\n",
      "  ProcessingJob ARN: arn:aws:sagemaker:us-east-1:697347838118:processing-job/pipelines-mibq5rtjzpby-VenueSignalEval-mI4vHSX47N\n",
      "----------------------------------------------------------------------\n",
      "Step: VenueSignalF1Cond\n",
      "  Status:   Succeeded\n",
      "  Duration: 0.1 seconds\n",
      "  Condition Outcome:True\n",
      "----------------------------------------------------------------------\n",
      "Step: VenueSignalCreateModel-CreateModel\n",
      "  Status:   Succeeded\n",
      "  Duration: 1.6 seconds\n",
      "  Model ARN:         arn:aws:sagemaker:us-east-1:697347838118:model/pipelines-mibq5rtjzpby-VenueSignalCreateMod-5oCkUMXxse\n",
      "----------------------------------------------------------------------\n",
      "Step: VenueSignalRegisterModel-RegisterModel\n",
      "  Status:   Succeeded\n",
      "  Duration: 1.6 seconds\n",
      "  ModelPackage ARN:  arn:aws:sagemaker:us-east-1:697347838118:model-package/venuesignal-model-group-697347838118/1\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datetime import timezone\n",
    "\n",
    "steps = execution.list_steps()\n",
    "\n",
    "# Sort by start time\n",
    "steps = sorted(steps, key=lambda x: x[\"StartTime\"])\n",
    "\n",
    "for step in steps:\n",
    "    name = step[\"StepName\"]\n",
    "    status = step[\"StepStatus\"]\n",
    "    start = step[\"StartTime\"]\n",
    "    end = step.get(\"EndTime\")\n",
    "\n",
    "    duration = None\n",
    "    if end:\n",
    "        duration = (end - start).total_seconds()\n",
    "\n",
    "    print(f\"Step: {name}\")\n",
    "    print(f\"  Status:   {status}\")\n",
    "    if duration is not None:\n",
    "        print(f\"  Duration: {duration:.1f} seconds\")\n",
    "\n",
    "    # Print useful metadata if present\n",
    "    metadata = step.get(\"Metadata\", {})\n",
    "    if \"ProcessingJob\" in metadata:\n",
    "        print(f\"  ProcessingJob ARN: {metadata['ProcessingJob']['Arn']}\")\n",
    "    if \"TrainingJob\" in metadata:\n",
    "        print(f\"  TrainingJob ARN:   {metadata['TrainingJob']['Arn']}\")\n",
    "    if \"RegisterModel\" in metadata:\n",
    "        print(f\"  ModelPackage ARN:  {metadata['RegisterModel']['Arn']}\")\n",
    "    if \"Model\" in metadata:\n",
    "        print(f\"  Model ARN:         {metadata['Model']['Arn']}\")\n",
    "    if \"Condition\" in metadata:\n",
    "        print(f\"  Condition Outcome:{metadata['Condition']['Outcome']}\")\n",
    "\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510badd-51cb-4654-9227-64fe215ab584",
   "metadata": {},
   "source": [
    "## Examining the Evaluation\n",
    "Examine the resulting model evaluation after the pipeline completes. Download the resulting evaluation.json file from S3 and print the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fc191b7-b4b5-4a4f-8591-536ac98e2316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification_metrics': {'accuracy': {'value': 0.7515157539012026},\n",
      "                            'auc': {'value': 0.7507763739583867},\n",
      "                            'f1': {'value': 0.8396613648024629},\n",
      "                            'precision': {'value': 0.770752384316496},\n",
      "                            'recall': {'value': 0.9221017044654176}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(\n",
    "        step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    )\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ca2a4-c58d-4caa-ad0f-58e20266762d",
   "metadata": {},
   "source": [
    "## Lineage\n",
    "Review the lineage of the artifacts generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "767ad862-e26b-4b94-8066-e960aff0ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'VenueSignal', 'StartTime': datetime.datetime(2026, 2, 21, 21, 59, 11, 175000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2026, 2, 21, 22, 1, 44, 806000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:processing-job/pipelines-mibq5rtjzpby-VenueSignal-JSxpfk0SA8'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...aa087173b0f29d3e60255d2/preprocessing.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://.../feature-store/training-data/alldata.csv</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...ine/mibq5rtjzpby/VenueSignal/output/test</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...bq5rtjzpby/VenueSignal/output/validation</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s3://...ne/mibq5rtjzpby/VenueSignal/output/train</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...aa087173b0f29d3e60255d2/preprocessing.py     Input  DataSet   \n",
       "1  s3://.../feature-store/training-data/alldata.csv     Input  DataSet   \n",
       "2  68331...com/sagemaker-scikit-learn:1.2-1-cpu-py3     Input    Image   \n",
       "3  s3://...ine/mibq5rtjzpby/VenueSignal/output/test    Output  DataSet   \n",
       "4  s3://...bq5rtjzpby/VenueSignal/output/validation    Output  DataSet   \n",
       "5  s3://...ne/mibq5rtjzpby/VenueSignal/output/train    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  \n",
       "4         Produced     artifact  \n",
       "5         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'VenueSignalTrain', 'StartTime': datetime.datetime(2026, 2, 21, 22, 1, 45, 228000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2026, 2, 21, 22, 4, 13, 454000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:training-job/pipelines-mibq5rtjzpby-VenueSignalTrain-hOwI3V2EXu'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...bq5rtjzpby/VenueSignal/output/validation</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...ne/mibq5rtjzpby/VenueSignal/output/train</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68331...-1.amazonaws.com/sagemaker-xgboost:1.7-1</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...gnalTrain-hOwI3V2EXu/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...bq5rtjzpby/VenueSignal/output/validation     Input  DataSet   \n",
       "1  s3://...ne/mibq5rtjzpby/VenueSignal/output/train     Input  DataSet   \n",
       "2  68331...-1.amazonaws.com/sagemaker-xgboost:1.7-1     Input    Image   \n",
       "3  s3://...gnalTrain-hOwI3V2EXu/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'VenueSignalEval', 'StartTime': datetime.datetime(2026, 2, 21, 22, 4, 13, 696000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 46, 722000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:processing-job/pipelines-mibq5rtjzpby-VenueSignalEval-mI4vHSX47N'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...2171e5a541d6a670b4aff43561/evaluation.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...ine/mibq5rtjzpby/VenueSignal/output/test</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...gnalTrain-hOwI3V2EXu/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68331...-1.amazonaws.com/sagemaker-xgboost:1.7-1</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...026-02-21-21-59-08-972/output/evaluation</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...2171e5a541d6a670b4aff43561/evaluation.py     Input  DataSet   \n",
       "1  s3://...ine/mibq5rtjzpby/VenueSignal/output/test     Input  DataSet   \n",
       "2  s3://...gnalTrain-hOwI3V2EXu/output/model.tar.gz     Input    Model   \n",
       "3  68331...-1.amazonaws.com/sagemaker-xgboost:1.7-1     Input    Image   \n",
       "4  s3://...026-02-21-21-59-08-972/output/evaluation    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'VenueSignalF1Cond', 'StartTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 262000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 392000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Condition': {'Outcome': 'True'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'VenueSignalRegisterModel-RegisterModel', 'StartTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 738000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 49, 298000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:model-package/venuesignal-model-group-697347838118/1'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...gnalTrain-hOwI3V2EXu/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68331...-1.amazonaws.com/sagemaker-xgboost:1.7-1</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>venuesignal-model-group-697347838118-1-1771711...</td>\n",
       "      <td>Input</td>\n",
       "      <td>ModelLifeCycle</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>venuesignal-model-group-697347838118-1-Pending...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>venuesignal-model-group-697347838118-177171160...</td>\n",
       "      <td>Output</td>\n",
       "      <td>ModelGroup</td>\n",
       "      <td>AssociatedWith</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name/Source Direction  \\\n",
       "0   s3://...gnalTrain-hOwI3V2EXu/output/model.tar.gz     Input   \n",
       "1   68331...-1.amazonaws.com/sagemaker-xgboost:1.7-1     Input   \n",
       "2  venuesignal-model-group-697347838118-1-1771711...     Input   \n",
       "3  venuesignal-model-group-697347838118-1-Pending...     Input   \n",
       "4  venuesignal-model-group-697347838118-177171160...    Output   \n",
       "\n",
       "             Type Association Type Lineage Type  \n",
       "0           Model    ContributedTo     artifact  \n",
       "1           Image    ContributedTo     artifact  \n",
       "2  ModelLifeCycle    ContributedTo       action  \n",
       "3        Approval    ContributedTo       action  \n",
       "4      ModelGroup   AssociatedWith      context  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'VenueSignalCreateModel-CreateModel', 'StartTime': datetime.datetime(2026, 2, 21, 22, 6, 47, 738000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2026, 2, 21, 22, 6, 49, 327000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:697347838118:model/pipelines-mibq5rtjzpby-VenueSignalCreateMod-5oCkUMXxse'}}, 'AttemptCount': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
