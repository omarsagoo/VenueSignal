{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# VenueSignal - Yelp Business Rating Prediction\n",
        "### AAI-540 Group 6\n",
        "\n",
        "---\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This notebook implements a complete end-to-end MLOps pipeline for predicting Yelp business ratings with a focus on parking availability constraints. The pipeline demonstrates MLOps best practices including:\n",
        "\n",
        "- **Data Lake Management**: S3-based data storage with proper versioning\n",
        "- **Data Cataloging**: Athena tables for queryable data access\n",
        "- **Feature Engineering**: Scalable feature store implementation\n",
        "- **Model Development**: Baseline and advanced models with proper evaluation\n",
        "- **Model Deployment**: SageMaker endpoints for inference\n",
        "- **Monitoring**: Comprehensive model, data, and infrastructure monitoring\n",
        "\n",
        "**Key Feature**: Uses AWS Account ID for bucket naming to enable each team member to run independently in their own AWS Learning Lab environment.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup & Configuration](#section-1)\n",
        "2. [Data Lake Setup](#section-2)\n",
        "3. [Athena Tables & Data Cataloging](#section-3)\n",
        "4. [Exploratory Data Analysis](#section-4)\n",
        "5. [Feature Engineering & Feature Store](#section-5)\n",
        "6. [Model Training](#section-6)\n",
        "   - 6.1 Benchmark Models\n",
        "   - 6.2 XGBoost Model\n",
        "   - 6.3 Model Comparison\n",
        "7. [Model Deployment](#section-7)\n",
        "8. [Monitoring & Observability](#section-8)\n",
        "9. [CI/CD](#section-9)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "## 1. Setup & Configuration <a id='section-1'></a>\n",
        "\n",
        "This section:\n",
        "- Verifies Python version\n",
        "- Imports all required libraries\n",
        "- Retrieves AWS Account ID for unique resource naming\n",
        "- Initializes AWS clients and SageMaker session\n",
        "- Configures S3 buckets using Account ID pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "python-version",
      "metadata": {
        "id": "python-version"
      },
      "outputs": [],
      "source": [
        "# Verify Python version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-header",
      "metadata": {
        "id": "imports-header"
      },
      "source": [
        "### 1.1 Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import gdown\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "# AWS SDK\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "\n",
        "# SageMaker\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.feature_store.feature_group import FeatureGroup\n",
        "from sagemaker.inputs import TrainingInput\n",
        "from sagemaker.serializers import CSVSerializer\n",
        "from sagemaker.deserializers import JSONDeserializer\n",
        "from sagemaker.model_monitor import DefaultModelMonitor\n",
        "sm_client = boto3.client('sagemaker')\n",
        "session = sagemaker.Session()\n",
        "role = get_execution_role()\n",
        "region = session.boto_region_name\n",
        "sagemaker_session=session\n",
        "\n",
        "# Athena\n",
        "from pyathena import connect\n",
        "from pyathena.pandas.cursor import PandasCursor\n",
        "\n",
        "# Model training and evaluation\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Monitoring\n",
        "from sagemaker.model_monitor import (\n",
        "    DataCaptureConfig, DefaultModelMonitor, ModelQualityMonitor,\n",
        "    CronExpressionGenerator, EndpointInput\n",
        ")\n",
        "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
        "from sagemaker.s3 import S3Downloader, S3Uploader\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from time import sleep\n",
        "from threading import Thread\n",
        "import io, csv\n",
        "\n",
        "# Google Drive download\n",
        "import gdown\n",
        "\n",
        "print(\"All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "account-id-header",
      "metadata": {
        "id": "account-id-header"
      },
      "source": [
        "### 1.2 Retrieve AWS Account ID\n",
        "\n",
        "**IMPORTANT**: This retrieves your unique AWS Account ID which will be used to create unique S3 bucket names.\n",
        "This allows each team member to run this notebook independently in their own AWS Learning Lab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "account-id",
      "metadata": {
        "id": "account-id"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Get AWS Account ID\n",
        "    account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
        "    print(f\"Successfully retrieved AWS Account ID: {account_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Cannot retrieve account information: {e}\")\n",
        "    raise\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aws-config-header",
      "metadata": {
        "id": "aws-config-header"
      },
      "source": [
        "### 1.3 Initialize AWS Clients and SageMaker Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aws-config",
      "metadata": {
        "id": "aws-config"
      },
      "outputs": [],
      "source": [
        "# Initialize SageMaker session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Get Execution role and AWS Region\n",
        "role = get_execution_role()\n",
        "print(\"RoleArn:\", role)\n",
        "REGION = sagemaker_session.boto_region_name\n",
        "print(\"Region:\", REGION)\n",
        "\n",
        "\n",
        "# Initialize AWS clients\n",
        "s3_client = boto3.client(\"s3\", region_name=REGION)\n",
        "s3_resource = boto3.resource(\"s3\", region_name=REGION)\n",
        "athena_client = boto3.client(\"athena\", region_name=REGION)\n",
        "sagemaker_client = boto3.client(\"sagemaker\", region_name=REGION)\n",
        "cloudwatch_client = boto3.client(\"cloudwatch\", region_name=REGION)\n",
        "logs_client = boto3.client(\"logs\", region_name=REGION)\n",
        "\n",
        "print(f\"AWS Region: {REGION}\")\n",
        "print(f\"SageMaker Execution Role: {role}\")\n",
        "print(f\"AWS clients initialized successfully\")\n",
        "\n",
        "# Also create cw_client alias for consistency\n",
        "cw_client = cloudwatch_client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9d4527-af5d-4a05-af38-13da4eccedcb",
      "metadata": {
        "id": "2b9d4527-af5d-4a05-af38-13da4eccedcb"
      },
      "outputs": [],
      "source": [
        "project_name = \"yelp-aai540-group6\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s3-config-header",
      "metadata": {
        "id": "s3-config-header"
      },
      "source": [
        "### 1.4 Configure S3 Buckets with Account ID Pattern\n",
        "\n",
        "**IMPORTANT**: All S3 buckets are created with your Account ID to ensure uniqueness.\n",
        "This pattern is used throughout the entire pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s3-config",
      "metadata": {
        "id": "s3-config"
      },
      "outputs": [],
      "source": [
        "# Base bucket name with Account ID\n",
        "BASE_BUCKET_NAME = f\"yelp-aai540-group6-{account_id}\"\n",
        "\n",
        "# S3 Prefixes (paths within buckets)\n",
        "RAW_DATA_PREFIX = \"yelp-dataset/json/\"\n",
        "PARQUET_PREFIX = \"yelp-dataset/parquet/\"\n",
        "ATHENA_RESULTS_PREFIX = \"athena/results/\"\n",
        "FEATURE_PREFIX = \"feature-store/\"\n",
        "MODEL_PREFIX = \"models/\"\n",
        "MONITORING_PREFIX = \"monitoring/\"\n",
        "\n",
        "# Individual directories within the base bucket\n",
        "DATA_JSON_DIR = f\"{BASE_BUCKET_NAME}/{RAW_DATA_PREFIX}\"  # Raw data storage\n",
        "DATA_PARQUET_DIR = f\"{BASE_BUCKET_NAME}/{PARQUET_PREFIX}\"  # Raw data storage\n",
        "ATHENA_DIR = f\"{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\"  # Athena queries and results\n",
        "FEATURE_DIR = f\"{BASE_BUCKET_NAME}/{FEATURE_PREFIX}\"  # Feature store offline\n",
        "MODEL_DIR = f\"{BASE_BUCKET_NAME}/{MODEL_PREFIX}\"  # Model artifacts\n",
        "MONITORING_DIR = f\"{BASE_BUCKET_NAME}/{MONITORING_PREFIX}\"  # Monitoring data\n",
        "\n",
        "# Full S3 paths\n",
        "ATHENA_RESULTS_S3 = f\"s3://{ATHENA_DIR}\"\n",
        "\n",
        "# Athena Database\n",
        "ATHENA_DB = \"yelp\"\n",
        "\n",
        "# Store configuration\n",
        "%store REGION\n",
        "%store role\n",
        "%store account_id\n",
        "%store BASE_BUCKET_NAME\n",
        "%store DATA_JSON_DIR\n",
        "%store DATA_PARQUET_DIR\n",
        "%store ATHENA_DIR\n",
        "%store FEATURE_DIR\n",
        "%store MODEL_DIR\n",
        "%store MONITORING_DIR\n",
        "%store ATHENA_RESULTS_S3\n",
        "%store ATHENA_DB\n",
        "\n",
        "# Display configuration\n",
        "print(\"=\"*80)\n",
        "print(\"S3 BUCKET CONFIGURATION (Account-Specific)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"AWS Account ID:     {account_id}\")\n",
        "print(f\"AWS Region:         {REGION}\")\n",
        "print(f\"AWS Role:         {role}\")\n",
        "print()\n",
        "print(\"S3 Bucket:\")\n",
        "print(f\"  Base Bucket:      {BASE_BUCKET_NAME}\")\n",
        "print(\"S3 Bucket Directories:\")\n",
        "print(f\"  JSON:       {DATA_JSON_DIR}\")\n",
        "print(f\"  Parquet:    {DATA_PARQUET_DIR}\")\n",
        "print(f\"  Athena:     {ATHENA_DIR}\")\n",
        "print(f\"  Feature:    {FEATURE_DIR}\")\n",
        "print(f\"  Model:      {MODEL_DIR}\")\n",
        "print(f\"  Monitoring: {MONITORING_DIR}\")\n",
        "print()\n",
        "print(\"Athena Configuration:\")\n",
        "print(f\"  Database:         {ATHENA_DB}\")\n",
        "print(f\"  Results Location: {ATHENA_RESULTS_S3}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-buckets-header",
      "metadata": {
        "id": "create-buckets-header"
      },
      "source": [
        "### 1.5 Create S3 Buckets\n",
        "\n",
        "This creates all required S3 buckets for the pipeline. Each bucket is unique to your AWS account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-buckets",
      "metadata": {
        "id": "create-buckets"
      },
      "outputs": [],
      "source": [
        "def create_bucket_if_not_exists(bucket_name, region=REGION):\n",
        "    \"\"\"\n",
        "    Create an S3 bucket if it doesn't already exist.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: Name of the bucket to create\n",
        "        region: AWS region for the bucket\n",
        "\n",
        "    Returns:\n",
        "        True if bucket was created or already exists, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if bucket exists\n",
        "        s3_client.head_bucket(Bucket=bucket_name)\n",
        "        print(f\"  Bucket already exists: {bucket_name}\")\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response['Error']['Code']\n",
        "        if error_code == '404':\n",
        "            # Bucket doesn't exist, create it\n",
        "            try:\n",
        "                if region == 'us-east-1':\n",
        "                    s3_client.create_bucket(Bucket=bucket_name)\n",
        "                else:\n",
        "                    s3_client.create_bucket(\n",
        "                        Bucket=bucket_name,\n",
        "                        CreateBucketConfiguration={'LocationConstraint': region}\n",
        "                    )\n",
        "                print(f\"  Created bucket: {bucket_name}\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"  Error creating bucket {bucket_name}: {create_error}\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"  Error checking bucket {bucket_name}: {e}\")\n",
        "            return False\n",
        "\n",
        "# Create all required buckets\n",
        "print(\"Creating S3 bucket...\")\n",
        "\n",
        "success = True\n",
        "if not create_bucket_if_not_exists(BASE_BUCKET_NAME):\n",
        "    success = False\n",
        "\n",
        "if success:\n",
        "    print(\"\\n S3 bucket is ready!\")\n",
        "else:\n",
        "    print(\"\\n Bucket could not be created. Please check errors above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Data Lake Setup <a id='section-2'></a>\n",
        "\n",
        "This section:\n",
        "- Downloads Yelp academic dataset from Google Drive\n",
        "- Uploads raw JSON files to S3 data lake\n",
        "- Organizes data in a structured format\n",
        "\n",
        "**Data Source**: Yelp Academic Dataset (5 files, ~8.5 GB total)\n",
        "- Business data (150k+ businesses)\n",
        "- Review data (7M+ reviews)\n",
        "- User data (2M+ users)\n",
        "- Check-in data\n",
        "- Tip data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gdrive-files-header",
      "metadata": {
        "id": "gdrive-files-header"
      },
      "source": [
        "### 2.1 Define Google Drive File IDs\n",
        "\n",
        "These are the file IDs for the Yelp dataset files stored in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gdrive-files",
      "metadata": {
        "id": "gdrive-files"
      },
      "outputs": [],
      "source": [
        "# Google Drive file IDs for Yelp dataset\n",
        "google_drive_file_ids = {\n",
        "    \"yelp_academic_dataset_business.json\": \"1-VQQyXape4lCTa_5bA9VTlJgqKkqqR3h\",\n",
        "    \"yelp_academic_dataset_checkin.json\": \"1LcnPYD4m3jp4l7EF8s8mqp3F8QUlcr9-\",\n",
        "    \"yelp_academic_dataset_review.json\": \"1Q_qpG918HVo4UpT1qyGCH4c_MfgQVgWg\",\n",
        "    \"yelp_academic_dataset_tip.json\": \"1vyYognzSAMenVakNyXgchwfZlVc76ZMk\",\n",
        "    \"yelp_academic_dataset_user.json\": \"1wYnBYQspgfh9PnVS3GTyjT8la5PXJni9\"\n",
        "}\n",
        "#https://drive.google.com/file/d/1Q_qpG918HVo4UpT1qyGCH4c_MfgQVgWg/view?usp=drive_link\n",
        "#https://drive.google.com/file/d/1wYnBYQspgfh9PnVS3GTyjT8la5PXJni9/view?usp=sharing\n",
        "#https://drive.google.com/file/d/1M8QVg2aiAwSSQO3zRJYj35PLnMKBa5L9/view?usp=drive_link-copy\n",
        "#https://drive.google.com/file/d/1kz33s_tiLydRDFRf4GMxxBIvrW_lEpTC/view?usp=drive_link-copy\n",
        "#https://drive.google.com/file/d/1-VQQyXape4lCTa_5bA9VTlJgqKkqqR3h/view?usp=drive_link\n",
        "#https://drive.google.com/file/d/1LcnPYD4m3jp4l7EF8s8mqp3F8QUlcr9-/view?usp=drive_link\n",
        "#https://drive.google.com/file/d/1eQ8nSwENhtwu7X1aNj8XgmHy5KwcMfEU/view?usp=drive_link\n",
        "#https://drive.google.com/file/d/1vyYognzSAMenVakNyXgchwfZlVc76ZMk/view?usp=drive_link\n",
        "#https://drive.google.com/file/d/1yLL_31R4J1Me_CEyZCYSsJrcQkzZtxKf/view?usp=drive_link\n",
        "\n",
        "\n",
        "print(f\"Files to download: {len(google_drive_file_ids)}\")\n",
        "for filename in google_drive_file_ids.keys():\n",
        "    print(f\"  - {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "download-upload-header",
      "metadata": {
        "id": "download-upload-header"
      },
      "source": [
        "### 2.2 Download and Upload to S3\n",
        "\n",
        "**Process**:\n",
        "1. Download each file from Google Drive\n",
        "2. Upload to your account-specific S3 data bucket\n",
        "3. Clean up local files to save disk space\n",
        "\n",
        "**Warning**: This will download ~8.5 GB of data. Ensure you have sufficient disk space and network bandwidth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download-upload",
      "metadata": {
        "id": "download-upload"
      },
      "outputs": [],
      "source": [
        "file_to_dir = {\n",
        "    \"yelp_academic_dataset_business.json\": \"business/\",\n",
        "    \"yelp_academic_dataset_checkin.json\": \"checkin/\",\n",
        "    \"yelp_academic_dataset_review.json\": \"review/\",\n",
        "    \"yelp_academic_dataset_tip.json\": \"tip/\",\n",
        "    \"yelp_academic_dataset_user.json\": \"user/\",\n",
        "}\n",
        "\n",
        "# Change to working directory\n",
        "work_dir = \"/home/sagemaker-user/VenueSignal\"\n",
        "os.makedirs(work_dir, exist_ok=True)\n",
        "os.chdir(work_dir)\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(f\"Target S3 bucket: {BASE_BUCKET_NAME}\")\n",
        "print(f\"Target S3 prefix: {RAW_DATA_PREFIX}\")\n",
        "print()\n",
        "\n",
        "\n",
        "def process_one_file(filename, file_id, s3_client, RAW_DATA_PREFIX, BASE_BUCKET_NAME):\n",
        "    # Step 1: Download from Google Drive\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    gdown.download(download_url, filename, quiet=True)\n",
        "\n",
        "    # Step 2: Upload to S3\n",
        "    file_dir = file_to_dir[filename]\n",
        "    s3_key = f\"{RAW_DATA_PREFIX}{file_dir}{filename}\"\n",
        "    s3_client.upload_file(filename, BASE_BUCKET_NAME, s3_key)\n",
        "\n",
        "    # Step 3: Clean up local file\n",
        "    if os.path.exists(filename):\n",
        "        os.remove(filename)\n",
        "\n",
        "    return filename, s3_key\n",
        "\n",
        "\n",
        "def download_and_upload_all_concurrently(\n",
        "    google_drive_file_ids: dict,\n",
        "    s3_client,\n",
        "    RAW_DATA_PREFIX: str,\n",
        "    BASE_BUCKET_NAME: str,\n",
        "    max_workers: int = 5,\n",
        "):\n",
        "    results = {\"ok\": [], \"failed\": []}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {\n",
        "            executor.submit(\n",
        "                process_one_file,\n",
        "                filename,\n",
        "                file_id,\n",
        "                s3_client,\n",
        "                RAW_DATA_PREFIX,\n",
        "                BASE_BUCKET_NAME,\n",
        "            ): filename\n",
        "            for filename, file_id in google_drive_file_ids.items()\n",
        "        }\n",
        "\n",
        "        for fut in as_completed(futures):\n",
        "            filename = futures[fut]\n",
        "            try:\n",
        "                fname, s3_key = fut.result()\n",
        "                results[\"ok\"].append((fname, s3_key))\n",
        "                print(f\"✓ {fname} -> s3://{BASE_BUCKET_NAME}/{s3_key}\")\n",
        "            except Exception as e:\n",
        "                results[\"failed\"].append((filename, str(e)))\n",
        "                print(f\"✗ {filename} failed: {e}\")\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "    print(f\"Successful: {len(results['ok'])}\")\n",
        "    print(f\"Failed:     {len(results['failed'])}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "results = download_and_upload_all_concurrently(\n",
        "    google_drive_file_ids=google_drive_file_ids,\n",
        "    s3_client=s3_client,\n",
        "    RAW_DATA_PREFIX=RAW_DATA_PREFIX,\n",
        "    BASE_BUCKET_NAME=BASE_BUCKET_NAME,\n",
        "    max_workers=5,\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\" All files processed successfully!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify-upload-header",
      "metadata": {
        "id": "verify-upload-header"
      },
      "source": [
        "### 2.3 Verify Data Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify-upload",
      "metadata": {
        "id": "verify-upload"
      },
      "outputs": [],
      "source": [
        "# List files in S3\n",
        "s3_path = f\"s3://{DATA_JSON_DIR}\"\n",
        "print(f\"Files in {s3_path}:\\n\")\n",
        "!aws s3 ls {s3_path} --recursive --human-readable\n",
        "\n",
        "# Create clickable link to S3 console\n",
        "from IPython.display import display, HTML\n",
        "s3_console_url = f\"https://s3.console.aws.amazon.com/s3/buckets/{DATA_JSON_DIR}?region={REGION}&tab=overview\"\n",
        "display(HTML(f'<b>View in S3 Console: <a target=\"_blank\" href=\"{s3_console_url}\">S3 Bucket - Yelp Dataset</a></b>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Athena Tables & Data Cataloging <a id='section-3'></a>\n",
        "\n",
        "This section:\n",
        "- Creates Athena database\n",
        "- Defines table schemas for JSON data\n",
        "- Creates queryable tables\n",
        "- Converts JSON to Parquet for better performance\n",
        "\n",
        "**Benefits of Athena**:\n",
        "- Query data in S3 using SQL\n",
        "- No data movement required\n",
        "- Pay only for queries run\n",
        "- Integrates with SageMaker Feature Store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "athena-db-header",
      "metadata": {
        "id": "athena-db-header"
      },
      "source": [
        "### 3.1 Create Athena Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-athena-db",
      "metadata": {
        "id": "create-athena-db"
      },
      "outputs": [],
      "source": [
        "def execute_athena_query(query, database=None, output_location=ATHENA_RESULTS_S3):\n",
        "    \"\"\"\n",
        "    Execute an Athena query and wait for completion.\n",
        "\n",
        "    Args:\n",
        "        query: SQL query to execute\n",
        "        database: Athena database name (optional)\n",
        "        output_location: S3 location for query results\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"QueryString\": query,\n",
        "        \"ResultConfiguration\": {\"OutputLocation\": output_location},\n",
        "    }\n",
        "    if database:\n",
        "        params[\"QueryExecutionContext\"] = {\"Database\": database}\n",
        "\n",
        "    # Start query\n",
        "    qx = athena_client.start_query_execution(**params)\n",
        "    qid = qx[\"QueryExecutionId\"]\n",
        "\n",
        "    # Wait for completion\n",
        "    while True:\n",
        "        resp = athena_client.get_query_execution(QueryExecutionId=qid)\n",
        "        state = resp[\"QueryExecution\"][\"Status\"][\"State\"]\n",
        "        if state in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
        "            break\n",
        "        time.sleep(1)\n",
        "\n",
        "    if state != \"SUCCEEDED\":\n",
        "        reason = resp[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"\")\n",
        "        raise RuntimeError(f\"Athena query {state}: {reason}\\n\\nQuery:\\n{query}\")\n",
        "\n",
        "    # Fetch results\n",
        "    paginator = athena_client.get_paginator(\"get_query_results\")\n",
        "    columns = None\n",
        "    data_rows = []\n",
        "\n",
        "    for page in paginator.paginate(QueryExecutionId=qid):\n",
        "        result_set = page.get(\"ResultSet\", {})\n",
        "        metadata = result_set.get(\"ResultSetMetadata\", {})\n",
        "        col_info = metadata.get(\"ColumnInfo\", [])\n",
        "\n",
        "        # Resolve headers from metadata once\n",
        "        if columns is None and col_info:\n",
        "            columns = [c.get(\"Name\") for c in col_info]\n",
        "\n",
        "        for row in result_set.get(\"Rows\", []):\n",
        "            values = [c.get(\"VarCharValue\") for c in row.get(\"Data\", [])]\n",
        "\n",
        "            # If Athena included a header-like first row equal to column names, skip it.\n",
        "            if columns and values == columns:\n",
        "                continue\n",
        "\n",
        "            # Normalize row length to match columns\n",
        "            if columns:\n",
        "                if len(values) < len(columns):\n",
        "                    values += [None] * (len(columns) - len(values))\n",
        "                elif len(values) > len(columns):\n",
        "                    values = values[:len(columns)]\n",
        "\n",
        "            data_rows.append(values)\n",
        "\n",
        "    # No result rows (common for DDL/DML)\n",
        "    if not data_rows and not columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # If we have columns but no data, still return empty df with correct headers\n",
        "    if columns and not data_rows:\n",
        "        return pd.DataFrame(columns=columns)\n",
        "\n",
        "    # If columns are missing for some reason, generate generic names\n",
        "    if not columns:\n",
        "        max_len = max((len(r) for r in data_rows), default=0)\n",
        "        columns = [f\"col_{i}\" for i in range(max_len)]\n",
        "        data_rows = [r + [None] * (max_len - len(r)) for r in data_rows]\n",
        "\n",
        "    return pd.DataFrame(data_rows, columns=columns)\n",
        "\n",
        "\n",
        "# Create Athena database\n",
        "print(f\"Creating Athena database: {ATHENA_DB}\")\n",
        "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {ATHENA_DB}\"\n",
        "try:\n",
        "    execute_athena_query(create_db_query)\n",
        "    print(f\" Database '{ATHENA_DB}' created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\" Error creating database: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "athena-tables-header",
      "metadata": {
        "id": "athena-tables-header"
      },
      "source": [
        "### 3.2 Define File Locations\n",
        "\n",
        "Map table names to their S3 file locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "define-files",
      "metadata": {
        "id": "define-files"
      },
      "outputs": [],
      "source": [
        "# Define JSON files\n",
        "FILES = {\n",
        "    'business': 'yelp_academic_dataset_business.json',\n",
        "    'review': 'yelp_academic_dataset_review.json',\n",
        "    'user': 'yelp_academic_dataset_user.json',\n",
        "    'checkin': 'yelp_academic_dataset_checkin.json',\n",
        "    'tip': 'yelp_academic_dataset_tip.json'\n",
        "}\n",
        "\n",
        "# Create S3 object keys\n",
        "OBJECT_KEYS = {\n",
        "    table: f\"{RAW_DATA_PREFIX}{table}/{fname}\" for table, fname in FILES.items()\n",
        "}\n",
        "\n",
        "print(\"File mappings:\")\n",
        "for table, key in OBJECT_KEYS.items():\n",
        "    print(f\"  {table:10} -> s3://{BASE_BUCKET_NAME}/{key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify-files-header",
      "metadata": {
        "id": "verify-files-header"
      },
      "source": [
        "### 3.3 Verify File Access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify-files",
      "metadata": {
        "id": "verify-files"
      },
      "outputs": [],
      "source": [
        "dest_locations = {}\n",
        "\n",
        "print(\"Verifying S3 file access...\\n\")\n",
        "for table, key in OBJECT_KEYS.items():\n",
        "    try:\n",
        "        s3_client.head_object(Bucket=BASE_BUCKET_NAME, Key=key)\n",
        "        print(f\" {table:10} {key}\")\n",
        "        dest_locations[table] = f\"s3://{DATA_JSON_DIR}{table}/\"\n",
        "    except ClientError:\n",
        "        print(f\" {table:10} {key} NOT FOUND\")\n",
        "\n",
        "print()\n",
        "print(\"JSON file directory destinations...\\n\")\n",
        "for t, loc in dest_locations.items():\n",
        "    print(f\"{t:8} -> {loc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-tables-header",
      "metadata": {
        "id": "create-tables-header"
      },
      "source": [
        "### 3.4 Create Athena Tables from JSON\n",
        "\n",
        "Create external tables in Athena that point to the JSON files in S3.\n",
        "\n",
        "If you experience any errors while running the table creation cells, uncomment the bellow cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2831da85-1999-46f1-a6c0-fb62db1bd78e",
      "metadata": {
        "id": "2831da85-1999-46f1-a6c0-fb62db1bd78e"
      },
      "outputs": [],
      "source": [
        "\n",
        "TABLES = [\"business\", \"review\", \"user\", \"checkin\", \"tip\", \"business_attributes\"]\n",
        "\n",
        "for table in TABLES:\n",
        "    print(f\"Dropping table: {ATHENA_DB}.{table}\")\n",
        "    execute_athena_query(\n",
        "        f\"DROP TABLE IF EXISTS {ATHENA_DB}.{table};\",\n",
        "        database=ATHENA_DB\n",
        "    )\n",
        "\n",
        "paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "to_delete = []\n",
        "for page in paginator.paginate(Bucket=BASE_BUCKET_NAME, Prefix=ATHENA_RESULTS_PREFIX):\n",
        "    for obj in page.get(\"Contents\", []):\n",
        "        to_delete.append({\"Key\": obj[\"Key\"]})\n",
        "\n",
        "if not to_delete:\n",
        "    print(\"✅ Nothing to delete under\", f\"s3://{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\")\n",
        "else:\n",
        "    # delete in batches of 1000 (S3 limit)\n",
        "    for i in range(0, len(to_delete), 1000):\n",
        "        s3_client.delete_objects(\n",
        "            Bucket=BASE_BUCKET_NAME,\n",
        "            Delete={\"Objects\": to_delete[i:i+1000]}\n",
        "        )\n",
        "    print(f\"✅ Deleted {len(to_delete)} objects under s3://{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\")\n",
        "\n",
        "print(\"✅ All tables dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21f5c51-702f-41bc-8ddc-a1769a33e7ae",
      "metadata": {
        "id": "b21f5c51-702f-41bc-8ddc-a1769a33e7ae"
      },
      "outputs": [],
      "source": [
        "# TABLES = [\"business\", \"review\", \"user\", \"checkin\", \"tip\", \"business_attributes\"]\n",
        "\n",
        "# for table in TABLES:\n",
        "#     print(f\"Dropping table: {ATHENA_DB}.{table}\")\n",
        "#     execute_athena_query(\n",
        "#         f\"DROP TABLE IF EXISTS {ATHENA_DB}.{table};\",\n",
        "#         database=ATHENA_DB\n",
        "#     )\n",
        "\n",
        "# paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "# to_delete = []\n",
        "# for page in paginator.paginate(Bucket=BASE_BUCKET_NAME, Prefix=ATHENA_RESULTS_PREFIX):\n",
        "#     for obj in page.get(\"Contents\", []):\n",
        "#         to_delete.append({\"Key\": obj[\"Key\"]})\n",
        "\n",
        "# if not to_delete:\n",
        "#     print(\"✅ Nothing to delete under\", f\"s3://{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\")\n",
        "# else:\n",
        "#     # delete in batches of 1000 (S3 limit)\n",
        "#     for i in range(0, len(to_delete), 1000):\n",
        "#         s3_client.delete_objects(\n",
        "#             Bucket=BASE_BUCKET_NAME,\n",
        "#             Delete={\"Objects\": to_delete[i:i+1000]}\n",
        "#         )\n",
        "#     print(f\"✅ Deleted {len(to_delete)} objects under s3://{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\")\n",
        "\n",
        "# print(\"✅ All tables dropped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336a0f02-658b-45a2-ac63-c08d027bf3ae",
      "metadata": {
        "id": "336a0f02-658b-45a2-ac63-c08d027bf3ae"
      },
      "outputs": [],
      "source": [
        "business_location = dest_locations[\"business\"]\n",
        "\n",
        "# parquet_prefix\n",
        "business_parquet_location = f\"s3://{DATA_PARQUET_DIR}business\"\n",
        "\n",
        "print(\"Creating temporary table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.business_temp (\n",
        "  business_id string,\n",
        "  name string,\n",
        "  address string,\n",
        "  city string,\n",
        "  state string,\n",
        "  postal_code string,\n",
        "  latitude double,\n",
        "  longitude double,\n",
        "  stars double,\n",
        "  review_count int,\n",
        "  is_open int,\n",
        "  attributes map<string,string>,\n",
        "  categories string,\n",
        "  hours map<string,string>\n",
        ")\n",
        "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
        "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
        "LOCATION '{business_location}'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Creating table with parquets\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE TABLE {ATHENA_DB}.business\n",
        "WITH (\n",
        "  format = 'PARQUET',\n",
        "  external_location = '{business_parquet_location}'\n",
        ") AS\n",
        "SELECT\n",
        "  business_id,\n",
        "  name,\n",
        "  address,\n",
        "  city,\n",
        "  state,\n",
        "  postal_code,\n",
        "  latitude,\n",
        "  longitude,\n",
        "  stars,\n",
        "  review_count,\n",
        "  is_open,\n",
        "  attributes,\n",
        "  categories,\n",
        "  hours\n",
        "\n",
        "FROM {ATHENA_DB}.business_temp;\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"drop temp table\")\n",
        "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.business_temp;\", database=ATHENA_DB)\n",
        "\n",
        "print(f\"✅ Created table {ATHENA_DB}.business\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a840eaa-1267-4a79-8ca9-d9999a9223e0",
      "metadata": {
        "id": "0a840eaa-1267-4a79-8ca9-d9999a9223e0"
      },
      "outputs": [],
      "source": [
        "review_location = dest_locations[\"review\"]\n",
        "# parquet_prefix\n",
        "review_parquet_location = f\"s3://{DATA_PARQUET_DIR}review\"\n",
        "\n",
        "print(\"Creating temporary table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.review_temp (\n",
        "  review_id string,\n",
        "  user_id string,\n",
        "  business_id string,\n",
        "  stars double,\n",
        "  useful int,\n",
        "  funny int,\n",
        "  cool int,\n",
        "  text string,\n",
        "  date string\n",
        ")\n",
        "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
        "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
        "LOCATION '{review_location}'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Creating table with parquets\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE TABLE {ATHENA_DB}.review\n",
        "WITH (\n",
        "  format = 'PARQUET',\n",
        "  external_location = '{review_parquet_location}',\n",
        "  partitioned_by = ARRAY['year']\n",
        ") AS\n",
        "SELECT\n",
        "  review_id,\n",
        "  user_id,\n",
        "  business_id,\n",
        "  stars,\n",
        "  useful,\n",
        "  funny,\n",
        "  cool,\n",
        "  text,\n",
        "  date,\n",
        "  CAST(substr(date, 1, 4) AS integer) AS year\n",
        "\n",
        "FROM {ATHENA_DB}.review_temp\n",
        "WHERE date IS NOT NULL;\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"drop temp table\")\n",
        "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.review_temp;\", database=ATHENA_DB)\n",
        "\n",
        "print(f\"✅ Created table {ATHENA_DB}.review\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c95502-abd9-4f55-abde-6f976136be83",
      "metadata": {
        "id": "05c95502-abd9-4f55-abde-6f976136be83"
      },
      "outputs": [],
      "source": [
        "user_location = dest_locations[\"user\"]\n",
        "\n",
        "# parquet_prefix\n",
        "user_parquet_location = f\"s3://{DATA_PARQUET_DIR}user\"\n",
        "\n",
        "print(\"Create temp table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.user_temp (\n",
        "  user_id string,\n",
        "  name string,\n",
        "  review_count int,\n",
        "  yelping_since string,\n",
        "  friends array<string>,\n",
        "  useful int,\n",
        "  funny int,\n",
        "  cool int,\n",
        "  fans int,\n",
        "  elite array<string>,\n",
        "  average_stars double,\n",
        "  compliment_hot int,\n",
        "  compliment_more int,\n",
        "  compliment_profile int,\n",
        "  compliment_cute int,\n",
        "  compliment_list int,\n",
        "  compliment_note int,\n",
        "  compliment_plain int,\n",
        "  compliment_cool int,\n",
        "  compliment_funny int,\n",
        "  compliment_writer int,\n",
        "  compliment_photos int\n",
        ")\n",
        "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
        "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
        "LOCATION '{user_location}'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Create Parquet table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE TABLE {ATHENA_DB}.user\n",
        "WITH (\n",
        "  format = 'PARQUET',\n",
        "  external_location = '{user_parquet_location}'\n",
        ") AS\n",
        "SELECT\n",
        "  user_id,\n",
        "  name,\n",
        "  review_count,\n",
        "  yelping_since,\n",
        "  friends,\n",
        "  useful,\n",
        "  funny,\n",
        "  cool,\n",
        "  fans,\n",
        "  elite,\n",
        "  average_stars,\n",
        "  compliment_hot,\n",
        "  compliment_more,\n",
        "  compliment_profile,\n",
        "  compliment_cute,\n",
        "  compliment_list,\n",
        "  compliment_note,\n",
        "  compliment_plain,\n",
        "  compliment_cool,\n",
        "  compliment_funny,\n",
        "  compliment_writer,\n",
        "  compliment_photos\n",
        "FROM {ATHENA_DB}.user_temp;\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Drop temp table\")\n",
        "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.user_temp;\", database=ATHENA_DB)\n",
        "\n",
        "print(f\"✅ Created table {ATHENA_DB}.user\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2990ba-ce4f-48af-9780-3fc93985a97f",
      "metadata": {
        "id": "9f2990ba-ce4f-48af-9780-3fc93985a97f"
      },
      "outputs": [],
      "source": [
        "checkin_location = dest_locations[\"checkin\"]\n",
        "\n",
        "# parquet_prefix\n",
        "checkin_parquet_location = f\"s3://{DATA_PARQUET_DIR}checkin\"\n",
        "\n",
        "print(\"Create temp table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.checkin_temp (\n",
        "  business_id string,\n",
        "  date string\n",
        ")\n",
        "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
        "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
        "LOCATION '{checkin_location}'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Create Parquet table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE TABLE {ATHENA_DB}.checkin\n",
        "WITH (\n",
        "  format = 'PARQUET',\n",
        "  external_location = '{checkin_parquet_location}'\n",
        ") AS\n",
        "SELECT\n",
        "  business_id,\n",
        "  date\n",
        "FROM {ATHENA_DB}.checkin_temp;\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Drop temp table\")\n",
        "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.checkin_temp;\", database=ATHENA_DB)\n",
        "\n",
        "\n",
        "print(f\"✅ Created table {ATHENA_DB}.checkin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be29d713-419d-466c-a35c-ca8ada23708f",
      "metadata": {
        "id": "be29d713-419d-466c-a35c-ca8ada23708f"
      },
      "outputs": [],
      "source": [
        "tip_location = dest_locations[\"tip\"]\n",
        "\n",
        "# parquet_prefix\n",
        "tip_parquet_location = f\"s3://{DATA_PARQUET_DIR}tip\"\n",
        "\n",
        "print(\"Create temp table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.tip_temp (\n",
        "  user_id string,\n",
        "  business_id string,\n",
        "  text string,\n",
        "  date string,\n",
        "  compliment_count int\n",
        ")\n",
        "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
        "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
        "LOCATION '{tip_location}'\n",
        "TBLPROPERTIES ('has_encrypted_data'='false');\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Create Parquet table\")\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE TABLE {ATHENA_DB}.tip\n",
        "WITH (\n",
        "  format = 'PARQUET',\n",
        "  external_location = '{tip_parquet_location}',\n",
        "  partitioned_by = ARRAY['year']\n",
        ") AS\n",
        "SELECT\n",
        "  user_id,\n",
        "  business_id,\n",
        "  text,\n",
        "  date,\n",
        "  compliment_count,\n",
        "  CAST(substr(date, 1, 4) AS integer) AS year\n",
        "\n",
        "FROM {ATHENA_DB}.tip_temp\n",
        "WHERE date IS NOT NULL;\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(\"Drop temp table\")\n",
        "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.tip_temp;\", database=ATHENA_DB)\n",
        "\n",
        "print(f\"✅ Created table {ATHENA_DB}.tip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f1ec58-5a9b-42c6-a5fc-0cc2f3b4fc3c",
      "metadata": {
        "id": "32f1ec58-5a9b-42c6-a5fc-0cc2f3b4fc3c"
      },
      "outputs": [],
      "source": [
        "business_attributes_location = f\"s3://{DATA_PARQUET_DIR}business_attributes\"\n",
        "\n",
        "execute_athena_query(f\"\"\"\n",
        "CREATE TABLE {ATHENA_DB}.business_attributes\n",
        "WITH (\n",
        "  format = 'PARQUET',\n",
        "  external_location = '{business_attributes_location}'\n",
        ") AS\n",
        "WITH normalized AS (\n",
        "  SELECT\n",
        "    business_id,\n",
        "    hours,\n",
        "\n",
        "    -- Normalize u'...' and '...' wrappers on keys + values\n",
        "    map_from_entries(\n",
        "      transform(\n",
        "        map_entries(attributes),\n",
        "        e -> CAST(\n",
        "          ROW(\n",
        "            regexp_replace(CAST(e[1] AS varchar), '^u?''(.*)''$', '$1'),\n",
        "            regexp_replace(CAST(e[2] AS varchar), '^u?''(.*)''$', '$1')\n",
        "          ) AS ROW(varchar, varchar)\n",
        "        )\n",
        "      )\n",
        "    ) AS attrs\n",
        "  FROM {ATHENA_DB}.business\n",
        "  WHERE attributes IS NOT NULL\n",
        "),\n",
        "parsed AS (\n",
        "  SELECT\n",
        "    business_id,\n",
        "    hours,\n",
        "    -- Convert literal \"None\" (any case) to NULL for all attribute lookups via helper expression pattern below\n",
        "    attrs,\n",
        "\n",
        "    -- Parse BusinessParking\n",
        "    TRY(\n",
        "      CAST(\n",
        "        json_parse(\n",
        "          replace(\n",
        "            replace(\n",
        "              replace(\n",
        "                replace(\n",
        "                  regexp_replace(attrs['businessparking'], 'u''(.*?)''', '\"$1\"'),\n",
        "                  '''', '\"'\n",
        "                ),\n",
        "                'False', 'false'\n",
        "              ),\n",
        "              'True', 'true'\n",
        "            ),\n",
        "            'None', 'null'\n",
        "          )\n",
        "        ) AS map(varchar, boolean)\n",
        "      )\n",
        "    ) AS parking_map,\n",
        "\n",
        "    -- Parse Ambience\n",
        "    TRY(\n",
        "      CAST(\n",
        "        json_parse(\n",
        "          replace(\n",
        "            replace(\n",
        "              replace(\n",
        "                replace(\n",
        "                  -- normalize u'...' keys inside the string\n",
        "                  regexp_replace(attrs['ambience'], 'u''(.*?)''', '\"$1\"'),\n",
        "                  '''', '\"'\n",
        "                ),\n",
        "                'False', 'false'\n",
        "              ),\n",
        "              'True', 'true'\n",
        "            ),\n",
        "            'None', 'null'\n",
        "          )\n",
        "        ) AS map(varchar, boolean)\n",
        "      )\n",
        "    ) AS ambience_map,\n",
        "\n",
        "    -- Parse GoodForMeal\n",
        "    TRY(\n",
        "      CAST(\n",
        "        json_parse(\n",
        "          replace(\n",
        "            replace(\n",
        "              replace(\n",
        "                replace(\n",
        "                  -- normalize u'...' keys\n",
        "                  regexp_replace(attrs['goodformeal'], 'u''(.*?)''', '\"$1\"'),\n",
        "                  '''', '\"'\n",
        "                ),\n",
        "                'False', 'false'\n",
        "              ),\n",
        "              'True', 'true'\n",
        "            ),\n",
        "            'None', 'null'\n",
        "          )\n",
        "        ) AS map(varchar, boolean)\n",
        "      )\n",
        "    ) AS goodformeal_map,\n",
        "\n",
        "    -- Parse BestNights\n",
        "    TRY(\n",
        "      CAST(\n",
        "        json_parse(\n",
        "          replace(\n",
        "            replace(\n",
        "              replace(\n",
        "                replace(\n",
        "                  -- normalize u'...' keys\n",
        "                  regexp_replace(attrs['bestnights'], 'u''(.*?)''', '\"$1\"'),\n",
        "                  '''', '\"'\n",
        "                ),\n",
        "                'False', 'false'\n",
        "              ),\n",
        "              'True', 'true'\n",
        "            ),\n",
        "            'None', 'null'\n",
        "          )\n",
        "        ) AS map(varchar, boolean)\n",
        "      )\n",
        "    ) AS bestnights_map,\n",
        "\n",
        "    -- Parse HairSpecializesIn\n",
        "    TRY(\n",
        "      CAST(\n",
        "        json_parse(\n",
        "          replace(\n",
        "            replace(\n",
        "              replace(\n",
        "                replace(\n",
        "                  -- normalize u'...' keys\n",
        "                  regexp_replace(attrs['hairspecializesin'], 'u''(.*?)''', '\"$1\"'),\n",
        "                  '''', '\"'\n",
        "                ),\n",
        "                'False', 'false'\n",
        "              ),\n",
        "              'True', 'true'\n",
        "            ),\n",
        "            'None', 'null'\n",
        "          )\n",
        "        ) AS map(varchar, boolean)\n",
        "      )\n",
        "    ) AS hairspecializesin_map,\n",
        "\n",
        "    -- Parse DietaryRestrictions\n",
        "    TRY(\n",
        "      CAST(\n",
        "        json_parse(\n",
        "          replace(\n",
        "            replace(\n",
        "              replace(\n",
        "                replace(\n",
        "                  -- normalize u'...' keys\n",
        "                  regexp_replace(attrs['dietaryrestrictions'], 'u''(.*?)''', '\"$1\"'),\n",
        "                  '''', '\"'\n",
        "                ),\n",
        "                'False', 'false'\n",
        "              ),\n",
        "              'True', 'true'\n",
        "            ),\n",
        "            'None', 'null'\n",
        "          )\n",
        "        ) AS map(varchar, boolean)\n",
        "      )\n",
        "    ) AS dietaryrestrictions_map\n",
        "  FROM normalized\n",
        ")\n",
        "SELECT\n",
        "    business_id,\n",
        "\n",
        "    -- Helper pattern: NULLIF(lower(x),'none') but preserving original case when not none\n",
        "    CASE WHEN attrs['acceptsinsurance'] IS NULL OR lower(attrs['acceptsinsurance']) = 'none' THEN NULL ELSE attrs['acceptsinsurance'] END AS acceptsinsurance,\n",
        "    CASE WHEN attrs['agesallowed'] IS NULL OR lower(attrs['agesallowed']) = 'none' THEN NULL ELSE attrs['agesallowed'] END AS agesallowed,\n",
        "    CASE WHEN attrs['alcohol'] IS NULL OR lower(attrs['alcohol']) = 'none' THEN NULL ELSE attrs['alcohol'] END AS alcohol,\n",
        "    CASE WHEN attrs['bikeparking'] IS NULL OR lower(attrs['bikeparking']) = 'none' THEN NULL ELSE attrs['bikeparking'] END AS bikeparking,\n",
        "    CASE WHEN attrs['businessacceptsbitcoin'] IS NULL OR lower(attrs['businessacceptsbitcoin']) = 'none' THEN NULL ELSE attrs['businessacceptsbitcoin'] END AS businessacceptsbitcoin,\n",
        "    CASE WHEN attrs['businessacceptscreditcards'] IS NULL OR lower(attrs['businessacceptscreditcards']) = 'none' THEN NULL ELSE attrs['businessacceptscreditcards'] END AS businessacceptscreditcards,\n",
        "    CASE WHEN attrs['byappointmentonly'] IS NULL OR lower(attrs['byappointmentonly']) = 'none' THEN NULL ELSE attrs['byappointmentonly'] END AS byappointmentonly,\n",
        "    CASE WHEN attrs['byob'] IS NULL OR lower(attrs['byob']) = 'none' THEN NULL ELSE attrs['byob'] END AS byob,\n",
        "    CASE WHEN attrs['byobcorkage'] IS NULL OR lower(attrs['byobcorkage']) = 'none' THEN NULL ELSE attrs['byobcorkage'] END AS byobcorkage,\n",
        "    CASE WHEN attrs['caters'] IS NULL OR lower(attrs['caters']) = 'none' THEN NULL ELSE attrs['caters'] END AS caters,\n",
        "    CASE WHEN attrs['coatcheck'] IS NULL OR lower(attrs['coatcheck']) = 'none' THEN NULL ELSE attrs['coatcheck'] END AS coatcheck,\n",
        "    CASE WHEN attrs['corkage'] IS NULL OR lower(attrs['corkage']) = 'none' THEN NULL ELSE attrs['corkage'] END AS corkage,\n",
        "    CASE WHEN attrs['dogsallowed'] IS NULL OR lower(attrs['dogsallowed']) = 'none' THEN NULL ELSE attrs['dogsallowed'] END AS dogsallowed,\n",
        "    CASE WHEN attrs['drivethru'] IS NULL OR lower(attrs['drivethru']) = 'none' THEN NULL ELSE attrs['drivethru'] END AS drivethru,\n",
        "    CASE WHEN attrs['goodfordancing'] IS NULL OR lower(attrs['goodfordancing']) = 'none' THEN NULL ELSE attrs['goodfordancing'] END AS goodfordancing,\n",
        "    CASE WHEN attrs['goodforkids'] IS NULL OR lower(attrs['goodforkids']) = 'none' THEN NULL ELSE attrs['goodforkids'] END AS goodforkids,\n",
        "    CASE WHEN attrs['happyhour'] IS NULL OR lower(attrs['happyhour']) = 'none' THEN NULL ELSE attrs['happyhour'] END AS happyhour,\n",
        "    CASE WHEN attrs['hastv'] IS NULL OR lower(attrs['hastv']) = 'none' THEN NULL ELSE attrs['hastv'] END AS hastv,\n",
        "    CASE WHEN attrs['music'] IS NULL OR lower(attrs['music']) = 'none' THEN NULL ELSE attrs['music'] END AS music,\n",
        "    CASE WHEN attrs['noiselevel'] IS NULL OR lower(attrs['noiselevel']) = 'none' THEN NULL ELSE attrs['noiselevel'] END AS noiselevel,\n",
        "    CASE WHEN attrs['open24hours'] IS NULL OR lower(attrs['open24hours']) = 'none' THEN NULL ELSE attrs['open24hours'] END AS open24hours,\n",
        "    CASE WHEN attrs['outdoorseating'] IS NULL OR lower(attrs['outdoorseating']) = 'none' THEN NULL ELSE attrs['outdoorseating'] END AS outdoorseating,\n",
        "    CASE WHEN attrs['restaurantsattire'] IS NULL OR lower(attrs['restaurantsattire']) = 'none' THEN NULL ELSE attrs['restaurantsattire'] END AS restaurantsattire,\n",
        "    CASE WHEN attrs['restaurantscounterservice'] IS NULL OR lower(attrs['restaurantscounterservice']) = 'none' THEN NULL ELSE attrs['restaurantscounterservice'] END AS restaurantscounterservice,\n",
        "    CASE WHEN attrs['restaurantsdelivery'] IS NULL OR lower(attrs['restaurantsdelivery']) = 'none' THEN NULL ELSE attrs['restaurantsdelivery'] END AS restaurantsdelivery,\n",
        "    CASE WHEN attrs['restaurantsgoodforgroups'] IS NULL OR lower(attrs['restaurantsgoodforgroups']) = 'none' THEN NULL ELSE attrs['restaurantsgoodforgroups'] END AS restaurantsgoodforgroups,\n",
        "    CASE WHEN attrs['restaurantspricerange2'] IS NULL OR lower(attrs['restaurantspricerange2']) = 'none' THEN NULL ELSE attrs['restaurantspricerange2'] END AS restaurantspricerange2,\n",
        "    CASE WHEN attrs['restaurantsreservations'] IS NULL OR lower(attrs['restaurantsreservations']) = 'none' THEN NULL ELSE attrs['restaurantsreservations'] END AS restaurantsreservations,\n",
        "    CASE WHEN attrs['restaurantstableservice'] IS NULL OR lower(attrs['restaurantstableservice']) = 'none' THEN NULL ELSE attrs['restaurantstableservice'] END AS restaurantstableservice,\n",
        "    CASE WHEN attrs['restaurantstakeout'] IS NULL OR lower(attrs['restaurantstakeout']) = 'none' THEN NULL ELSE attrs['restaurantstakeout'] END AS restaurantstakeout,\n",
        "    CASE WHEN attrs['smoking'] IS NULL OR lower(attrs['smoking']) = 'none' THEN NULL ELSE attrs['smoking'] END AS smoking,\n",
        "    CASE WHEN attrs['wheelchairaccessible'] IS NULL OR lower(attrs['wheelchairaccessible']) = 'none' THEN NULL ELSE attrs['wheelchairaccessible'] END AS wheelchairaccessible,\n",
        "    CASE WHEN attrs['wifi'] IS NULL OR lower(attrs['wifi']) = 'none' THEN NULL ELSE attrs['wifi'] END AS wifi,\n",
        "\n",
        "    -- Parking\n",
        "    parking_map['garage']    AS parking_garage,\n",
        "    parking_map['street']    AS parking_street,\n",
        "    parking_map['validated'] AS parking_validated,\n",
        "    parking_map['lot']       AS parking_lot,\n",
        "    parking_map['valet']     AS parking_valet,\n",
        "\n",
        "    -- Ambience\n",
        "    ambience_map['divey']     AS ambience_divey,\n",
        "    ambience_map['hipster']  AS ambience_hipster,\n",
        "    ambience_map['casual']   AS ambience_casual,\n",
        "    ambience_map['touristy'] AS ambience_touristy,\n",
        "    ambience_map['trendy']   AS ambience_trendy,\n",
        "    ambience_map['intimate'] AS ambience_intimate,\n",
        "    ambience_map['romantic'] AS ambience_romantic,\n",
        "    ambience_map['classy']   AS ambience_classy,\n",
        "    ambience_map['upscale']  AS ambience_upscale,\n",
        "\n",
        "    -- GoodForMeal\n",
        "    goodformeal_map['dessert']    AS good_for_dessert,\n",
        "    goodformeal_map['latenight'] AS good_for_latenight,\n",
        "    goodformeal_map['lunch']     AS good_for_lunch,\n",
        "    goodformeal_map['dinner']    AS good_for_dinner,\n",
        "    goodformeal_map['brunch']    AS good_for_brunch,\n",
        "    goodformeal_map['breakfast'] AS good_for_breakfast,\n",
        "\n",
        "    -- BestNights\n",
        "    bestnights_map['monday']    AS bestnight_monday,\n",
        "    bestnights_map['tuesday']   AS bestnight_tuesday,\n",
        "    bestnights_map['wednesday'] AS bestnight_wednesday,\n",
        "    bestnights_map['thursday']  AS bestnight_thursday,\n",
        "    bestnights_map['friday']    AS bestnight_friday,\n",
        "    bestnights_map['saturday']  AS bestnight_saturday,\n",
        "    bestnights_map['sunday']    AS bestnight_sunday,\n",
        "\n",
        "    -- HairSpecializesIn\n",
        "    hairspecializesin_map['africanamerican'] AS hair_africanamerican,\n",
        "    hairspecializesin_map['asian']           AS hair_asian,\n",
        "    hairspecializesin_map['coloring']        AS hair_coloring,\n",
        "    hairspecializesin_map['curly']           AS hair_curly,\n",
        "    hairspecializesin_map['extensions']      AS hair_extensions,\n",
        "    hairspecializesin_map['kids']            AS hair_kids,\n",
        "    hairspecializesin_map['perms']           AS hair_perms,\n",
        "    hairspecializesin_map['straightperms']   AS hair_straightperms,\n",
        "\n",
        "    -- DietaryRestrictions\n",
        "    dietaryrestrictions_map['dairy-free']      AS dairy_free,\n",
        "    dietaryrestrictions_map['gluten-free']    AS gluten_free,\n",
        "    dietaryrestrictions_map['vegan']           AS vegan,\n",
        "    dietaryrestrictions_map['kosher']          AS kosher,\n",
        "    dietaryrestrictions_map['halal']           AS halal,\n",
        "    dietaryrestrictions_map['soy-free']        AS soy_free,\n",
        "    dietaryrestrictions_map['vegetarian']      AS vegetarian,\n",
        "\n",
        "    -- Hours\n",
        "    hours['monday']    AS hours_monday,\n",
        "    hours['tuesday']   AS hours_tuesday,\n",
        "    hours['wednesday'] AS hours_wednesday,\n",
        "    hours['thursday']  AS hours_thursday,\n",
        "    hours['friday']    AS hours_friday,\n",
        "    hours['saturday']  AS hours_saturday,\n",
        "    hours['sunday']    AS hours_sunday,\n",
        "\n",
        "    CARDINALITY(map_keys(hours)) AS open_days_count,\n",
        "    CASE WHEN hours['saturday'] IS NOT NULL OR hours['sunday'] IS NOT NULL THEN true ELSE false END AS open_on_weekend\n",
        "\n",
        "FROM parsed;\n",
        "\"\"\", database=ATHENA_DB)\n",
        "\n",
        "print(f\"✅ Built {ATHENA_DB}.business_attributes\")\n",
        "print(\"📍 Location:\", business_attributes_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pyathena-connect-header",
      "metadata": {
        "id": "pyathena-connect-header"
      },
      "source": [
        "### 3.5 Connect to Athena with PyAthena\n",
        "\n",
        "Create a connection to query the tables using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pyathena-connect",
      "metadata": {
        "id": "pyathena-connect"
      },
      "outputs": [],
      "source": [
        "# Create PyAthena connection\n",
        "conn = connect(\n",
        "    s3_staging_dir=ATHENA_RESULTS_S3,\n",
        "    region_name=REGION,\n",
        "    cursor_class=PandasCursor\n",
        ")\n",
        "\n",
        "print(f\" Connected to Athena database: {ATHENA_DB}\")\n",
        "print(f\"   Results location: {ATHENA_RESULTS_S3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-tables-header",
      "metadata": {
        "id": "test-tables-header"
      },
      "source": [
        "### 3.6 Test Athena Tables\n",
        "\n",
        "Run sample queries to verify table creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-tables",
      "metadata": {
        "id": "test-tables"
      },
      "outputs": [],
      "source": [
        "# Query business table\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    COUNT(*) as total_businesses,\n",
        "    COUNT(DISTINCT city) as unique_cities,\n",
        "    COUNT(DISTINCT state) as unique_states\n",
        "FROM {ATHENA_DB}.business\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"Testing business table...\")\n",
        "df = pd.read_sql(query, conn)\n",
        "display(df)\n",
        "\n",
        "\n",
        "\n",
        "# Query review table\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    COUNT(*) as total_reviews,\n",
        "    AVG(stars) as avg_stars,\n",
        "    MIN(stars) as min_stars,\n",
        "    MAX(stars) as max_stars\n",
        "FROM {ATHENA_DB}.review\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nTesting review table...\")\n",
        "df = pd.read_sql(query, conn)\n",
        "display(df)\n",
        "\n",
        "print(\"\\n Athena tables are working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Exploratory Data Analysis <a id='section-4'></a>\n",
        "\n",
        "This section explores the Yelp dataset to understand:\n",
        "- Business distribution across cities and states\n",
        "- Review patterns and rating distributions\n",
        "- Parking availability and its relationship to ratings\n",
        "- Data quality issues\n",
        "\n",
        "**Focus**: Understanding how parking constraints affect business ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-sample-header",
      "metadata": {
        "id": "load-sample-header"
      },
      "source": [
        "### 4.1 Load Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-sample",
      "metadata": {
        "id": "load-sample"
      },
      "outputs": [],
      "source": [
        "# Load a sample of businesses with parking information\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    business_id,\n",
        "    name,\n",
        "    city,\n",
        "    state,\n",
        "    stars,\n",
        "    review_count,\n",
        "    categories\n",
        "FROM {ATHENA_DB}.business\n",
        "WHERE is_open = 1\n",
        "    AND review_count >= 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"Loading sample business data...\")\n",
        "business_df = pd.read_sql(query, conn)\n",
        "print(f\" Loaded {len(business_df):,} businesses\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nSample data:\")\n",
        "display(business_df.head())\n",
        "\n",
        "# Show basic statistics\n",
        "print(f\"\\n Data Summary:\")\n",
        "print(f\"   Total businesses: {len(business_df):,}\")\n",
        "print(f\"   Unique cities: {business_df['city'].nunique():,}\")\n",
        "print(f\"   Unique states: {business_df['state'].nunique()}\")\n",
        "print(f\"   Average rating: {business_df['stars'].mean():.2f}\")\n",
        "print(f\"   Average reviews: {business_df['review_count'].mean():.0f}\")\n",
        "\n",
        "print(\"\\n Note: Parking features will be extracted from review text in Section 5\")\n",
        "print(\"   This provides more accurate parking information than business attributes!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parking-analysis-header",
      "metadata": {
        "id": "parking-analysis-header"
      },
      "source": [
        "### 4.2 Analyze Parking Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "parking-analysis",
      "metadata": {
        "id": "parking-analysis"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BUSINESS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Top cities\n",
        "print(\"\\n  Top 10 Cities:\")\n",
        "top_cities = business_df['city'].value_counts().head(10)\n",
        "for idx, (city, count) in enumerate(top_cities.items(), 1):\n",
        "    print(f\"   {idx:2d}. {city:20s} - {count:,} businesses\")\n",
        "\n",
        "# Top states\n",
        "print(\"\\n  Top 10 States:\")\n",
        "top_states = business_df['state'].value_counts().head(10)\n",
        "for idx, (state, count) in enumerate(top_states.items(), 1):\n",
        "    print(f\"   {idx:2d}. {state:5s} - {count:,} businesses\")\n",
        "\n",
        "# Rating distribution\n",
        "print(\"\\n Rating Distribution:\")\n",
        "rating_dist = business_df['stars'].value_counts().sort_index()\n",
        "for stars, count in rating_dist.items():\n",
        "    bar_length = int(count / rating_dist.max() * 40)\n",
        "    bar = \"█\" * bar_length\n",
        "    print(f\"   {stars:.1f} stars: {bar} ({count:,})\")\n",
        "\n",
        "# Review count statistics\n",
        "print(\"\\n Review Count Statistics:\")\n",
        "review_stats = business_df['review_count'].describe()\n",
        "print(f\"   Min:     {review_stats['min']:,.0f}\")\n",
        "print(f\"   25th:    {review_stats['25%']:,.0f}\")\n",
        "print(f\"   Median:  {review_stats['50%']:,.0f}\")\n",
        "print(f\"   75th:    {review_stats['75%']:,.0f}\")\n",
        "print(f\"   Max:     {review_stats['max']:,.0f}\")\n",
        "print(f\"   Mean:    {review_stats['mean']:,.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "visualization-header",
      "metadata": {
        "id": "visualization-header"
      },
      "source": [
        "### 4.3 Visualize Key Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualizations",
      "metadata": {
        "id": "visualizations"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Rating Distribution\n",
        "axes[0, 0].hist(business_df['stars'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[0, 0].set_title('Distribution of Business Ratings', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Stars', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Count', fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Review Count Distribution (Log Scale)\n",
        "axes[0, 1].hist(business_df['review_count'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
        "axes[0, 1].set_yscale('log')\n",
        "axes[0, 1].set_title('Distribution of Review Counts (Log Scale)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Number of Reviews', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Count (log scale)', fontsize=11)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Top 10 Cities\n",
        "top_cities = business_df['city'].value_counts().head(10)\n",
        "y_pos = range(len(top_cities))\n",
        "axes[1, 0].barh(y_pos, top_cities.values, alpha=0.7, color='green')\n",
        "axes[1, 0].set_yticks(y_pos)\n",
        "axes[1, 0].set_yticklabels(top_cities.index)\n",
        "axes[1, 0].set_title('Top 10 Cities by Business Count', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Number of Businesses', fontsize=11)\n",
        "axes[1, 0].invert_yaxis()\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 4. Top 10 States\n",
        "top_states = business_df['state'].value_counts().head(10)\n",
        "x_pos = range(len(top_states))\n",
        "axes[1, 1].bar(x_pos, top_states.values, alpha=0.7, color='purple')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(top_states.index, rotation=45)\n",
        "axes[1, 1].set_title('Top 10 States by Business Count', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Number of Businesses', fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Adjust layout and save\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_business_overview.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Visualizations created successfully!\")\n",
        "print(\"   Saved as: eda_business_overview.png\")\n",
        "\n",
        "# Additional: Category analysis if categories exist\n",
        "if 'categories' in business_df.columns:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CATEGORY ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Extract individual categories\n",
        "    all_categories = []\n",
        "    for cats in business_df['categories'].dropna():\n",
        "        if isinstance(cats, str):\n",
        "            all_categories.extend([c.strip() for c in cats.split(',')])\n",
        "\n",
        "    from collections import Counter\n",
        "    category_counts = Counter(all_categories)\n",
        "\n",
        "    print(f\"\\n Total unique categories: {len(category_counts):,}\")\n",
        "    print(\"\\n🔝 Top 15 Categories:\")\n",
        "    for idx, (cat, count) in enumerate(category_counts.most_common(15), 1):\n",
        "        print(f\"   {idx:2d}. {cat:30s} - {count:,} businesses\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" Section 4 Complete - EDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Feature Engineering & Feature Store <a id='section-5'></a>\n",
        "\n",
        "This section:\n",
        "- Engineers features from raw data\n",
        "- Creates parking-related features\n",
        "- Stores features in SageMaker Feature Store\n",
        "- Splits data into train/test/validation sets\n",
        "\n",
        "**Key Features**:\n",
        "- Parking availability indicators\n",
        "- Review aggregations\n",
        "- Business characteristics\n",
        "- Target variable: High rating indicator (4+ stars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-full-data-header",
      "metadata": {
        "id": "load-full-data-header"
      },
      "source": [
        "### 5.1 Load Full Dataset from Athena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-full-data",
      "metadata": {
        "id": "load-full-data"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Query to join business and review data\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    b.business_id,\n",
        "    b.name,\n",
        "    b.city,\n",
        "    b.state,\n",
        "    b.stars as business_stars,\n",
        "    b.review_count as business_review_count,\n",
        "    b.categories,\n",
        "    r.review_id,\n",
        "    r.user_id,\n",
        "    r.stars as review_stars,\n",
        "    r.useful,\n",
        "    r.funny,\n",
        "    r.cool,\n",
        "    r.text as review_text,\n",
        "    r.date as review_date\n",
        "FROM {ATHENA_DB}.business b\n",
        "INNER JOIN {ATHENA_DB}.review r\n",
        "    ON b.business_id = r.business_id\n",
        "WHERE b.is_open = 1\n",
        "    AND b.review_count >= 10\n",
        "    AND r.stars IS NOT NULL\n",
        "LIMIT 100000\n",
        "\"\"\"\n",
        "\n",
        "print(\"⏳ Executing query...\")\n",
        "df = pd.read_sql(query, conn)\n",
        "print(f\"\\n Loaded {len(df):,} reviews from {df['business_id'].nunique():,} businesses\")\n",
        "print(f\"   Date range: {df['review_date'].min()} to {df['review_date'].max()}\")\n",
        "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\n Sample data:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "engineer-features-header",
      "metadata": {
        "id": "engineer-features-header"
      },
      "source": [
        "### 5.2 Engineer Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "engineer-features",
      "metadata": {
        "id": "engineer-features"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract Parking Information from Review Text\n",
        "print(\"\\n1️  Extracting parking information from review text...\")\n",
        "\n",
        "def extract_parking_features(text):\n",
        "    \"\"\"\n",
        "    Extract parking-related features from review text.\n",
        "    Returns a dict with parking indicators.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return {\n",
        "            'mentions_parking': 0,\n",
        "            'parking_positive': 0,\n",
        "            'parking_negative': 0,\n",
        "            'parking_type_lot': 0,\n",
        "            'parking_type_street': 0,\n",
        "            'parking_type_garage': 0,\n",
        "            'parking_type_valet': 0,\n",
        "            'parking_free': 0,\n",
        "            'parking_paid': 0\n",
        "        }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Check if parking is mentioned\n",
        "    parking_keywords = ['parking', 'park', 'parked']\n",
        "    mentions_parking = int(any(keyword in text_lower for keyword in parking_keywords))\n",
        "\n",
        "    # Positive parking indicators\n",
        "    positive_keywords = [\n",
        "        'easy parking', 'plenty of parking', 'ample parking',\n",
        "        'free parking', 'good parking', 'great parking',\n",
        "        'lots of parking', 'parking available', 'easy to park'\n",
        "    ]\n",
        "    parking_positive = int(any(keyword in text_lower for keyword in positive_keywords))\n",
        "\n",
        "    # Negative parking indicators\n",
        "    negative_keywords = [\n",
        "        'no parking', 'parking nightmare', 'hard to park',\n",
        "        'difficult parking', 'parking is terrible', 'parking sucks',\n",
        "        'nowhere to park', 'parking is bad', 'limited parking',\n",
        "        'parking is horrible', 'parking was awful'\n",
        "    ]\n",
        "    parking_negative = int(any(keyword in text_lower for keyword in negative_keywords))\n",
        "\n",
        "    # Parking types\n",
        "    parking_type_lot = int('parking lot' in text_lower or 'lot parking' in text_lower)\n",
        "    parking_type_street = int('street parking' in text_lower or 'park on the street' in text_lower)\n",
        "    parking_type_garage = int('parking garage' in text_lower or 'garage parking' in text_lower)\n",
        "    parking_type_valet = int('valet' in text_lower)\n",
        "\n",
        "    # Cost indicators\n",
        "    parking_free = int('free parking' in text_lower or 'free park' in text_lower)\n",
        "    parking_paid = int('paid parking' in text_lower or 'pay for parking' in text_lower or 'parking fee' in text_lower)\n",
        "\n",
        "    return {\n",
        "        'mentions_parking': mentions_parking,\n",
        "        'parking_positive': parking_positive,\n",
        "        'parking_negative': parking_negative,\n",
        "        'parking_type_lot': parking_type_lot,\n",
        "        'parking_type_street': parking_type_street,\n",
        "        'parking_type_garage': parking_type_garage,\n",
        "        'parking_type_valet': parking_type_valet,\n",
        "        'parking_free': parking_free,\n",
        "        'parking_paid': parking_paid\n",
        "    }\n",
        "\n",
        "# Apply parking extraction\n",
        "parking_features = df['review_text'].apply(extract_parking_features)\n",
        "parking_df = pd.DataFrame(parking_features.tolist())\n",
        "\n",
        "# Add to main dataframe\n",
        "for col in parking_df.columns:\n",
        "    df[col] = parking_df[col]\n",
        "\n",
        "print(f\" Extracted parking features from {len(df):,} reviews\")\n",
        "print(f\"\\n   Parking Statistics:\")\n",
        "print(f\"   - Reviews mentioning parking: {df['mentions_parking'].sum():,} ({df['mentions_parking'].mean()*100:.1f}%)\")\n",
        "print(f\"   - Positive parking mentions: {df['parking_positive'].sum():,}\")\n",
        "print(f\"   - Negative parking mentions: {df['parking_negative'].sum():,}\")\n",
        "print(f\"   - Free parking mentions: {df['parking_free'].sum():,}\")\n",
        "\n",
        "\n",
        "# Step 2: Parse review date and create temporal features\n",
        "print(\"\\n2️  Creating temporal features...\")\n",
        "\n",
        "df['review_date'] = pd.to_datetime(df['review_date'])\n",
        "df['review_year'] = df['review_date'].dt.year\n",
        "df['review_month'] = df['review_date'].dt.month\n",
        "df['review_day_of_week'] = df['review_date'].dt.dayofweek\n",
        "df['review_quarter'] = df['review_date'].dt.quarter\n",
        "\n",
        "print(\" Created temporal features\")\n",
        "\n",
        "\n",
        "# Step 3: Create engagement score\n",
        "print(\"\\n3️  Creating engagement features...\")\n",
        "\n",
        "df['engagement_score'] = df['useful'] + df['funny'] + df['cool']\n",
        "df['is_engaged'] = (df['engagement_score'] > 0).astype(int)\n",
        "\n",
        "print(\" Created engagement features\")\n",
        "\n",
        "\n",
        "# Step 4: Create target variable\n",
        "print(\"\\n4️  Creating target variable...\")\n",
        "\n",
        "df['is_highly_rated'] = (df['review_stars'] >= 4).astype(int)\n",
        "\n",
        "print(f\" Created target variable\")\n",
        "print(f\"   - Highly rated (4+ stars): {df['is_highly_rated'].sum():,} ({df['is_highly_rated'].mean()*100:.1f}%)\")\n",
        "print(f\"   - Not highly rated: {(1-df['is_highly_rated']).sum():,} ({(1-df['is_highly_rated']).mean()*100:.1f}%)\")\n",
        "\n",
        "\n",
        "# Step 5: Business-level aggregations\n",
        "print(\"\\n5️  Creating business-level aggregates...\")\n",
        "\n",
        "business_agg = df.groupby('business_id').agg({\n",
        "    'review_stars': ['mean', 'std', 'count', 'min', 'max'],\n",
        "    'engagement_score': ['mean', 'sum'],\n",
        "    'is_highly_rated': 'mean',\n",
        "    'mentions_parking': ['sum', 'mean'],\n",
        "    'parking_positive': 'sum',\n",
        "    'parking_negative': 'sum',\n",
        "    'parking_type_lot': 'sum',\n",
        "    'parking_type_street': 'sum',\n",
        "    'parking_type_garage': 'sum',\n",
        "    'parking_type_valet': 'sum',\n",
        "    'parking_free': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten column names\n",
        "business_agg.columns = [\n",
        "    'business_id',\n",
        "    'avg_review_stars', 'std_review_stars', 'total_reviews', 'min_review_stars', 'max_review_stars',\n",
        "    'avg_engagement', 'total_engagement',\n",
        "    'pct_highly_rated',\n",
        "    'total_parking_mentions', 'pct_reviews_mention_parking',\n",
        "    'total_positive_parking', 'total_negative_parking',\n",
        "    'total_lot_mentions', 'total_street_mentions', 'total_garage_mentions', 'total_valet_mentions',\n",
        "    'total_free_parking_mentions'\n",
        "]\n",
        "\n",
        "# Create derived parking features\n",
        "business_agg['has_parking_data'] = (business_agg['total_parking_mentions'] > 0).astype(int)\n",
        "business_agg['parking_sentiment'] = (\n",
        "    (business_agg['total_positive_parking'] - business_agg['total_negative_parking']) /\n",
        "    (business_agg['total_parking_mentions'] + 1)  # +1 to avoid division by zero\n",
        ")\n",
        "\n",
        "# Determine dominant parking type\n",
        "business_agg['primary_parking_type'] = business_agg[\n",
        "    ['total_lot_mentions', 'total_street_mentions', 'total_garage_mentions', 'total_valet_mentions']\n",
        "].idxmax(axis=1).str.replace('total_', '').str.replace('_mentions', '')\n",
        "\n",
        "print(f\" Created business-level aggregates\")\n",
        "print(f\"   - Unique businesses: {len(business_agg):,}\")\n",
        "print(f\"   - Businesses with parking data: {business_agg['has_parking_data'].sum():,} ({business_agg['has_parking_data'].mean()*100:.1f}%)\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\n Sample business aggregates:\")\n",
        "display(business_agg.head())\n",
        "\n",
        "\n",
        "# Step 6: Merge aggregates back to reviews\n",
        "print(\"\\n6️  Merging business aggregates back to review data...\")\n",
        "\n",
        "df = df.merge(business_agg, on='business_id', how='left', suffixes=('', '_agg'))\n",
        "\n",
        "print(f\" Merged aggregates - DataFrame now has {len(df.columns)} columns\")\n",
        "\n",
        "\n",
        "# Step 7: Handle categories\n",
        "print(\"\\n7️  Processing business categories...\")\n",
        "\n",
        "# Check if business is a restaurant\n",
        "df['is_restaurant'] = df['categories'].fillna('').str.contains('Restaurant|Food', case=False, na=False).astype(int)\n",
        "\n",
        "# Extract price range from categories (if present)\n",
        "def extract_price_range(categories):\n",
        "    if pd.isna(categories):\n",
        "        return 2  # Default to medium price\n",
        "    # Look for price indicators in categories\n",
        "    if any(word in categories.lower() for word in ['$$$', 'expensive', 'upscale', 'fine dining']):\n",
        "        return 4\n",
        "    elif any(word in categories.lower() for word in ['$$', 'moderate']):\n",
        "        return 3\n",
        "    elif any(word in categories.lower() for word in ['$', 'cheap', 'budget', 'fast food']):\n",
        "        return 1\n",
        "    else:\n",
        "        return 2  # Default\n",
        "\n",
        "df['price_range_numeric'] = df['categories'].apply(extract_price_range)\n",
        "\n",
        "print(f\" Processed categories\")\n",
        "print(f\"   - Restaurants: {df['is_restaurant'].sum():,} ({df['is_restaurant'].mean()*100:.1f}%)\")\n",
        "\n",
        "\n",
        "# Step 8: Create enhanced parking score\n",
        "print(\"\\n8️  Creating enhanced parking score...\")\n",
        "\n",
        "# Weighted parking score combining multiple factors\n",
        "df['enhanced_parking_score'] = (\n",
        "    df['parking_positive'] * 2 +           # Positive mention worth 2 points\n",
        "    df['parking_negative'] * -2 +          # Negative mention -2 points\n",
        "    df['parking_free'] * 1 +               # Free parking worth 1 point\n",
        "    df['parking_type_lot'] * 0.5 +         # Lot parking worth 0.5\n",
        "    df['parking_type_garage'] * 0.5 +      # Garage parking worth 0.5\n",
        "    df['parking_type_valet'] * 0.3 +       # Valet worth 0.3\n",
        "    df['parking_type_street'] * 0.2        # Street parking worth 0.2\n",
        ")\n",
        "\n",
        "# Normalize to 0-10 scale\n",
        "df['enhanced_parking_score'] = (\n",
        "    (df['enhanced_parking_score'] - df['enhanced_parking_score'].min()) /\n",
        "    (df['enhanced_parking_score'].max() - df['enhanced_parking_score'].min()) * 10\n",
        ")\n",
        "\n",
        "print(f\" Created enhanced parking score\")\n",
        "print(f\"   - Mean score: {df['enhanced_parking_score'].mean():.2f}\")\n",
        "print(f\"   - Median score: {df['enhanced_parking_score'].median():.2f}\")\n",
        "\n",
        "\n",
        "# Display final feature summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n Final Dataset Summary:\")\n",
        "print(f\"   Total records: {len(df):,}\")\n",
        "print(f\"   Total features: {len(df.columns)}\")\n",
        "print(f\"   Unique businesses: {df['business_id'].nunique():,}\")\n",
        "print(f\"   Unique users: {df['user_id'].nunique():,}\")\n",
        "print(f\"\\n Target Variable Distribution:\")\n",
        "print(f\"   Highly rated (4+ stars): {df['is_highly_rated'].sum():,} ({df['is_highly_rated'].mean()*100:.1f}%)\")\n",
        "print(f\"   Not highly rated: {(1-df['is_highly_rated']).sum():,} ({(1-df['is_highly_rated']).mean()*100:.1f}%)\")\n",
        "print(f\"\\n Parking Feature Coverage:\")\n",
        "print(f\"   Reviews with parking mentions: {df['mentions_parking'].sum():,} ({df['mentions_parking'].mean()*100:.1f}%)\")\n",
        "print(f\"   Businesses with parking data: {business_agg['has_parking_data'].sum():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prepare-feature-store-header",
      "metadata": {
        "id": "prepare-feature-store-header"
      },
      "source": [
        "### 5.3 Prepare Data for Feature Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prepare-feature-store",
      "metadata": {
        "id": "prepare-feature-store"
      },
      "outputs": [],
      "source": [
        "# Select features for Feature Store\n",
        "feature_columns = [\n",
        "    'review_id',  # Primary key\n",
        "    'business_id',\n",
        "    'user_id',\n",
        "    # Parking features (extracted from review text)\n",
        "    'mentions_parking',\n",
        "    'parking_positive',\n",
        "    'parking_negative',\n",
        "    'parking_type_lot',\n",
        "    'parking_type_street',\n",
        "    'parking_type_garage',\n",
        "    'parking_type_valet',\n",
        "    'parking_free',\n",
        "    'parking_paid',\n",
        "    'enhanced_parking_score',\n",
        "    # Business features (from aggregates)\n",
        "    'business_stars',\n",
        "    'business_review_count',\n",
        "    'avg_review_stars',\n",
        "    'std_review_stars',\n",
        "    'total_reviews',\n",
        "    'avg_engagement',\n",
        "    'pct_highly_rated',\n",
        "    'has_parking_data',\n",
        "    'parking_sentiment',\n",
        "    # Review features\n",
        "    'review_stars',\n",
        "    'useful',\n",
        "    'funny',\n",
        "    'cool',\n",
        "    'engagement_score',\n",
        "    'is_engaged',\n",
        "    'review_year',\n",
        "    'review_month',\n",
        "    'review_quarter',\n",
        "    # Business type\n",
        "    'is_restaurant',\n",
        "    'price_range_numeric',\n",
        "    # Target\n",
        "    'is_highly_rated'\n",
        "]\n",
        "\n",
        "# Create feature store dataframe\n",
        "print(\"\\n1️  Selecting features...\")\n",
        "fs_df = df[feature_columns].copy()\n",
        "\n",
        "# Add event time (required by Feature Store)\n",
        "print(\"\\n2️  Adding event time...\")\n",
        "fs_df['event_time'] = pd.Timestamp.now().isoformat()\n",
        "\n",
        "# Remove any remaining nulls\n",
        "print(\"\\n3️  Handling missing values...\")\n",
        "initial_count = len(fs_df)\n",
        "fs_df = fs_df.dropna()\n",
        "print(f\"   Dropped {initial_count - len(fs_df):,} rows with missing values\")\n",
        "print(f\"   Remaining: {len(fs_df):,} rows\")\n",
        "\n",
        "# Add data split column for train/test/validation\n",
        "print(\"\\n4️  Creating train/test/validation splits...\")\n",
        "np.random.seed(42)\n",
        "fs_df['split'] = np.random.choice(\n",
        "    ['train', 'validation', 'test', 'production'],\n",
        "    size=len(fs_df),\n",
        "    p=[0.4, 0.1, 0.1, 0.4]  # 40% train, 10% val, 10% test, 40% production\n",
        ")\n",
        "\n",
        "print(\"\\n Data split distribution:\")\n",
        "print(fs_df['split'].value_counts().sort_index())\n",
        "print(f\"\\n   Train:      {len(fs_df[fs_df['split']=='train']):,} rows\")\n",
        "print(f\"   Validation: {len(fs_df[fs_df['split']=='validation']):,} rows\")\n",
        "print(f\"   Test:       {len(fs_df[fs_df['split']=='test']):,} rows\")\n",
        "print(f\"   Production: {len(fs_df[fs_df['split']=='production']):,} rows\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\n Feature Store DataFrame sample:\")\n",
        "display(fs_df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" SECTION 5 COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFeature Store DataFrame:\")\n",
        "print(f\"  - Total records: {len(fs_df):,}\")\n",
        "print(f\"  - Total features: {len(feature_columns)}\")\n",
        "print(f\"  - Memory usage: {fs_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "print(f\"\\n Ready to ingest into SageMaker Feature Store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create-feature-store-header",
      "metadata": {
        "id": "create-feature-store-header"
      },
      "source": [
        "### 5.4 Create SageMaker Feature Store\n",
        "\n",
        "Store the engineered features in SageMaker Feature Store for:\n",
        "- Versioned feature access\n",
        "- Online and offline feature serving\n",
        "- Feature reuse across models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config-feature-store",
      "metadata": {
        "id": "config-feature-store"
      },
      "outputs": [],
      "source": [
        "# Feature store configuration using Account ID\n",
        "feature_group_name = f\"venuesignal-features-{account_id}\"\n",
        "feature_store_bucket = f\"s3://{FEATURE_DIR}\"\n",
        "\n",
        "print(f\"Feature Group Name: {feature_group_name}\")\n",
        "print(f\"Offline Store: {feature_store_bucket}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f658f9a9-3123-4ca2-8ad6-a7465552c889",
      "metadata": {
        "id": "f658f9a9-3123-4ca2-8ad6-a7465552c889"
      },
      "outputs": [],
      "source": [
        "# Create feature group\n",
        "feature_group = FeatureGroup(\n",
        "    name=feature_group_name,\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "# Load feature definitions from dataframe\n",
        "feature_group.load_feature_definitions(data_frame=fs_df)\n",
        "\n",
        "print(f\"\\n Feature group configured with {len(fs_df.columns)} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-feature-group",
      "metadata": {
        "id": "create-feature-group"
      },
      "outputs": [],
      "source": [
        "# Create the feature group (if it doesn't exist)\n",
        "try:\n",
        "    feature_group.create(\n",
        "        s3_uri=feature_store_bucket,\n",
        "        record_identifier_name=\"review_id\",\n",
        "        event_time_feature_name=\"event_time\",\n",
        "        role_arn=role,\n",
        "        enable_online_store=False  # Only offline store for this project\n",
        "    )\n",
        "    print(f\" Created feature group: {feature_group_name}\")\n",
        "    print(\"   Waiting for creation to complete (this may take a few minutes)...\")\n",
        "\n",
        "    # Wait for feature group to be created\n",
        "    import time\n",
        "    while True:\n",
        "        status = feature_group.describe()['FeatureGroupStatus']\n",
        "        if status == 'Created':\n",
        "            print(\" Feature group is ready!\")\n",
        "            break\n",
        "        elif status == 'CreateFailed':\n",
        "            print(\" Feature group creation failed\")\n",
        "            break\n",
        "        print(f\"   Status: {status}...\")\n",
        "        time.sleep(30)\n",
        "\n",
        "except Exception as e:\n",
        "    if 'ResourceInUse' in str(e):\n",
        "        print(f\" Feature group '{feature_group_name}' already exists\")\n",
        "    else:\n",
        "        print(f\" Error creating feature group: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ingest-features",
      "metadata": {
        "id": "ingest-features"
      },
      "outputs": [],
      "source": [
        "# Ingest features into feature store\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Fixing event_time format...\")\n",
        "current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "fs_df['event_time'] = current_time  # Use same timestamp for all rows\n",
        "fs_df['event_time'] = fs_df['event_time'].astype(str)  # Ensure string type\n",
        "print(f\" Fixed event_time: {current_time}\")\n",
        "\n",
        "# Now ingest in small batches\n",
        "print(f\"\\nIngesting {len(fs_df):,} records...\")\n",
        "batch_size = 1000\n",
        "successful = 0\n",
        "\n",
        "for i in range(0, len(fs_df), batch_size):\n",
        "    batch = fs_df.iloc[i:i+batch_size].copy()\n",
        "    batch['event_time'] = current_time  # Ensure format\n",
        "\n",
        "    try:\n",
        "        feature_group.ingest(data_frame=batch, max_workers=2, wait=True)\n",
        "        successful += len(batch)\n",
        "        print(f\"   Batch {i//batch_size + 1}: {successful:,} total records\", end='\\r')\n",
        "    except Exception as e:\n",
        "        if 'already' in str(e).lower():\n",
        "            print(f\"    Batch {i//batch_size + 1}: Already ingested\")\n",
        "        else:\n",
        "            print(f\"   Batch {i//batch_size + 1}: {str(e)[:80]}\")\n",
        "\n",
        "print(f\"\\n\\n Ingestion complete! {successful:,} records ingested\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "export-features-header",
      "metadata": {
        "id": "export-features-header"
      },
      "source": [
        "### 5.5 Export Features for Training\n",
        "\n",
        "Export features from Feature Store to S3 for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export-features",
      "metadata": {
        "id": "export-features"
      },
      "outputs": [],
      "source": [
        "# Step 1: Split the data\n",
        "print(\"\\n1️  Splitting data...\")\n",
        "train_df = fs_df[fs_df['split'] == 'train'].drop(columns=['event_time', 'split'])\n",
        "validation_df = fs_df[fs_df['split'] == 'validation'].drop(columns=['event_time', 'split'])\n",
        "test_df = fs_df[fs_df['split'] == 'test'].drop(columns=['event_time', 'split'])\n",
        "production_df = fs_df[fs_df['split'] == 'production'].drop(columns=['event_time', 'split'])\n",
        "\n",
        "print(f\"    Train:      {len(train_df):,} records\")\n",
        "print(f\"    Validation: {len(validation_df):,} records\")\n",
        "print(f\"    Test:       {len(test_df):,} records\")\n",
        "print(f\"    Production: {len(production_df):,} records\")\n",
        "\n",
        "# Step 2: Save locally (SIMPLE path - no complex directories!)\n",
        "print(\"\\n2️  Saving to local files...\")\n",
        "\n",
        "# Just use /tmp directly - simple and always works!\n",
        "train_df.to_csv('/tmp/train.csv', index=False)\n",
        "print(\"    /tmp/train.csv\")\n",
        "\n",
        "validation_df.to_csv('/tmp/validation.csv', index=False)\n",
        "print(\"    /tmp/validation.csv\")\n",
        "\n",
        "test_df.to_csv('/tmp/test.csv', index=False)\n",
        "print(\"    /tmp/test.csv\")\n",
        "\n",
        "production_df.to_csv('/tmp/production.csv', index=False)\n",
        "print(\"    /tmp/production.csv\")\n",
        "\n",
        "# Step 3: Upload to S3\n",
        "print(\"\\n3️  Uploading to S3...\")\n",
        "\n",
        "# Define S3 paths\n",
        "train_data_path = f\"s3://{FEATURE_DIR}training-data/train.csv\"\n",
        "validation_data_path = f\"s3://{FEATURE_DIR}training-data/validation.csv\"\n",
        "test_data_path = f\"s3://{FEATURE_DIR}training-data/test.csv\"\n",
        "production_data_path = f\"s3://{FEATURE_DIR}training-data/production.csv\"\n",
        "\n",
        "# Upload each file\n",
        "uploads = [\n",
        "    ('/tmp/train.csv', train_data_path, 'train.csv'),\n",
        "    ('/tmp/validation.csv', validation_data_path, 'validation.csv'),\n",
        "    ('/tmp/test.csv', test_data_path, 'test.csv'),\n",
        "    ('/tmp/production.csv', production_data_path, 'production.csv')\n",
        "]\n",
        "\n",
        "for local_file, s3_uri, display_name in uploads:\n",
        "    bucket = s3_uri.split('/')[2]\n",
        "    key = '/'.join(s3_uri.split('/')[3:])\n",
        "\n",
        "    try:\n",
        "        s3_client.upload_file(local_file, bucket, key)\n",
        "        print(f\"    {display_name} → {s3_uri}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    {display_name} failed: {e}\")\n",
        "\n",
        "# Step 4: Store variables\n",
        "print(\"\\n4️  Storing variables...\")\n",
        "%store train_data_path\n",
        "%store validation_data_path\n",
        "%store test_data_path\n",
        "%store production_data_path\n",
        "\n",
        "print(\"    Variables stored\")\n",
        "\n",
        "# Step 5: Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" SECTION 5.7 COMPLETE - DATA EXPORTED!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n S3 Locations:\")\n",
        "print(f\"   Train:      {train_data_path}\")\n",
        "print(f\"   Validation: {validation_data_path}\")\n",
        "print(f\"   Test:       {test_data_path}\")\n",
        "print(f\"   Production: {production_data_path}\")\n",
        "\n",
        "print(f\"  Local Files (temporary):\")\n",
        "for filename in ['train.csv', 'validation.csv', 'test.csv', 'production.csv']:\n",
        "    filepath = f'/tmp/{filename}'\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"   {filepath:30s} - {size_mb:6.2f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" SECTION 5 COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n Summary:\")\n",
        "print(f\"    Features engineered from review text\")\n",
        "print(f\"    {len(fs_df):,} total records processed\")\n",
        "print(f\"    Training data split and exported\")\n",
        "print(f\"    All data in S3 and ready for model training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Model Training <a id='section-6'></a>\n",
        "\n",
        "This section trains and evaluates multiple models:\n",
        "\n",
        "1. **Baseline Model #1**: Simple heuristic (business average rating)\n",
        "2. **Baseline Model #2**: Linear regression with key features\n",
        "3. **XGBoost Model**: Gradient boosted trees for classification\n",
        "\n",
        "**Goal**: Predict whether a review will be highly rated (4+ stars) based on business characteristics, especially parking availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae9194d-17bb-4577-9fb0-e8c65ba3c1c1",
      "metadata": {
        "id": "8ae9194d-17bb-4577-9fb0-e8c65ba3c1c1"
      },
      "outputs": [],
      "source": [
        "# Define S3 paths\n",
        "train_data_path = f\"s3://{FEATURE_DIR}training-data/train.csv\"\n",
        "validation_data_path = f\"s3://{FEATURE_DIR}training-data/validation.csv\"\n",
        "test_data_path = f\"s3://{FEATURE_DIR}training-data/test.csv\"\n",
        "production_data_path = f\"s3://{FEATURE_DIR}training-data/production.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-training-data-header",
      "metadata": {
        "id": "load-training-data-header"
      },
      "source": [
        "### 6.1 Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-training-data",
      "metadata": {
        "id": "load-training-data"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load training data\n",
        "print(\"\\n1️  Loading training data...\")\n",
        "train_df = pd.read_csv(train_data_path)\n",
        "validation_df = pd.read_csv(validation_data_path)\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "print(f\"   Training:   {len(train_df):,} records\")\n",
        "print(f\"   Validation: {len(validation_df):,} records\")\n",
        "print(f\"   Test:       {len(test_df):,} records\")\n",
        "\n",
        "# Separate features and targets\n",
        "print(\"\\n2️  Preparing features and targets...\")\n",
        "\n",
        "# Classification target (binary)\n",
        "y_train_class = train_df['is_highly_rated']\n",
        "y_val_class = validation_df['is_highly_rated']\n",
        "y_test_class = test_df['is_highly_rated']\n",
        "\n",
        "# Regression target (actual stars)\n",
        "y_train_stars = train_df['review_stars']\n",
        "y_val_stars = validation_df['review_stars']\n",
        "y_test_stars = test_df['review_stars']\n",
        "\n",
        "print(f\"\\n   Classification target distribution:\")\n",
        "print(f\"   - Highly rated (1): {y_train_class.sum():,} ({y_train_class.mean()*100:.1f}%)\")\n",
        "print(f\"   - Not highly rated (0): {(1-y_train_class).sum():,} ({(1-y_train_class.mean())*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n   Star rating distribution:\")\n",
        "print(f\"   - Mean: {y_train_stars.mean():.2f}\")\n",
        "print(f\"   - Std:  {y_train_stars.std():.2f}\")\n",
        "print(f\"   - Range: {y_train_stars.min():.0f} - {y_train_stars.max():.0f}\")\n",
        "\n",
        "def evaluate_model_comprehensive(y_true_class, y_pred_class, y_true_stars, y_pred_stars,\n",
        "                                   y_pred_proba=None, dataset_name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation with both classification and regression metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true_class: True binary labels (0/1)\n",
        "    - y_pred_class: Predicted binary labels (0/1)\n",
        "    - y_true_stars: True star ratings (1-5)\n",
        "    - y_pred_stars: Predicted star ratings (1-5)\n",
        "    - y_pred_proba: Predicted probabilities (optional, for ROC-AUC)\n",
        "    - dataset_name: Name for display\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Classification Metrics\n",
        "    results['accuracy'] = accuracy_score(y_true_class, y_pred_class)\n",
        "    results['precision'] = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
        "    results['recall'] = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
        "    results['f1'] = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
        "\n",
        "    if y_pred_proba is not None:\n",
        "        results['roc_auc'] = roc_auc_score(y_true_class, y_pred_proba)\n",
        "    else:\n",
        "        results['roc_auc'] = None\n",
        "\n",
        "    # Regression Metrics\n",
        "    results['mse'] = mean_squared_error(y_true_stars, y_pred_stars)\n",
        "    results['rmse'] = np.sqrt(results['mse'])\n",
        "    results['mae'] = mean_absolute_error(y_true_stars, y_pred_stars)\n",
        "    results['r2'] = r2_score(y_true_stars, y_pred_stars)\n",
        "\n",
        "    # Custom Metrics: Within X stars\n",
        "    abs_error = np.abs(y_true_stars - y_pred_stars)\n",
        "    results['within_0.5_stars'] = (abs_error <= 0.5).mean()\n",
        "    results['within_1.0_stars'] = (abs_error <= 1.0).mean()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_results(results, model_name, dataset_name):\n",
        "    \"\"\"Pretty print evaluation results.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{model_name} - {dataset_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    print(f\"\\n Classification Metrics (Binary: Highly Rated vs Not):\")\n",
        "    print(f\"   Accuracy:  {results['accuracy']:.4f}\")\n",
        "    print(f\"   Precision: {results['precision']:.4f}\")\n",
        "    print(f\"   Recall:    {results['recall']:.4f}\")\n",
        "    print(f\"   F1-Score:  {results['f1']:.4f}\")\n",
        "    if results['roc_auc'] is not None:\n",
        "        print(f\"   ROC-AUC:   {results['roc_auc']:.4f}\")\n",
        "\n",
        "    print(f\"\\n Regression Metrics (Star Rating Prediction):\")\n",
        "    print(f\"   MSE:       {results['mse']:.4f}\")\n",
        "    print(f\"   RMSE:      {results['rmse']:.4f}\")\n",
        "    print(f\"   MAE:       {results['mae']:.4f}\")\n",
        "    print(f\"   R²:        {results['r2']:.4f}\")\n",
        "\n",
        "    print(f\"\\n Accuracy Metrics (Star Prediction):\")\n",
        "    print(f\"   Within 0.5 stars: {results['within_0.5_stars']*100:.2f}%\")\n",
        "    print(f\"   Within 1.0 stars: {results['within_1.0_stars']*100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baseline-1-header",
      "metadata": {
        "id": "baseline-1-header"
      },
      "source": [
        "### 6.2 Baseline Model #1: Simple Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baseline-1",
      "metadata": {
        "id": "baseline-1"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BASELINE MODEL #1: Simple Heuristic\")\n",
        "print(\"=\"*80)\n",
        "print(\"Approach: Predict highly_rated if avg_review_stars >= 4.0\")\n",
        "\n",
        "# Classification predictions\n",
        "baseline1_pred_class_train = (train_df['avg_review_stars'] >= 4.0).astype(int)\n",
        "baseline1_pred_class_val = (validation_df['avg_review_stars'] >= 4.0).astype(int)\n",
        "baseline1_pred_class_test = (test_df['avg_review_stars'] >= 4.0).astype(int)\n",
        "\n",
        "# Star predictions (use average directly)\n",
        "baseline1_pred_stars_train = train_df['avg_review_stars']\n",
        "baseline1_pred_stars_val = validation_df['avg_review_stars']\n",
        "baseline1_pred_stars_test = test_df['avg_review_stars']\n",
        "\n",
        "# Evaluate\n",
        "baseline1_results_train = evaluate_model_comprehensive(\n",
        "    y_train_class, baseline1_pred_class_train,\n",
        "    y_train_stars, baseline1_pred_stars_train,\n",
        "    y_pred_proba=None, dataset_name=\"Training\"\n",
        ")\n",
        "\n",
        "baseline1_results_val = evaluate_model_comprehensive(\n",
        "    y_val_class, baseline1_pred_class_val,\n",
        "    y_val_stars, baseline1_pred_stars_val,\n",
        "    y_pred_proba=None, dataset_name=\"Validation\"\n",
        ")\n",
        "\n",
        "baseline1_results_test = evaluate_model_comprehensive(\n",
        "    y_test_class, baseline1_pred_class_test,\n",
        "    y_test_stars, baseline1_pred_stars_test,\n",
        "    y_pred_proba=None, dataset_name=\"Test\"\n",
        ")\n",
        "\n",
        "print_results(baseline1_results_train, \"Baseline #1: Heuristic\", \"Training Set\")\n",
        "print_results(baseline1_results_val, \"Baseline #1: Heuristic\", \"Validation Set\")\n",
        "print_results(baseline1_results_test, \"Baseline #1: Heuristic\", \"Test Set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baseline-2-header",
      "metadata": {
        "id": "baseline-2-header"
      },
      "source": [
        "### 6.3 Baseline Model #2: Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baseline-2",
      "metadata": {
        "id": "baseline-2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Select features\n",
        "baseline2_features = [\n",
        "    'avg_review_stars',\n",
        "    'enhanced_parking_score',\n",
        "    'business_review_count'\n",
        "]\n",
        "\n",
        "print(f\"Features: {', '.join(baseline2_features)}\")\n",
        "\n",
        "# Prepare feature matrices\n",
        "X_train = train_df[baseline2_features].fillna(0)\n",
        "X_val = validation_df[baseline2_features].fillna(0)\n",
        "X_test = test_df[baseline2_features].fillna(0)\n",
        "\n",
        "print(f\"\\n⏳ Training logistic regression...\")\n",
        "baseline2_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "baseline2_model.fit(X_train, y_train_class)\n",
        "print(\" Model trained!\")\n",
        "\n",
        "# Classification predictions\n",
        "baseline2_pred_class_train = baseline2_model.predict(X_train)\n",
        "baseline2_pred_class_val = baseline2_model.predict(X_val)\n",
        "baseline2_pred_class_test = baseline2_model.predict(X_test)\n",
        "\n",
        "# Probabilities\n",
        "baseline2_prob_train = baseline2_model.predict_proba(X_train)[:, 1]\n",
        "baseline2_prob_val = baseline2_model.predict_proba(X_val)[:, 1]\n",
        "baseline2_prob_test = baseline2_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Convert probabilities to star predictions (1-5 scale)\n",
        "# Probability 0-1 maps to stars 1-5\n",
        "def prob_to_stars(prob):\n",
        "    \"\"\"Convert probability to star rating (1-5 scale).\"\"\"\n",
        "    return 1 + (prob * 4)  # Maps 0->1, 0.5->3, 1->5\n",
        "\n",
        "baseline2_pred_stars_train = prob_to_stars(baseline2_prob_train)\n",
        "baseline2_pred_stars_val = prob_to_stars(baseline2_prob_val)\n",
        "baseline2_pred_stars_test = prob_to_stars(baseline2_prob_test)\n",
        "\n",
        "# Evaluate\n",
        "baseline2_results_train = evaluate_model_comprehensive(\n",
        "    y_train_class, baseline2_pred_class_train,\n",
        "    y_train_stars, baseline2_pred_stars_train,\n",
        "    y_pred_proba=baseline2_prob_train, dataset_name=\"Training\"\n",
        ")\n",
        "\n",
        "baseline2_results_val = evaluate_model_comprehensive(\n",
        "    y_val_class, baseline2_pred_class_val,\n",
        "    y_val_stars, baseline2_pred_stars_val,\n",
        "    y_pred_proba=baseline2_prob_val, dataset_name=\"Validation\"\n",
        ")\n",
        "\n",
        "baseline2_results_test = evaluate_model_comprehensive(\n",
        "    y_test_class, baseline2_pred_class_test,\n",
        "    y_test_stars, baseline2_pred_stars_test,\n",
        "    y_pred_proba=baseline2_prob_test, dataset_name=\"Test\"\n",
        ")\n",
        "\n",
        "print_results(baseline2_results_train, \"Baseline #2: Logistic Regression\", \"Training Set\")\n",
        "print_results(baseline2_results_val, \"Baseline #2: Logistic Regression\", \"Validation Set\")\n",
        "print_results(baseline2_results_test, \"Baseline #2: Logistic Regression\", \"Test Set\")\n",
        "\n",
        "# Feature importance\n",
        "print(f\"\\n Feature Importance (Coefficients):\")\n",
        "for feature, coef in zip(baseline2_features, baseline2_model.coef_[0]):\n",
        "    print(f\"   {feature:30s}: {coef:8.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9dc1524-ab3a-495c-8ed7-3c39bb927209",
      "metadata": {
        "id": "a9dc1524-ab3a-495c-8ed7-3c39bb927209"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =============================================================================\n",
        "# COMPREHENSIVE MODEL COMPARISON\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON - VALIDATION SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC',\n",
        "        'MSE', 'RMSE', 'MAE', 'R²',\n",
        "        'Within 0.5★', 'Within 1.0★'\n",
        "    ],\n",
        "    'Baseline #1 (Heuristic)': [\n",
        "        baseline1_results_val['accuracy'],\n",
        "        baseline1_results_val['precision'],\n",
        "        baseline1_results_val['recall'],\n",
        "        baseline1_results_val['f1'],\n",
        "        baseline1_results_val['roc_auc'] if baseline1_results_val['roc_auc'] else 0,\n",
        "        baseline1_results_val['mse'],\n",
        "        baseline1_results_val['rmse'],\n",
        "        baseline1_results_val['mae'],\n",
        "        baseline1_results_val['r2'],\n",
        "        baseline1_results_val['within_0.5_stars'],\n",
        "        baseline1_results_val['within_1.0_stars']\n",
        "    ],\n",
        "    'Baseline #2 (LogReg)': [\n",
        "        baseline2_results_val['accuracy'],\n",
        "        baseline2_results_val['precision'],\n",
        "        baseline2_results_val['recall'],\n",
        "        baseline2_results_val['f1'],\n",
        "        baseline2_results_val['roc_auc'],\n",
        "        baseline2_results_val['mse'],\n",
        "        baseline2_results_val['rmse'],\n",
        "        baseline2_results_val['mae'],\n",
        "        baseline2_results_val['r2'],\n",
        "        baseline2_results_val['within_0.5_stars'],\n",
        "        baseline2_results_val['within_1.0_stars']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n Validation Set Performance:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Highlight best scores\n",
        "print(\"\\n Best Scores (Validation Set):\")\n",
        "print(\"\\nClassification Metrics:\")\n",
        "print(f\"   Best Accuracy:  {max(baseline1_results_val['accuracy'], baseline2_results_val['accuracy']):.4f}\")\n",
        "print(f\"   Best F1-Score:  {max(baseline1_results_val['f1'], baseline2_results_val['f1']):.4f}\")\n",
        "\n",
        "print(\"\\nRegression Metrics:\")\n",
        "print(f\"   Best RMSE:      {min(baseline1_results_val['rmse'], baseline2_results_val['rmse']):.4f}\")\n",
        "print(f\"   Best R²:        {max(baseline1_results_val['r2'], baseline2_results_val['r2']):.4f}\")\n",
        "\n",
        "print(\"\\nPrediction Accuracy:\")\n",
        "print(f\"   Best Within 0.5★: {max(baseline1_results_val['within_0.5_stars'], baseline2_results_val['within_0.5_stars'])*100:.2f}%\")\n",
        "print(f\"   Best Within 1.0★: {max(baseline1_results_val['within_1.0_stars'], baseline2_results_val['within_1.0_stars'])*100:.2f}%\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STORE RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "# Store for later comparison with XGBoost\n",
        "baseline_results = {\n",
        "    'baseline1': {\n",
        "        'train': baseline1_results_train,\n",
        "        'val': baseline1_results_val,\n",
        "        'test': baseline1_results_test\n",
        "    },\n",
        "    'baseline2': {\n",
        "        'train': baseline2_results_train,\n",
        "        'val': baseline2_results_val,\n",
        "        'test': baseline2_results_test\n",
        "    }\n",
        "}\n",
        "\n",
        "%store baseline_results\n",
        "%store baseline2_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xgboost-header",
      "metadata": {
        "id": "xgboost-header"
      },
      "source": [
        "### 6.4 XGBoost Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53cdfebf-547d-4a06-9be6-88313909371b",
      "metadata": {
        "id": "53cdfebf-547d-4a06-9be6-88313909371b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.inputs import TrainingInput\n",
        "from sagemaker.serializers import CSVSerializer\n",
        "from sagemaker.deserializers import JSONDeserializer\n",
        "import boto3\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "print(\"\\n1️  Configuring XGBoost training...\")\n",
        "\n",
        "# Select features for XGBoost (more features than baseline)\n",
        "xgb_features = [\n",
        "    # Business features\n",
        "    'avg_review_stars',\n",
        "    'std_review_stars',\n",
        "    'business_review_count',\n",
        "    'pct_highly_rated',\n",
        "    # Parking features\n",
        "    'enhanced_parking_score',\n",
        "    'parking_positive',\n",
        "    'parking_negative',\n",
        "    'parking_sentiment',\n",
        "    'has_parking_data',\n",
        "    # Review engagement\n",
        "    'avg_engagement',\n",
        "    # Business attributes\n",
        "    'is_restaurant',\n",
        "    'price_range_numeric'\n",
        "]\n",
        "\n",
        "print(f\"   Selected {len(xgb_features)} features for XGBoost\")\n",
        "print(f\"\\n   Features:\")\n",
        "for i, feat in enumerate(xgb_features, 1):\n",
        "    print(f\"   {i:2d}. {feat}\")\n",
        "\n",
        "# Prepare data for XGBoost (target must be first column, no header)\n",
        "print(\"\\n2️  Preparing data for XGBoost format...\")\n",
        "\n",
        "def prepare_xgb_data(df, features, target='is_highly_rated'):\n",
        "    \"\"\"Prepare data in XGBoost format: target first, no header.\"\"\"\n",
        "    # Select features and fill missing values\n",
        "    X = df[features].fillna(0)\n",
        "    y = df[target]\n",
        "\n",
        "    # Combine with target first\n",
        "    xgb_data = pd.concat([y, X], axis=1)\n",
        "\n",
        "    return xgb_data\n",
        "\n",
        "# Prepare datasets\n",
        "train_xgb = prepare_xgb_data(train_df, xgb_features)\n",
        "val_xgb = prepare_xgb_data(validation_df, xgb_features)\n",
        "\n",
        "print(f\"   Training data shape: {train_xgb.shape}\")\n",
        "print(f\"   Validation data shape: {val_xgb.shape}\")\n",
        "\n",
        "# Save locally\n",
        "print(\"\\n3️  Saving XGBoost-formatted data...\")\n",
        "train_xgb.to_csv('/tmp/train_xgb.csv', index=False, header=False)\n",
        "val_xgb.to_csv('/tmp/val_xgb.csv', index=False, header=False)\n",
        "print(\"    Data saved in XGBoost format\")\n",
        "\n",
        "# Upload to S3\n",
        "print(\"\\n4️  Uploading to S3...\")\n",
        "xgb_train_path = f\"s3://{MODEL_DIR}xgboost-training/train.csv\"\n",
        "xgb_val_path = f\"s3://{MODEL_DIR}xgboost-training/validation.csv\"\n",
        "xgb_output_path = f\"s3://{MODEL_DIR}xgboost-output\"\n",
        "\n",
        "bucket = BASE_BUCKET_NAME\n",
        "s3_client.upload_file('/tmp/train_xgb.csv', bucket, f'{MODEL_PREFIX}xgboost-training/train.csv')\n",
        "s3_client.upload_file('/tmp/val_xgb.csv', bucket, f'{MODEL_PREFIX}xgboost-training/validation.csv')\n",
        "\n",
        "print(f\"    Training data: {xgb_train_path}\")\n",
        "print(f\"    Validation data: {xgb_val_path}\")\n",
        "print(f\"    Output path: {xgb_output_path}\")\n",
        "\n",
        "# Get XGBoost container\n",
        "print(\"\\n5️  Getting XGBoost container...\")\n",
        "from sagemaker.image_uris import retrieve\n",
        "\n",
        "container = retrieve('xgboost', REGION, version='1.5-1')\n",
        "print(f\"   Container: {container}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1546b9d1-e9a6-462b-8594-076bfefad325",
      "metadata": {
        "id": "1546b9d1-e9a6-462b-8594-076bfefad325"
      },
      "outputs": [],
      "source": [
        "# Create estimator\n",
        "print(\"\\n6️  Creating XGBoost estimator...\")\n",
        "\n",
        "xgb_estimator = sagemaker.estimator.Estimator(\n",
        "    container,\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type='ml.m5.xlarge',\n",
        "    output_path=xgb_output_path,\n",
        "    sagemaker_session=sagemaker_session,\n",
        "    base_job_name='venuesignal-xgboost'\n",
        ")\n",
        "\n",
        "# Set hyperparameters\n",
        "xgb_estimator.set_hyperparameters(\n",
        "    objective='binary:logistic',\n",
        "    num_round=100,\n",
        "    max_depth=6,\n",
        "    eta=0.3,\n",
        "    gamma=0,\n",
        "    min_child_weight=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric='auc',\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "print(\"    Estimator configured\")\n",
        "print(f\"\\n   Hyperparameters:\")\n",
        "print(f\"   - Objective: binary:logistic\")\n",
        "print(f\"   - Num rounds: 100\")\n",
        "print(f\"   - Max depth: 6\")\n",
        "print(f\"   - Learning rate (eta): 0.3\")\n",
        "print(f\"   - Early stopping: 10 rounds\")\n",
        "\n",
        "# Create training input channels\n",
        "print(\"\\n7️  Creating training input channels...\")\n",
        "\n",
        "train_input = TrainingInput(xgb_train_path, content_type='text/csv')\n",
        "val_input = TrainingInput(xgb_val_path, content_type='text/csv')\n",
        "\n",
        "# Train model\n",
        "print(\"\\n8️  Starting XGBoost training...\")\n",
        "print(f\"   Training on {len(train_xgb):,} records\")\n",
        "print(f\"   Validating on {len(val_xgb):,} records\")\n",
        "\n",
        "xgb_estimator.fit({\n",
        "    'train': train_input,\n",
        "    'validation': val_input\n",
        "})\n",
        "\n",
        "print(\"\\n XGBoost training complete!\")\n",
        "print(f\"   Model artifacts: {xgb_estimator.model_data}\")\n",
        "\n",
        "# Store model data path\n",
        "xgb_model_data = xgb_estimator.model_data\n",
        "%store xgb_model_data\n",
        "%store xgb_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb131fe5-ba9c-4cae-94d3-b551a1cafb8c",
      "metadata": {
        "id": "fb131fe5-ba9c-4cae-94d3-b551a1cafb8c"
      },
      "outputs": [],
      "source": [
        "# Section 6.3: XGBoost Evaluation\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 6.3: XGBOOST MODEL EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# For evaluation, we need to make predictions locally\n",
        "# Download the model and make predictions\n",
        "\n",
        "print(\"\\n Making predictions with trained XGBoost model...\")\n",
        "print(\"   (Note: In production, this would use SageMaker endpoint)\")\n",
        "\n",
        "# Alternative: Use local XGBoost with same hyperparameters\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"\\n1️  Training local XGBoost for evaluation...\")\n",
        "\n",
        "# Prepare data for local XGBoost\n",
        "X_train_xgb = train_df[xgb_features].fillna(0)\n",
        "X_val_xgb = validation_df[xgb_features].fillna(0)\n",
        "X_test_xgb = test_df[xgb_features].fillna(0)\n",
        "\n",
        "# Train local model with same hyperparameters\n",
        "xgb_local = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.3,\n",
        "    gamma=0,\n",
        "    min_child_weight=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric='auc',\n",
        "    early_stopping_rounds=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_local.fit(\n",
        "    X_train_xgb, y_train_class,\n",
        "    eval_set=[(X_val_xgb, y_val_class)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\" Local XGBoost model trained\")\n",
        "\n",
        "# Make predictions\n",
        "print(\"\\n2️  Making predictions...\")\n",
        "\n",
        "# Classification predictions\n",
        "xgb_pred_class_train = xgb_local.predict(X_train_xgb)\n",
        "xgb_pred_class_val = xgb_local.predict(X_val_xgb)\n",
        "xgb_pred_class_test = xgb_local.predict(X_test_xgb)\n",
        "\n",
        "# Probabilities\n",
        "xgb_prob_train = xgb_local.predict_proba(X_train_xgb)[:, 1]\n",
        "xgb_prob_val = xgb_local.predict_proba(X_val_xgb)[:, 1]\n",
        "xgb_prob_test = xgb_local.predict_proba(X_test_xgb)[:, 1]\n",
        "\n",
        "# Convert to star predictions\n",
        "xgb_pred_stars_train = prob_to_stars(xgb_prob_train)\n",
        "xgb_pred_stars_val = prob_to_stars(xgb_prob_val)\n",
        "xgb_pred_stars_test = prob_to_stars(xgb_prob_test)\n",
        "\n",
        "print(\" Predictions complete\")\n",
        "\n",
        "# Comprehensive evaluation\n",
        "print(\"\\n3️  Evaluating XGBoost model...\")\n",
        "\n",
        "xgb_results_train = evaluate_model_comprehensive(\n",
        "    y_train_class, xgb_pred_class_train,\n",
        "    y_train_stars, xgb_pred_stars_train,\n",
        "    y_pred_proba=xgb_prob_train, dataset_name=\"Training\"\n",
        ")\n",
        "\n",
        "xgb_results_val = evaluate_model_comprehensive(\n",
        "    y_val_class, xgb_pred_class_val,\n",
        "    y_val_stars, xgb_pred_stars_val,\n",
        "    y_pred_proba=xgb_prob_val, dataset_name=\"Validation\"\n",
        ")\n",
        "\n",
        "xgb_results_test = evaluate_model_comprehensive(\n",
        "    y_test_class, xgb_pred_class_test,\n",
        "    y_test_stars, xgb_pred_stars_test,\n",
        "    y_pred_proba=xgb_prob_test, dataset_name=\"Test\"\n",
        ")\n",
        "\n",
        "print_results(xgb_results_train, \"XGBoost Model\", \"Training Set\")\n",
        "print_results(xgb_results_val, \"XGBoost Model\", \"Validation Set\")\n",
        "print_results(xgb_results_test, \"XGBoost Model\", \"Test Set\")\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"XGBOOST FEATURE IMPORTANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "importance_dict = dict(zip(xgb_features, xgb_local.feature_importances_))\n",
        "importance_sorted = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\n Top 10 Most Important Features:\")\n",
        "for i, (feature, importance) in enumerate(importance_sorted[:10], 1):\n",
        "    bar_length = int(importance * 50)\n",
        "    bar = \"█\" * bar_length\n",
        "    print(f\"   {i:2d}. {feature:30s} {bar} {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Model Deployment <a id='section-7'></a>\n",
        "\n",
        "This section:\n",
        "- Registers the XGBoost model in SageMaker Model Registry\n",
        "- Creates a SageMaker endpoint for real-time inference\n",
        "- Tests the deployed model\n",
        "\n",
        "**Deployment Strategy**: Real-time endpoint for individual predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32fac225-3cbe-4331-b8b3-258b1d01bf34",
      "metadata": {
        "id": "32fac225-3cbe-4331-b8b3-258b1d01bf34"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from time import sleep, gmtime, strftime, time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#sklearn\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import f1_score, precision_recall_curve, auc, precision_score, recall_score\n",
        "\n",
        "# Sagemaker imports\n",
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "# Initialize clients\n",
        "sm = boto3.client('sagemaker', region_name=REGION)\n",
        "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff884da-2ee2-44d9-9a8b-5efdccf7615b",
      "metadata": {
        "id": "eff884da-2ee2-44d9-9a8b-5efdccf7615b"
      },
      "source": [
        "### Section 7.1 Prepare Model Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d641975c-7872-4361-8a4e-015535c19ca2",
      "metadata": {
        "id": "d641975c-7872-4361-8a4e-015535c19ca2"
      },
      "outputs": [],
      "source": [
        "#@title Section 7.1 Prepare Model Metadata\n",
        "# Get training job details\n",
        "model_name = xgb_estimator.latest_training_job.name\n",
        "print(f\"\\n📋 Model Information:\")\n",
        "print(f\"   Training job name: {model_name}\")\n",
        "\n",
        "# Get model artifacts\n",
        "info = sm.describe_training_job(TrainingJobName=model_name)\n",
        "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
        "print(f\"   Model artifacts: {model_data}\")\n",
        "\n",
        "# Get XGBoost container image\n",
        "from sagemaker.image_uris import retrieve\n",
        "image = retrieve('xgboost', REGION, version='1.7-1')\n",
        "print(f\"   Container image: {image}\")\n",
        "\n",
        "# Datacapture URI\n",
        "data_capture_prefix = f\"{MODEL_PREFIX}datacapture\"\n",
        "s3_capture_upload_path = f\"s3://{BASE_BUCKET_NAME}/{data_capture_prefix}\"\n",
        "\n",
        "print(f\"   S3 Datacapture URI: {s3_capture_upload_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb892608-8915-41bb-81e3-5ddaa0fdc003",
      "metadata": {
        "id": "eb892608-8915-41bb-81e3-5ddaa0fdc003"
      },
      "source": [
        "### Section 7.2: Create SageMaker Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "829242be-c71c-46ab-bca3-aa53331e165d",
      "metadata": {
        "id": "829242be-c71c-46ab-bca3-aa53331e165d",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#@title Section 7.2: Create SageMaker Model\n",
        "print(f\"\\n Creating SageMaker model: {model_name}\")\n",
        "\n",
        "# Define primary container\n",
        "primary_container = {\n",
        "    'Image': image,\n",
        "    'ModelDataUrl': model_data\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Create model in SageMaker Model Registry\n",
        "    create_model_response = sm.create_model(\n",
        "        ModelName=model_name,\n",
        "        ExecutionRoleArn=role,\n",
        "        PrimaryContainer=primary_container\n",
        "    )\n",
        "\n",
        "    print(f\" Model created successfully!\")\n",
        "    print(f\"   Model ARN: {create_model_response['ModelArn']}\")\n",
        "    model_created = True\n",
        "\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    if 'already exists' in error_msg.lower():\n",
        "        print(f\"  Model already exists, will use existing model\")\n",
        "        model_created = True\n",
        "    else:\n",
        "        print(f\" Error creating model: {error_msg[:200]}\")\n",
        "        model_created = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d1184b-c3f0-4028-80c7-9332bc6cc273",
      "metadata": {
        "id": "50d1184b-c3f0-4028-80c7-9332bc6cc273"
      },
      "source": [
        "### Section 7.3: Create the custom inference script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ada21221-2afd-4b9e-a955-3a6ba3c129a2",
      "metadata": {
        "id": "ada21221-2afd-4b9e-a955-3a6ba3c129a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/tmp/venuesignal_model\", exist_ok=True)\n",
        "\n",
        "inference_script = '''\n",
        "import os\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    model_file = os.path.join(model_dir, \"xgboost-model\")\n",
        "    model = xgb.Booster()\n",
        "    model.load_model(model_file)\n",
        "    return model\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    if request_content_type == \"text/csv\":\n",
        "        data = np.array([\n",
        "            [float(x) for x in row.split(\",\")]\n",
        "            for row in request_body.strip().split(\"\\\\n\")\n",
        "            if row.strip()\n",
        "        ])\n",
        "        return xgb.DMatrix(data)\n",
        "    raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    return model.predict(input_data)\n",
        "\n",
        "def output_fn(predictions, accept):\n",
        "    \"\"\"\n",
        "    Return TWO columns per row: probability,predicted_class\n",
        "    - _c0 = probability  (float)\n",
        "    - _c1 = predicted class (0 or 1)\n",
        "    This prevents the Model Quality Monitor Spark job from\n",
        "    treating the single probability column as the join ID,\n",
        "    which causes endpointOutput_0 to not exist.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for p in predictions:\n",
        "        prob = float(p)\n",
        "        pred_class = 1 if prob >= 0.5 else 0\n",
        "        lines.append(f\"{prob},{pred_class}\")\n",
        "    return \"\\\\n\".join(lines), \"text/csv\"\n",
        "'''\n",
        "\n",
        "script_path = \"/tmp/venuesignal_model/inference.py\"\n",
        "with open(script_path, \"w\") as f:\n",
        "    f.write(inference_script)\n",
        "\n",
        "print(f\"✓ Inference script written: {script_path}\")\n",
        "print(\"  output_fn now returns: probability,predicted_class (2 columns)\")\n",
        "print(\"  _c0 = probability, _c1 = predicted class\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "variant-name-definition",
      "metadata": {
        "id": "variant-name-definition"
      },
      "outputs": [],
      "source": [
        "# variant_name will be set after deployment in Cell 85\n",
        "# hardcoding here is a placeholder only\n",
        "variant_name = 'venuesignal-xgboost'\n",
        "print(f\"Placeholder variant name: {variant_name}\")\n",
        "print(\"Note: actual variant name will be confirmed after deployment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65270a28-7474-457f-b503-cee97d7c81b7",
      "metadata": {
        "id": "65270a28-7474-457f-b503-cee97d7c81b7"
      },
      "source": [
        "### Section 7.4: Deploy Model to Real-Time Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb19211-6cda-4ae8-8acb-73d6c9728f4d",
      "metadata": {
        "id": "9eb19211-6cda-4ae8-8acb-73d6c9728f4d"
      },
      "outputs": [],
      "source": [
        "# Delete old endpoint first\n",
        "try:\n",
        "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
        "    print(f\"✓ Deleting old endpoint: {endpoint_name}\")\n",
        "    waiter = sagemaker_client.get_waiter(\"endpoint_deleted\")\n",
        "    waiter.wait(EndpointName=endpoint_name)\n",
        "    print(\"✓ Endpoint deleted\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "from sagemaker.xgboost import XGBoostModel\n",
        "\n",
        "script_s3_uri = f\"s3://{BASE_BUCKET_NAME}/{MODEL_PREFIX}code/\"\n",
        "S3Uploader.upload(\"/tmp/venuesignal_model/inference.py\", script_s3_uri)\n",
        "print(f\"\\n✓ Inference script uploaded to: {script_s3_uri}\")\n",
        "\n",
        "print(\"\\nDeploying model with custom inference script...\")\n",
        "print(\"This takes ~5-10 minutes...\")\n",
        "\n",
        "xgb_model = XGBoostModel(\n",
        "    model_data=xgb_model_data,\n",
        "    role=role,\n",
        "    entry_point=\"inference.py\",\n",
        "    source_dir=\"/tmp/venuesignal_model\",\n",
        "    framework_version=\"1.7-1\",\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "endpoint_name = (\n",
        "    f\"venuesignal-endpoint-{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
        ")\n",
        "\n",
        "predictor = xgb_model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    data_capture_config=sagemaker.model_monitor.DataCaptureConfig(\n",
        "        enable_capture=True,\n",
        "        sampling_percentage=100,\n",
        "        destination_s3_uri=s3_capture_upload_path,\n",
        "        capture_options=[\"Input\", \"Output\"],\n",
        "        csv_content_types=[\"text/csv\"],\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"✓ Endpoint deployed: {endpoint_name}\")\n",
        "%store endpoint_name\n",
        "\n",
        "print(\"\\nSending initial traffic...\")\n",
        "runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
        "for i in range(20):\n",
        "    payload = (\n",
        "        production_df[xgb_features]\n",
        "        .iloc[i % len(production_df)]\n",
        "        .fillna(0)\n",
        "        .to_csv(header=None, index=False)\n",
        "        .strip()\n",
        "    )\n",
        "    runtime.invoke_endpoint(\n",
        "        EndpointName=endpoint_name,\n",
        "        ContentType=\"text/csv\",\n",
        "        Body=payload,\n",
        "        InferenceId=str(i),\n",
        "    )\n",
        "    sleep(1)\n",
        "print(\"✓ 20 requests sent — endpoint ready\")\n",
        "\n",
        "endpoint_deployed = True\n",
        "# Override variant_name with actual deployed value\n",
        "ep_desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
        "variant_name = ep_desc[\"ProductionVariants\"][0][\"VariantName\"]\n",
        "print(f\"✓ Actual variant name: {variant_name}\")\n",
        "%store endpoint_name\n",
        "%store variant_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bafa70e7-a3b2-4412-ab35-0011e045aaf6",
      "metadata": {
        "id": "bafa70e7-a3b2-4412-ab35-0011e045aaf6"
      },
      "source": [
        "### Section 7.5: Test Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f2352a-0bfa-4302-869a-08b2155dfc3e",
      "metadata": {
        "id": "57f2352a-0bfa-4302-869a-08b2155dfc3e"
      },
      "outputs": [],
      "source": [
        "#@title Section 7.5: Test Endpoint\n",
        "if endpoint_deployed:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SECTION 7.5: TEST ENDPOINT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n Testing endpoint with sample predictions...\")\n",
        "\n",
        "    # Prepare test sample (features only, no header)\n",
        "    test_sample = test_df[xgb_features].head(10).fillna(0)\n",
        "    test_csv = test_sample.to_csv(header=None, index=False).strip('\\n').split('\\n')\n",
        "\n",
        "    try:\n",
        "        # Test with first sample\n",
        "        invoke_endpoint_response = sagemaker_runtime.invoke_endpoint(\n",
        "            EndpointName=endpoint_name,\n",
        "            ContentType='text/csv',\n",
        "            Body=test_csv[0]\n",
        "        )\n",
        "\n",
        "        prediction = invoke_endpoint_response['Body'].read().decode('utf-8')\n",
        "        print(f\" Endpoint responding successfully!\")\n",
        "        print(f\"   Sample prediction: {prediction}\")\n",
        "\n",
        "        # Test with multiple samples\n",
        "        print(f\"\\n Testing with {len(test_csv)} samples...\")\n",
        "\n",
        "        body = \"\"\n",
        "        for row in test_csv:\n",
        "            body += row + \"\\n\"\n",
        "\n",
        "        response = sagemaker_runtime.invoke_endpoint(\n",
        "            EndpointName=endpoint_name,\n",
        "            ContentType='text/csv',\n",
        "            Body=body\n",
        "        )\n",
        "\n",
        "        predictions_str = response['Body'].read().decode('utf-8')\n",
        "        predictions = [float(val) for val in predictions_str.strip().split(\"\\n\") if val.strip()]\n",
        "\n",
        "        print(f\" Received {len(predictions)} predictions\")\n",
        "        print(f\"\\n   Sample predictions:\")\n",
        "        for i, pred in enumerate(predictions[:5]):\n",
        "            print(f\"   Sample {i+1}: {pred:.4f} (probability)\")\n",
        "\n",
        "        # Store endpoint info\n",
        "        %store endpoint_name\n",
        "\n",
        "        print(f\"\\n Endpoint deployed and tested successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error invoking endpoint: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "756249b5-175b-45ca-881e-047be7fc5196",
      "metadata": {
        "id": "756249b5-175b-45ca-881e-047be7fc5196"
      },
      "source": [
        "### Section 7.6: Full Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb5ccc8-08ac-4a0d-93bc-184d5bc6af6b",
      "metadata": {
        "id": "ccb5ccc8-08ac-4a0d-93bc-184d5bc6af6b"
      },
      "outputs": [],
      "source": [
        "#@title Section 7.6: Full Test Set Evaluation\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 7.6: FULL TEST SET EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load test data\n",
        "X_test_eval = test_df[xgb_features].fillna(0)\n",
        "y_test_eval = test_df['is_highly_rated']\n",
        "y_test_stars = test_df['review_stars']\n",
        "\n",
        "if endpoint_deployed:\n",
        "    print(\"\\n Generating predictions using deployed endpoint...\")\n",
        "\n",
        "    try:\n",
        "        # Prepare test data\n",
        "        test_csv_full = X_test_eval.to_csv(header=None, index=False).strip('\\n').split('\\n')\n",
        "\n",
        "        # Create request body\n",
        "        body = \"\"\n",
        "        for row in test_csv_full:\n",
        "            body += row + \"\\n\"\n",
        "\n",
        "        # Invoke endpoint\n",
        "        response = sagemaker_runtime.invoke_endpoint(\n",
        "            EndpointName=endpoint_name,\n",
        "            ContentType='text/csv',\n",
        "            Body=body\n",
        "        )\n",
        "\n",
        "        # Parse predictions\n",
        "        predictions_str = response['Body'].read().decode('utf-8')\n",
        "        predictions_proba = np.array([float(val) for val in predictions_str.split().split(\"\\n\") if val.strip()])\n",
        "        predictions_raw = predictions_str.strip().split(\"\\n\")\n",
        "        predictions_class = [float(val) for val in predictions_raw if val.strip()]\n",
        "        predictions_stars = prob_to_stars(predictions_proba)\n",
        "\n",
        "\n",
        "        print(f\" Generated {len(predictions_proba):,} predictions from endpoint\")\n",
        "        prediction_source = \"SageMaker Endpoint\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Endpoint invocation failed: {e}\")\n",
        "        print(\"   Falling back to local predictions...\")\n",
        "\n",
        "        predictions_class = xgb_local.predict(X_test_eval)\n",
        "        predictions_proba = xgb_local.predict_proba(X_test_eval)[:, 1]\n",
        "        predictions_stars = prob_to_stars(predictions_proba)\n",
        "        prediction_source = \"Local Model\"\n",
        "else:\n",
        "    print(\"\\n Generating predictions using local model...\")\n",
        "\n",
        "    predictions_class = xgb_local.predict(X_test_eval)\n",
        "    predictions_proba = xgb_local.predict_proba(X_test_eval)[:, 1]\n",
        "    predictions_stars = prob_to_stars(predictions_proba)\n",
        "\n",
        "    print(f\" Generated {len(predictions_proba):,} predictions locally\")\n",
        "    prediction_source = \"Local Model\"\n",
        "\n",
        "# Evaluate\n",
        "print(f\"\\n Test Set Performance (using {prediction_source}):\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
        "\n",
        "accuracy = accuracy_score(y_test_eval, predictions_class)\n",
        "precision = precision_score(y_test_eval, predictions_class)\n",
        "recall = recall_score(y_test_eval, predictions_class)\n",
        "f1 = f1_score(y_test_eval, predictions_class)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_stars, predictions_stars))\n",
        "mae = np.mean(np.abs(y_test_stars - predictions_stars))\n",
        "within_05 = (np.abs(y_test_stars - predictions_stars) <= 0.5).mean()\n",
        "within_10 = (np.abs(y_test_stars - predictions_stars) <= 1.0).mean()\n",
        "\n",
        "print(f\"\\n   Classification Metrics:\")\n",
        "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"   Precision: {precision:.4f}\")\n",
        "print(f\"   Recall:    {recall:.4f}\")\n",
        "print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "print(f\"\\n   Regression Metrics:\")\n",
        "print(f\"   RMSE:      {rmse:.4f} stars\")\n",
        "print(f\"   MAE:       {mae:.4f} stars\")\n",
        "\n",
        "print(f\"\\n   Prediction Accuracy:\")\n",
        "print(f\"   Within 0.5 star: {within_05*100:.2f}%\")\n",
        "print(f\"   Within 1.0 star: {within_10*100:.2f}%\")\n",
        "\n",
        "# Save predictions\n",
        "print(f\"\\n Saving predictions...\")\n",
        "\n",
        "predictions_df = pd.DataFrame({\n",
        "    'business_id': test_df['business_id'],\n",
        "    'review_id': test_df['review_id'],\n",
        "    'actual_stars': y_test_stars,\n",
        "    'is_highly_rated': y_test_class,\n",
        "    'predicted_stars': predictions_stars,\n",
        "    'predicted_class': predictions_class,\n",
        "    'probability': predictions_proba,\n",
        "    'error': np.abs(y_test_stars - predictions_stars),\n",
        "    'within_1_star': np.abs(y_test_stars - predictions_stars) <= 1.0\n",
        "})\n",
        "\n",
        "# Save locally and to S3\n",
        "predictions_local = '/tmp/test_predictions.csv'\n",
        "predictions_df.to_csv(predictions_local, index=False)\n",
        "\n",
        "predictions_s3 = f\"s3://{MODEL_DIR}predictions/test_predictions.csv\"\n",
        "s3_client.upload_file(predictions_local, BASE_BUCKET_NAME, f'{MODEL_PREFIX}predictions/test_predictions.csv')\n",
        "\n",
        "print(f\"    Saved to: {predictions_s3}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\n Sample Predictions:\")\n",
        "display(predictions_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1e7262-ca75-465e-a9d4-3139e55fdc3a",
      "metadata": {
        "id": "4f1e7262-ca75-465e-a9d4-3139e55fdc3a"
      },
      "source": [
        "### Section 7.7: Create Model Package Group (Model Registry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f658c9-752e-4a95-90ec-77c696c1bae9",
      "metadata": {
        "id": "16f658c9-752e-4a95-90ec-77c696c1bae9"
      },
      "outputs": [],
      "source": [
        "model_package_group_name = f\"venuesignal-model-group-{account_id}\"\n",
        "model_description = \"VenueSignal model package group: predicts business ratings based on parking features\"\n",
        "\n",
        "print(f\"\\n Creating model package group...\")\n",
        "print(f\"   Name: {model_package_group_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c816bd-5b07-45d0-b372-fd476000ac6f",
      "metadata": {
        "id": "63c816bd-5b07-45d0-b372-fd476000ac6f"
      },
      "outputs": [],
      "source": [
        "#@title Section 7.7: Create Model Package Group (Model Registry)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 7.7: CREATE MODEL PACKAGE GROUP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    model_package_group_input_dict = {\n",
        "        'ModelPackageGroupName': model_package_group_name,\n",
        "        'ModelPackageGroupDescription': model_description\n",
        "    }\n",
        "\n",
        "    create_model_package_group_response = sm.create_model_package_group(\n",
        "        **model_package_group_input_dict\n",
        "    )\n",
        "\n",
        "    print(f\" Model package group created!\")\n",
        "    print(f\"   ARN: {create_model_package_group_response['ModelPackageGroupArn']}\")\n",
        "\n",
        "    model_package_group_created = True\n",
        "\n",
        "except Exception as e:\n",
        "    if 'already exists' in str(e).lower():\n",
        "        print(f\"  Model package group already exists\")\n",
        "        model_package_group_created = True\n",
        "    else:\n",
        "        print(f\" Error: {e}\")\n",
        "        model_package_group_created = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c314882f-6184-4d1c-8aca-df4de5662f41",
      "metadata": {
        "id": "c314882f-6184-4d1c-8aca-df4de5662f41"
      },
      "source": [
        "### Section 7.8: Register Model Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6b4f00-3d58-485c-96fc-b3fd7101ca0e",
      "metadata": {
        "id": "0a6b4f00-3d58-485c-96fc-b3fd7101ca0e"
      },
      "outputs": [],
      "source": [
        "#@title Section 7.8: Register Model Version\n",
        "\n",
        "if model_package_group_created:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SECTION 7.8: REGISTER MODEL VERSION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n Registering model version to package group...\")\n",
        "\n",
        "    try:\n",
        "        # Define inference specification\n",
        "        modelpackage_inference_specification = {\n",
        "            'InferenceSpecification': {\n",
        "                'Containers': [\n",
        "                    {\n",
        "                        'Image': image,\n",
        "                        'ModelDataUrl': info['ModelArtifacts']['S3ModelArtifacts'],\n",
        "                    }\n",
        "                ],\n",
        "                'SupportedContentTypes': ['text/csv'],\n",
        "                'SupportedResponseMIMETypes': ['text/csv'],\n",
        "                'SupportedTransformInstanceTypes': ['ml.m5.xlarge'],\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Create model package input\n",
        "        create_model_package_input_dict = {\n",
        "            'ModelPackageGroupName': model_package_group_name,\n",
        "            'ModelPackageDescription': model_description,\n",
        "            'ModelApprovalStatus': 'Approved'\n",
        "        }\n",
        "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
        "\n",
        "        # Register model\n",
        "        create_model_package_response = sm.create_model_package(\n",
        "            **create_model_package_input_dict\n",
        "        )\n",
        "\n",
        "        model_package_arn = create_model_package_response['ModelPackageArn']\n",
        "        print(f\" Model version registered!\")\n",
        "        print(f\"   Model Package ARN: {model_package_arn}\")\n",
        "\n",
        "        # Store for later use\n",
        "        %store model_package_group_name\n",
        "        %store model_package_arn\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error registering model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2c97cf-b815-46a1-b844-8dc608ce43c0",
      "metadata": {
        "id": "7d2c97cf-b815-46a1-b844-8dc608ce43c0"
      },
      "source": [
        "### Section 7.9: Deployment Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c26d23-d88e-4887-b93a-97a3662fc34c",
      "metadata": {
        "id": "b9c26d23-d88e-4887-b93a-97a3662fc34c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEPLOYMENT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n Deployment Status:\")\n",
        "\n",
        "print(f\"\\n1. SageMaker Model:\")\n",
        "if model_created:\n",
        "    print(f\"   Created: {model_name}\")\n",
        "    print(f\"   Location: {model_data}\")\n",
        "else:\n",
        "    print(f\"    Not created\")\n",
        "\n",
        "print(f\"\\n2. Endpoint Configuration:\")\n",
        "print(f\"   Created automatically by XGBoostModel.deploy()\")\n",
        "print(f\"   Instance type: ml.m5.large\")\n",
        "\n",
        "print(f\"\\n3. Endpoint:\")\n",
        "if endpoint_deployed:\n",
        "    print(f\"   Created: {endpoint_name}\")\n",
        "    print(f\"   Variant: {variant_name}\")\n",
        "    print(f\"   Status: InService\")\n",
        "else:\n",
        "    print(f\"    Not deployed\")\n",
        "\n",
        "print(f\"\\n4. Data Capture:\")\n",
        "print(f\"   Enabled: True\")\n",
        "print(f\"   Path: {s3_capture_upload_path}\")\n",
        "print(f\"   Content type: text/csv (no BASE64)\")\n",
        "\n",
        "print(f\"\\n5. Model Registry:\")\n",
        "if 'model_package_group_name' in dir():\n",
        "    print(f\"   Group: {model_package_group_name}\")\n",
        "else:\n",
        "    print(f\"   Not registered\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" SECTION 7 COMPLETE - MODEL DEPLOYMENT\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deMilUhp64pD",
      "metadata": {
        "id": "deMilUhp64pD"
      },
      "source": [
        "## 8. Monitoring & Observability <a id='section-8'></a>\n",
        "\n",
        "This section implements comprehensive production monitoring across three pillars:\n",
        "\n",
        "1. **Model Quality Monitoring** (Section 8.2): Tracks prediction accuracy and classification drift against a validated baseline\n",
        "2. **Data Quality Monitoring** (Section 8.3): Detects feature distribution shifts in incoming data\n",
        "3. **Infrastructure Monitoring** (Section 8.4): Monitors endpoint health, latency, and resource utilization\n",
        "4. **CloudWatch Dashboard** (Section 8.5): Centralized visualization of all monitoring signals\n",
        "5. **Monitoring Reports** (Section 8.6): Automated report generation and violation review\n",
        "\n",
        "All monitors are baselined, scheduled to run **hourly**, and publish metrics to CloudWatch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oP-WNir464pD",
      "metadata": {
        "id": "oP-WNir464pD"
      },
      "source": [
        "### 8.1 Configure Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K4_FqtS764pD",
      "metadata": {
        "id": "K4_FqtS764pD"
      },
      "outputs": [],
      "source": [
        "# ── Imports ────────────────────────────────────────────────────────────────\n",
        "import copy\n",
        "import json\n",
        "import random\n",
        "import io\n",
        "import csv\n",
        "import time\n",
        "import uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import sagemaker\n",
        "\n",
        "from threading import Thread\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from time import sleep\n",
        "\n",
        "from sagemaker import get_execution_role, image_uris, Session\n",
        "from sagemaker.s3 import S3Downloader, S3Uploader\n",
        "from sagemaker.model_monitor import (\n",
        "    DefaultModelMonitor,\n",
        "    ModelQualityMonitor,\n",
        "    CronExpressionGenerator,\n",
        "    DataCaptureConfig,\n",
        "    EndpointInput,\n",
        ")\n",
        "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
        "\n",
        "# ── Core references (already defined in earlier sections) ─────────────────\n",
        "# Confirm key variables are available\n",
        "print(\"Confirming monitoring configuration...\")\n",
        "print(f\"  Endpoint     : {endpoint_name}\")\n",
        "print(f\"  Variant      : {variant_name}\")\n",
        "print(f\"  Bucket       : {BASE_BUCKET_NAME}\")\n",
        "print(f\"  Monitoring   : s3://{MONITORING_DIR}\")\n",
        "print(f\"  Region       : {REGION}\")\n",
        "print(f\"  Role         : {role[:50]}...\")\n",
        "\n",
        "# ── S3 paths for all monitoring artefacts ──────────────────────────────────\n",
        "monitoring_output_path      = f\"s3://{MONITORING_DIR}monitoring-output\"\n",
        "baseline_results_path       = f\"s3://{MONITORING_DIR}baseline-results\"\n",
        "monitoring_reports_path     = f\"s3://{MONITORING_DIR}reports\"\n",
        "reports_uri                 = monitoring_reports_path\n",
        "\n",
        "# Model Quality paths\n",
        "mq_baseline_data_uri    = f\"s3://{BASE_BUCKET_NAME}/{MONITORING_PREFIX}baselining/data\"\n",
        "mq_baseline_results_uri = f\"s3://{BASE_BUCKET_NAME}/{MONITORING_PREFIX}baselining/results\"\n",
        "mq_results_uri          = f\"s3://{BASE_BUCKET_NAME}/{MONITORING_PREFIX}model-quality-results\"\n",
        "ground_truth_upload_path = f\"s3://{MONITORING_DIR}ground_truth_data\"\n",
        "\n",
        "# Data Quality paths\n",
        "dq_baseline_uri  = f\"s3://{MONITORING_DIR}data-quality-baseline\"\n",
        "dq_results_uri   = f\"s3://{MONITORING_DIR}data-quality-results\"\n",
        "\n",
        "# ── Retrieve the model monitor image ──────────────────────────────────────\n",
        "monitor_image_uri = image_uris.retrieve(framework=\"model-monitor\", region=REGION)\n",
        "\n",
        "print(\"\\n✓ Monitoring paths configured\")\n",
        "print(f\"  MQ baseline data   : {mq_baseline_data_uri}\")\n",
        "print(f\"  MQ baseline results: {mq_baseline_results_uri}\")\n",
        "print(f\"  MQ results         : {mq_results_uri}\")\n",
        "print(f\"  Ground truth       : {ground_truth_upload_path}\")\n",
        "print(f\"  DQ baseline        : {dq_baseline_uri}\")\n",
        "print(f\"  DQ results         : {dq_results_uri}\")\n",
        "print(f\"  Reports            : {monitoring_reports_path}\")\n",
        "print(f\"  Monitor image      : {monitor_image_uri}\")\n",
        "\n",
        "%store monitoring_output_path\n",
        "%store monitoring_reports_path\n",
        "%store ground_truth_upload_path\n",
        "%store mq_results_uri\n",
        "%store dq_results_uri\n",
        "%store reports_uri\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gMgQ_Wya64pD",
      "metadata": {
        "id": "gMgQ_Wya64pD"
      },
      "source": [
        "### 8.2 Model Quality Monitoring\n",
        "\n",
        "Model Quality Monitoring continuously evaluates classification performance\n",
        "(accuracy, precision, recall, F1, AUC) of the live endpoint against a\n",
        "baseline derived from held-out validation predictions.\n",
        "\n",
        "**Process:**\n",
        "1. Upload a baseline dataset of validation predictions\n",
        "2. Run a baselining job to compute statistics and suggest constraints\n",
        "3. Stream live traffic through the endpoint (data capture)\n",
        "4. Upload synthetic ground-truth labels\n",
        "5. Schedule an hourly monitoring job that merges captured inferences with\n",
        "   ground truth and flags constraint violations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4SWxagC564pE",
      "metadata": {
        "id": "4SWxagC564pE"
      },
      "source": [
        "#### 8.2.1 Upload Baseline Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MwjbqfS-64pE",
      "metadata": {
        "id": "MwjbqfS-64pE"
      },
      "outputs": [],
      "source": [
        "# predictions_local was written in Section 7.6 (test_predictions.csv)\n",
        "# It contains: review_id, probability, predicted_class, is_highly_rated\n",
        "print(f\"Uploading baseline dataset from: {predictions_local}\")\n",
        "baseline_dataset_uri = S3Uploader.upload(predictions_local, mq_baseline_data_uri)\n",
        "print(f\"✓ Baseline dataset uploaded: {baseline_dataset_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xuxTCuuU64pE",
      "metadata": {
        "id": "xuxTCuuU64pE"
      },
      "source": [
        "#### 8.2.2 Create Model Quality Monitor & Baseline Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HbbUoT0z64pE",
      "metadata": {
        "id": "HbbUoT0z64pE"
      },
      "outputs": [],
      "source": [
        "# Instantiate the ModelQualityMonitor\n",
        "xgboost_model_quality_monitor = ModelQualityMonitor(\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    volume_size_in_gb=20,\n",
        "    max_runtime_in_seconds=1800,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "print(\"✓ ModelQualityMonitor created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2O1M22M64pE",
      "metadata": {
        "id": "d2O1M22M64pE"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Run the baseline suggestion job.\n",
        "# Problem type is BinaryClassification; column names match test_predictions.csv\n",
        "mq_baseline_job_name = (\n",
        "    f\"venuesignal-mq-baseline-{account_id}-\"\n",
        "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
        ")\n",
        "print(f\"Creating model quality baseline: {mq_baseline_job_name}\")\n",
        "print(\"This will take approximately 10-15 minutes…\")\n",
        "\n",
        "xgboost_model_quality_monitor.suggest_baseline(\n",
        "    job_name=mq_baseline_job_name,\n",
        "    baseline_dataset=baseline_dataset_uri,\n",
        "    dataset_format=DatasetFormat.csv(header=True),\n",
        "    output_s3_uri=mq_baseline_results_uri,\n",
        "    problem_type=\"BinaryClassification\",\n",
        "    inference_attribute=\"predicted_class\",\n",
        "    probability_attribute=\"probability\",\n",
        "    ground_truth_attribute=\"is_highly_rated\",\n",
        "    wait=True,\n",
        "    logs=False,\n",
        ")\n",
        "print(f\"\\n✓ Baseline job complete: {mq_baseline_job_name}\")\n",
        "%store mq_baseline_job_name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oFn1sdJM64pE",
      "metadata": {
        "id": "oFn1sdJM64pE"
      },
      "source": [
        "#### 8.2.3 Review Baseline Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u4T_GXG-64pE",
      "metadata": {
        "id": "u4T_GXG-64pE"
      },
      "outputs": [],
      "source": [
        "mq_baseline_job = xgboost_model_quality_monitor.latest_baselining_job\n",
        "\n",
        "# ── Statistics ─────────────────────────────────────────────────────────────\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL QUALITY BASELINE — STATISTICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    binary_metrics = mq_baseline_job.baseline_statistics().body_dict[\n",
        "        \"binary_classification_metrics\"\n",
        "    ]\n",
        "    import pandas as pd\n",
        "    print(pd.json_normalize(binary_metrics).T.to_string())\n",
        "except Exception as e:\n",
        "    print(f\"Warning: could not retrieve statistics — {e}\")\n",
        "\n",
        "# ── Constraints ─────────────────────────────────────────────────────────────\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL QUALITY BASELINE — SUGGESTED CONSTRAINTS\")\n",
        "print(\"=\" * 70)\n",
        "try:\n",
        "    constraints = mq_baseline_job.suggested_constraints().body_dict[\n",
        "        \"binary_classification_constraints\"\n",
        "    ]\n",
        "    print(pd.DataFrame(constraints).T.to_string())\n",
        "except Exception as e:\n",
        "    print(f\"Warning: could not retrieve constraints — {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ovdJyh64pF",
      "metadata": {
        "id": "71ovdJyh64pF"
      },
      "source": [
        "#### 8.2.4 Generate Live Traffic for Data Capture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7943a2d1-fc49-4155-b55b-bcace220df8c",
      "metadata": {
        "id": "7943a2d1-fc49-4155-b55b-bcace220df8c"
      },
      "outputs": [],
      "source": [
        "# Start a background thread that continuously invokes the endpoint so that\n",
        "# SageMaker captures input/output pairs.  The thread runs indefinitely;\n",
        "# restart the kernel to stop it.\n",
        "\n",
        "def _invoke_endpoint_loop(ep_name, feature_df):\n",
        "    runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
        "    ids = list(range(500))  # must match ground truth IDs exactly (0-499)\n",
        "    while True:\n",
        "        try:\n",
        "            for i in ids:\n",
        "                row = feature_df.iloc[i % len(feature_df)]\n",
        "                buf = io.StringIO()\n",
        "                csv.writer(buf).writerow(row.fillna(0).tolist())\n",
        "                runtime.invoke_endpoint(\n",
        "                    EndpointName=ep_name,\n",
        "                    ContentType=\"text/csv\",\n",
        "                    Body=buf.getvalue().strip(),\n",
        "                    InferenceId=str(i),  # 0-499 matches ground truth\n",
        "                )\n",
        "                sleep(2)\n",
        "        except Exception:\n",
        "            pass  # retry on transient errors\n",
        "\n",
        "traffic_thread = Thread(\n",
        "    target=_invoke_endpoint_loop,\n",
        "    args=(endpoint_name, production_df[xgb_features]),\n",
        "    daemon=True,\n",
        ")\n",
        "traffic_thread.start()\n",
        "print(\"✓ Endpoint traffic thread started (daemon — stops with kernel)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VTdTrrKv64pF",
      "metadata": {
        "id": "VTdTrrKv64pF"
      },
      "source": [
        "#### 8.2.5 Verify Captured Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CkIMrtiS64pF",
      "metadata": {
        "id": "CkIMrtiS64pF"
      },
      "outputs": [],
      "source": [
        "print(\"Waiting for capture files to appear in S3…\", end=\"\")\n",
        "capture_files = []\n",
        "for _ in range(120):\n",
        "    capture_files = sorted(\n",
        "        S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\")\n",
        "    )\n",
        "    if capture_files:\n",
        "        sample_lines = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")\n",
        "        sample_record = json.loads(sample_lines[0])\n",
        "        if \"inferenceId\" in sample_record.get(\"eventMetadata\", {}):\n",
        "            break\n",
        "    print(\".\", end=\"\", flush=True)\n",
        "    sleep(1)\n",
        "print()\n",
        "\n",
        "if capture_files:\n",
        "    print(f\"✓ Found {len(capture_files)} capture file(s)\")\n",
        "    print(\"\\nLatest capture files:\")\n",
        "    for f in capture_files[-3:]:\n",
        "        print(f\"  {f}\")\n",
        "    print(\"\\nSample capture record (first event):\")\n",
        "    print(json.dumps(sample_record, indent=2)[:800])\n",
        "else:\n",
        "    print(\"⚠ No capture files found yet — ensure data capture is enabled on the endpoint\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gEWsiNYg64pF",
      "metadata": {
        "id": "gEWsiNYg64pF"
      },
      "source": [
        "#### 8.2.6 Generate Synthetic Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4mWWihcG64pF",
      "metadata": {
        "id": "4mWWihcG64pF"
      },
      "outputs": [],
      "source": [
        "# In production, ground truth arrives asynchronously (actual outcomes).\n",
        "# Here we simulate it with random labels (70 % positive) for demonstration.\n",
        "\n",
        "def _ground_truth_record(inference_id):\n",
        "    random.seed(inference_id)\n",
        "    return {\n",
        "        \"groundTruthData\": {\n",
        "            \"data\": \"1\" if random.random() < 0.7 else \"0\",\n",
        "            \"encoding\": \"CSV\",\n",
        "        },\n",
        "        \"eventMetadata\": {\"eventId\": str(inference_id)},\n",
        "        \"eventVersion\": \"0\",\n",
        "    }\n",
        "\n",
        "def _upload_ground_truth(records, upload_time):\n",
        "    body = \"\\n\".join(json.dumps(r) for r in records)\n",
        "    target = f\"{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
        "    S3Uploader.upload_string_as_file_body(body, target)\n",
        "    print(f\"  Uploaded {len(records)} ground-truth records → {target}\")\n",
        "\n",
        "def _ground_truth_loop(n=500):\n",
        "    while True:\n",
        "        records = [_ground_truth_record(i) for i in range(n)]\n",
        "        _upload_ground_truth(records, datetime.utcnow())\n",
        "        sleep(3600)  # re-upload once per hour\n",
        "\n",
        "gt_thread = Thread(target=_ground_truth_loop, daemon=True)\n",
        "gt_thread.start()\n",
        "\n",
        "# Upload one batch immediately so the first monitoring job has data\n",
        "_upload_ground_truth([_ground_truth_record(i) for i in range(500)], datetime.utcnow())\n",
        "print(\"\\n✓ Ground-truth thread started; initial batch uploaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OmE9jpiz64pG",
      "metadata": {
        "id": "OmE9jpiz64pG"
      },
      "source": [
        "#### 8.2.7 Create Hourly Model Quality Monitoring Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d74e81d-27aa-4b16-a947-c2b379873347",
      "metadata": {
        "id": "7d74e81d-27aa-4b16-a947-c2b379873347"
      },
      "outputs": [],
      "source": [
        "# Delete old schedule\n",
        "try:\n",
        "    sagemaker_client.delete_monitoring_schedule(\n",
        "        MonitoringScheduleName=mq_schedule_name\n",
        "    )\n",
        "    print(f\"✓ Deleted old schedule: {mq_schedule_name}\")\n",
        "    time.sleep(15)\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "# Now output has 2 columns: _c0=probability, _c1=predicted_class\n",
        "mq_endpoint_input = EndpointInput(\n",
        "    endpoint_name=endpoint_name,\n",
        "    probability_attribute=\"0\",     # _c0 = probability value\n",
        "    inference_attribute=\"1\",       # _c1 = predicted class (0 or 1)\n",
        "    probability_threshold_attribute=0.5,\n",
        "    destination=\"/opt/ml/processing/input_data\",\n",
        ")\n",
        "\n",
        "mq_schedule_name = (\n",
        "    f\"venuesignal-mq-schedule-{account_id}-\"\n",
        "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
        ")\n",
        "print(f\"Creating model quality schedule: {mq_schedule_name}\")\n",
        "\n",
        "try:\n",
        "    xgboost_model_quality_monitor.create_monitoring_schedule(\n",
        "        monitor_schedule_name=mq_schedule_name,\n",
        "        endpoint_input=mq_endpoint_input,\n",
        "        output_s3_uri=mq_results_uri,\n",
        "        problem_type=\"BinaryClassification\",\n",
        "        ground_truth_input=ground_truth_upload_path,\n",
        "        constraints=mq_baseline_job.suggested_constraints(),\n",
        "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
        "        enable_cloudwatch_metrics=True,\n",
        "    )\n",
        "    print(f\"✓ Model quality schedule created: {mq_schedule_name}\")\n",
        "    print(f\"  probability_attribute : '0' → endpointOutput_0 (probability)\")\n",
        "    print(f\"  inference_attribute   : '1' → endpointOutput_1 (predicted class)\")\n",
        "    %store mq_schedule_name\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nzSHPjdI64pG",
      "metadata": {
        "id": "nzSHPjdI64pG"
      },
      "source": [
        "#### 8.2.8 Verify Model Quality Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "szeujJ9R64pG",
      "metadata": {
        "id": "szeujJ9R64pG"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    desc = sagemaker_client.describe_monitoring_schedule(\n",
        "        MonitoringScheduleName=mq_schedule_name\n",
        "    )\n",
        "    status = desc[\"MonitoringScheduleStatus\"]\n",
        "    print(f\"Schedule : {mq_schedule_name}\")\n",
        "    print(f\"Status   : {status}\")\n",
        "    if status == \"Scheduled\":\n",
        "        print(\"✓ Model quality schedule is active and running hourly\")\n",
        "    elif \"FailureReason\" in desc:\n",
        "        print(f\"✗ Failure: {desc['FailureReason']}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Could not describe schedule: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AHgPwEZr64pG",
      "metadata": {
        "id": "AHgPwEZr64pG"
      },
      "source": [
        "---\n",
        "\n",
        "### 8.3 Data Quality Monitoring\n",
        "\n",
        "Data Quality Monitoring detects feature-level drift between the baseline\n",
        "training distribution and incoming inference data.  It monitors statistics\n",
        "(mean, standard deviation, missing-value rates, etc.) for every feature\n",
        "and raises violations when values exceed the baselined thresholds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dUPvgvmf64pG",
      "metadata": {
        "id": "dUPvgvmf64pG"
      },
      "source": [
        "#### 8.3.1 Verify Data Capture is Enabled on the Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gnsaLJMu64pG",
      "metadata": {
        "id": "gnsaLJMu64pG"
      },
      "outputs": [],
      "source": [
        "# Data capture must be enabled on the endpoint config; it was configured\n",
        "# in Section 7.3.  This cell verifies the configuration.\n",
        "try:\n",
        "    ep_desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
        "    config_name = ep_desc[\"EndpointConfigName\"]\n",
        "    config_desc = sagemaker_client.describe_endpoint_config(\n",
        "        EndpointConfigName=config_name\n",
        "    )\n",
        "    if \"DataCaptureConfig\" in config_desc:\n",
        "        dc = config_desc[\"DataCaptureConfig\"]\n",
        "        print(f\"✓ Data capture is enabled on endpoint: {endpoint_name}\")\n",
        "        print(f\"  Capture destination : {dc.get('DestinationS3Uri', 'N/A')}\")\n",
        "        print(f\"  Capture percentage  : {dc.get('InitialSamplingPercentage', 100)}%\")\n",
        "        data_capture_enabled = True\n",
        "    else:\n",
        "        print(\"⚠ Data capture is NOT enabled on this endpoint configuration.\")\n",
        "        print(\"  Ensure Section 7.3 was executed with DataCaptureConfig.\")\n",
        "        data_capture_enabled = False\n",
        "except Exception as e:\n",
        "    print(f\"Error checking endpoint config: {e}\")\n",
        "    data_capture_enabled = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JXTQEW6e64pH",
      "metadata": {
        "id": "JXTQEW6e64pH"
      },
      "source": [
        "#### 8.3.2 Configure Data Quality Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j2DZ6e2G64pH",
      "metadata": {
        "id": "j2DZ6e2G64pH"
      },
      "outputs": [],
      "source": [
        "# training_data_uri was defined in Section 6 and stored via %store\n",
        "# It points to the training CSV used to fit the XGBoost model.\n",
        "print(f\"Training data URI : {train_data_path}\")\n",
        "print(f\"DQ baseline URI   : {dq_baseline_uri}\")\n",
        "print(f\"DQ results URI    : {dq_results_uri}\")\n",
        "\n",
        "# Verify the training CSV is accessible\n",
        "try:\n",
        "    bucket = train_data_path.split('/')[2]\n",
        "    key    = '/'.join(train_data_path.split('/')[3:])\n",
        "    s3_client.head_object(Bucket=bucket, Key=key)\n",
        "    print(\"✓ Training data verified in S3\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Training data not found — {e}\")\n",
        "    print(\"  Update train_data_path if the file has moved.\")\n",
        "\n",
        "%store dq_baseline_uri\n",
        "%store dq_results_uri\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rmeV1k9_64pH",
      "metadata": {
        "id": "rmeV1k9_64pH"
      },
      "source": [
        "#### 8.3.3 Create Data Quality Monitor & Baseline Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u-yYjDW464pH",
      "metadata": {
        "id": "u-yYjDW464pH"
      },
      "outputs": [],
      "source": [
        "data_quality_monitor = DefaultModelMonitor(\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    volume_size_in_gb=20,\n",
        "    max_runtime_in_seconds=1800,\n",
        "    sagemaker_session=sagemaker_session,  # use the module-level session object\n",
        ")\n",
        "print(\"✓ DefaultModelMonitor created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478d89f4-9fe2-4eae-935e-1f89d179f4dc",
      "metadata": {
        "id": "478d89f4-9fe2-4eae-935e-1f89d179f4dc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Rebuild baseline using ONLY the xgb_features columns —\n",
        "# these are the exact columns the endpoint receives at inference time\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# Load training data and keep only the feature columns\n",
        "train_features_only = pd.read_csv(train_data_path)\n",
        "\n",
        "# Keep only the columns the endpoint actually sees\n",
        "missing = [f for f in xgb_features if f not in train_features_only.columns]\n",
        "if missing:\n",
        "    print(f\"⚠ Missing features in training CSV: {missing}\")\n",
        "else:\n",
        "    train_features_only = train_features_only[xgb_features]\n",
        "    print(f\"✓ Keeping {len(xgb_features)} feature columns (matching endpoint input)\")\n",
        "\n",
        "# Save the features-only CSV to a new S3 path\n",
        "features_baseline_local = \"/tmp/train_features_only.csv\"\n",
        "train_features_only.to_csv(features_baseline_local, index=False)\n",
        "\n",
        "features_baseline_s3 = (\n",
        "    f\"s3://{BASE_BUCKET_NAME}/{MONITORING_PREFIX}dq-baseline-input/train_features_only.csv\"\n",
        ")\n",
        "s3_client.upload_file(\n",
        "    features_baseline_local,\n",
        "    BASE_BUCKET_NAME,\n",
        "    f\"{MONITORING_PREFIX}dq-baseline-input/train_features_only.csv\",\n",
        ")\n",
        "print(f\"✓ Features-only baseline data uploaded: {features_baseline_s3}\")\n",
        "print(f\"  Columns  : {len(xgb_features)}\")\n",
        "print(f\"  Rows     : {len(train_features_only):,}\")\n",
        "\n",
        "# Delete old baseline results so there's no confusion\n",
        "try:\n",
        "    old_files = S3Downloader.list(dq_baseline_uri)\n",
        "    for f in old_files:\n",
        "        key = f.replace(f\"s3://{BASE_BUCKET_NAME}/\", \"\")\n",
        "        s3_client.delete_object(Bucket=BASE_BUCKET_NAME, Key=key)\n",
        "    print(f\"✓ Cleared old baseline from: {dq_baseline_uri}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "# Delete and recreate the data quality monitor to clear cached baseline\n",
        "data_quality_monitor = DefaultModelMonitor(\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type=\"ml.m5.xlarge\",\n",
        "    volume_size_in_gb=20,\n",
        "    max_runtime_in_seconds=1800,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bSkSy-4X64pH",
      "metadata": {
        "id": "bSkSy-4X64pH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "dq_baseline_job_name = (\n",
        "    f\"venuesignal-dq-baseline-{account_id}-\"\n",
        "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
        ")\n",
        "print(f\"Creating data quality baseline: {dq_baseline_job_name}\")\n",
        "print(\"This will take approximately 15-20 minutes…\")\n",
        "\n",
        "data_quality_monitor.suggest_baseline(\n",
        "    job_name=dq_baseline_job_name,\n",
        "    baseline_dataset=features_baseline_s3,\n",
        "    dataset_format=DatasetFormat.csv(header=True),\n",
        "    output_s3_uri=dq_baseline_uri,\n",
        "    wait=True,\n",
        "    logs=False,\n",
        ")\n",
        "print(f\"\\n✓ Data quality baseline job complete: {dq_baseline_job_name}\")\n",
        "%store dq_baseline_job_name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LXAK_RXc64pH",
      "metadata": {
        "id": "LXAK_RXc64pH"
      },
      "source": [
        "#### 8.3.4 Review Data Quality Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3LY1h36G64pH",
      "metadata": {
        "id": "3LY1h36G64pH"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"DATA QUALITY BASELINE REVIEW\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "dq_baseline_job = data_quality_monitor.latest_baselining_job\n",
        "if dq_baseline_job is None:\n",
        "    print(\"✗ No baseline job found — run Section 8.3.3 first\")\n",
        "else:\n",
        "    print(f\"✓ Baseline job : {dq_baseline_job.job_name}\")\n",
        "\n",
        "    stats_file       = f\"{dq_baseline_uri}/statistics.json\"\n",
        "    constraints_file = f\"{dq_baseline_uri}/constraints.json\"\n",
        "\n",
        "    try:\n",
        "        local_stats = S3Downloader.download(stats_file, \"/tmp/dq_stats\")\n",
        "        with open(local_stats[0]) as fh:\n",
        "            dq_stats = json.load(fh)\n",
        "\n",
        "        def _fmt(val, decimals=4):\n",
        "            \"\"\"Format a numeric value; return 'N/A' if missing or non-numeric.\"\"\"\n",
        "            try:\n",
        "                return f\"{float(val):.{decimals}f}\"\n",
        "            except (TypeError, ValueError):\n",
        "                return \"N/A\"\n",
        "\n",
        "        features = dq_stats.get(\"features\", [])\n",
        "        print(f\"\\nTotal features analysed: {len(features)}\")\n",
        "        print(\"\\nFirst 8 features:\")\n",
        "        for feat in features[:8]:\n",
        "            name  = feat[\"name\"]\n",
        "            ftype = feat.get(\"inferred_type\", \"unknown\")\n",
        "            if \"numerical_statistics\" in feat:\n",
        "                ns = feat[\"numerical_statistics\"]\n",
        "                print(\n",
        "                    f\"  {name:<35} [{ftype}]  \"\n",
        "                    f\"mean={_fmt(ns.get('mean'))}  \"\n",
        "                    f\"std={_fmt(ns.get('stdDev'))}\"\n",
        "                )\n",
        "            else:\n",
        "                ss = feat.get(\"string_statistics\", {})\n",
        "                print(\n",
        "                    f\"  {name:<35} [{ftype}]  \"\n",
        "                    f\"distinct={ss.get('distinct_count', 'N/A')}\"\n",
        "                )\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Could not download statistics file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GE-Ojr2R64pI",
      "metadata": {
        "id": "GE-Ojr2R64pI"
      },
      "source": [
        "#### 8.3.5 Create Hourly Data Quality Monitoring Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_0THtd2764pI",
      "metadata": {
        "id": "_0THtd2764pI"
      },
      "outputs": [],
      "source": [
        "# EndpointInput for DefaultModelMonitor\n",
        "dq_endpoint_input = EndpointInput(\n",
        "    endpoint_name=endpoint_name,\n",
        "    destination=\"/opt/ml/processing/input_data\",\n",
        "    s3_input_mode=\"File\",\n",
        "    s3_data_distribution_type=\"FullyReplicated\",\n",
        ")\n",
        "\n",
        "dq_schedule_prefix = \"venuesignal\"\n",
        "dq_schedule_name = (\n",
        "    f\"{dq_schedule_prefix}-dq-schedule-\"\n",
        "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
        ")\n",
        "print(f\"Creating data quality schedule: {dq_schedule_name}\")\n",
        "\n",
        "try:\n",
        "    data_quality_monitor.create_monitoring_schedule(\n",
        "        monitor_schedule_name=dq_schedule_name,\n",
        "        endpoint_input=dq_endpoint_input,\n",
        "        output_s3_uri=dq_results_uri,\n",
        "        statistics=data_quality_monitor.baseline_statistics(),\n",
        "        constraints=data_quality_monitor.suggested_constraints(),\n",
        "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
        "        enable_cloudwatch_metrics=True,\n",
        "    )\n",
        "    print(f\"✓ Data quality schedule created: {dq_schedule_name}\")\n",
        "    print(f\"  Monitoring : INPUT features (13 columns)\")\n",
        "    print(f\"  Baseline   : {dq_baseline_uri}\")\n",
        "    print(f\"  Results    : {dq_results_uri}\")\n",
        "    %store dq_schedule_name\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_31uSGI664pI",
      "metadata": {
        "id": "_31uSGI664pI"
      },
      "source": [
        "#### 8.3.6 Verify Data Quality Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JXWc3pbt64pI",
      "metadata": {
        "id": "JXWc3pbt64pI"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    desc = sagemaker_client.describe_monitoring_schedule(\n",
        "        MonitoringScheduleName=dq_schedule_name\n",
        "    )\n",
        "    status = desc[\"MonitoringScheduleStatus\"]\n",
        "    print(f\"Schedule : {dq_schedule_name}\")\n",
        "    print(f\"Status   : {status}\")\n",
        "    if status == \"Scheduled\":\n",
        "        print(\"✓ Data quality schedule is active and running hourly\")\n",
        "    elif \"FailureReason\" in desc:\n",
        "        print(f\"✗ Failure: {desc['FailureReason']}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Could not describe schedule: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eQCPgLHl64pI",
      "metadata": {
        "id": "eQCPgLHl64pI"
      },
      "source": [
        "---\n",
        "\n",
        "### 8.4 Infrastructure Monitoring\n",
        "\n",
        "Infrastructure monitoring uses CloudWatch to track endpoint-level hardware\n",
        "and performance metrics (CPU, memory, disk, latency, error rates) and runs\n",
        "integration tests to validate end-to-end system health.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4qlAvSyl64pI",
      "metadata": {
        "id": "4qlAvSyl64pI"
      },
      "source": [
        "#### 8.4.1 Query SageMaker Endpoint Metrics from CloudWatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45IKhmds64pI",
      "metadata": {
        "id": "45IKhmds64pI"
      },
      "outputs": [],
      "source": [
        "def get_cw_metric(metric_name, namespace=\"AWS/SageMaker\", statistic=\"Average\", hours=1):\n",
        "    \"\"\"Return the most recent datapoint for an endpoint metric.\"\"\"\n",
        "    cw  = cloudwatch_client\n",
        "    end = datetime.now(timezone.utc)\n",
        "    start = end - timedelta(hours=hours)\n",
        "    dims = [\n",
        "        {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
        "        {\"Name\": \"VariantName\",  \"Value\": variant_name},\n",
        "    ]\n",
        "    try:\n",
        "        resp = cw.get_metric_statistics(\n",
        "            Namespace=namespace,\n",
        "            MetricName=metric_name,\n",
        "            Dimensions=dims,\n",
        "            StartTime=start,\n",
        "            EndTime=end,\n",
        "            Period=300,\n",
        "            Statistics=[statistic],\n",
        "        )\n",
        "        pts = resp.get(\"Datapoints\", [])\n",
        "        if pts:\n",
        "            return sorted(pts, key=lambda x: x[\"Timestamp\"])[-1][statistic]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "print(\"Querying endpoint metrics for the last hour…\")\n",
        "print(\"=\" * 60)\n",
        "latency      = get_cw_metric(\"ModelLatency\")\n",
        "invocations  = get_cw_metric(\"Invocations\",          statistic=\"Sum\")\n",
        "errors_4xx   = get_cw_metric(\"Invocation4XXErrors\",   statistic=\"Sum\")\n",
        "errors_5xx   = get_cw_metric(\"Invocation5XXErrors\",   statistic=\"Sum\")\n",
        "overhead     = get_cw_metric(\"OverheadLatency\")\n",
        "\n",
        "cpu    = get_cw_metric(\"CPUUtilization\",    namespace=\"/aws/sagemaker/Endpoints\")\n",
        "memory = get_cw_metric(\"MemoryUtilization\", namespace=\"/aws/sagemaker/Endpoints\")\n",
        "disk   = get_cw_metric(\"DiskUtilization\",   namespace=\"/aws/sagemaker/Endpoints\")\n",
        "\n",
        "print(f\"  Avg Model Latency   : {latency/1000:.2f} ms\"  if latency     else \"  Avg Model Latency   : N/A (no data yet)\")\n",
        "print(f\"  Total Invocations   : {invocations:.0f}\"      if invocations else \"  Total Invocations   : N/A\")\n",
        "print(f\"  4XX Errors          : {errors_4xx:.0f}\"       if errors_4xx  else \"  4XX Errors          : N/A\")\n",
        "print(f\"  5XX Errors          : {errors_5xx:.0f}\"       if errors_5xx  else \"  5XX Errors          : N/A\")\n",
        "print(f\"  Overhead Latency    : {overhead/1000:.2f} ms\" if overhead    else \"  Overhead Latency    : N/A\")\n",
        "print(f\"  CPU Utilization     : {cpu:.2f}%\"             if cpu         else \"  CPU Utilization     : N/A\")\n",
        "print(f\"  Memory Utilization  : {memory:.2f}%\"          if memory      else \"  Memory Utilization  : N/A\")\n",
        "print(f\"  Disk Utilization    : {disk:.2f}%\"            if disk        else \"  Disk Utilization    : N/A\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fn_qk1ii64pJ",
      "metadata": {
        "id": "fn_qk1ii64pJ"
      },
      "source": [
        "#### 8.4.2 Endpoint Health Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7yed0gP_64pJ",
      "metadata": {
        "id": "7yed0gP_64pJ"
      },
      "outputs": [],
      "source": [
        "def endpoint_health_check(ep_name, payload):\n",
        "    \"\"\"Single health check; returns latency and HTTP status.\"\"\"\n",
        "    runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
        "    try:\n",
        "        t0 = time.time()\n",
        "        resp = runtime.invoke_endpoint(\n",
        "            EndpointName=ep_name,\n",
        "            ContentType=\"text/csv\",\n",
        "            Body=payload,\n",
        "            InferenceId=str(uuid.uuid4()),\n",
        "        )\n",
        "        return {\n",
        "            \"status\": \"healthy\",\n",
        "            \"latency_ms\": (time.time() - t0) * 1000,\n",
        "            \"http_status\": resp[\"ResponseMetadata\"][\"HTTPStatusCode\"],\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "        }\n",
        "    except Exception as exc:\n",
        "        return {\n",
        "            \"status\": \"unhealthy\",\n",
        "            \"error\": str(exc),\n",
        "            \"timestamp\": datetime.utcnow().isoformat(),\n",
        "        }\n",
        "\n",
        "def run_health_check_batch(ep_name, payload, n=10):\n",
        "    \"\"\"Run n health checks and return a summary dict.\"\"\"\n",
        "    results = []\n",
        "    print(f\"Running {n} health checks on {ep_name}…\")\n",
        "    for i in range(n):\n",
        "        results.append(endpoint_health_check(ep_name, payload))\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f\"  Completed {i+1}/{n}\")\n",
        "        sleep(1)\n",
        "\n",
        "    healthy = [r for r in results if r[\"status\"] == \"healthy\"]\n",
        "    success_rate = len(healthy) / len(results)\n",
        "    avg_latency  = np.mean([r[\"latency_ms\"] for r in healthy]) if healthy else None\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"✓ Health Check Summary\")\n",
        "    print(f\"  Success rate : {success_rate:.1%}\")\n",
        "    print(f\"  Avg latency  : {avg_latency:.2f} ms\" if avg_latency else \"  Avg latency  : N/A\")\n",
        "    print(f\"  Failures     : {len(results) - len(healthy)}\")\n",
        "    print(\"=\" * 60)\n",
        "    return {\n",
        "        \"total\": len(results),\n",
        "        \"successful\": len(healthy),\n",
        "        \"failed\": len(results) - len(healthy),\n",
        "        \"success_rate\": success_rate,\n",
        "        \"avg_latency_ms\": avg_latency,\n",
        "    }\n",
        "\n",
        "# Use a single production row as health-check payload\n",
        "hc_payload = production_df[xgb_features].iloc[0].fillna(0).to_csv(header=None, index=False).strip()\n",
        "health_summary = run_health_check_batch(endpoint_name, hc_payload)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QRPTneYb64pJ",
      "metadata": {
        "id": "QRPTneYb64pJ"
      },
      "source": [
        "#### 8.4.3 Integration Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WmVssAkD64pJ",
      "metadata": {
        "id": "WmVssAkD64pJ"
      },
      "outputs": [],
      "source": [
        "INTEGRATION_TEST_CASES = [\n",
        "    {\n",
        "        \"name\": \"High-rated business with ample parking\",\n",
        "        \"input\": production_df[xgb_features].iloc[0].fillna(0)\n",
        "                   .to_csv(header=None, index=False).strip(),\n",
        "        \"expected_range\": (0.5, 1.0),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Low parking availability\",\n",
        "        \"input\": production_df[xgb_features].iloc[50].fillna(0)\n",
        "                   .to_csv(header=None, index=False).strip(),\n",
        "        \"expected_range\": (0.0, 1.0),\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Average business profile\",\n",
        "        \"input\": production_df[xgb_features].iloc[100].fillna(0)\n",
        "                   .to_csv(header=None, index=False).strip(),\n",
        "        \"expected_range\": (0.0, 1.0),\n",
        "    },\n",
        "]\n",
        "\n",
        "def run_integration_tests(ep_name, test_cases):\n",
        "    \"\"\"Run integration tests and return pass-rate summary.\"\"\"\n",
        "    runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
        "    results = []\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Running integration tests…\")\n",
        "    print(\"=\" * 60)\n",
        "    for tc in test_cases:\n",
        "        try:\n",
        "            resp = runtime.invoke_endpoint(\n",
        "                EndpointName=ep_name,\n",
        "                ContentType=\"text/csv\",\n",
        "                Body=tc[\"input\"],\n",
        "            )\n",
        "            # XGBoost may return multiple newline-separated scores;\n",
        "            # take the first value only\n",
        "            raw = resp[\"Body\"].read().decode().strip()\n",
        "            probability = float(raw.split(\"\\n\")[0].strip())\n",
        "            lo, hi = tc[\"expected_range\"]\n",
        "            passed = lo <= probability <= hi\n",
        "            print(f\"{'✓ PASS' if passed else '✗ FAIL'}  {tc['name']}\")\n",
        "            print(f\"       probability={probability:.4f}  expected=[{lo}, {hi}]\")\n",
        "            results.append({\"name\": tc[\"name\"], \"passed\": passed, \"value\": probability})\n",
        "        except Exception as exc:\n",
        "            print(f\"✗ ERROR  {tc['name']}: {exc}\")\n",
        "            results.append({\"name\": tc[\"name\"], \"passed\": False, \"error\": str(exc)})\n",
        "\n",
        "    passed_n = sum(1 for r in results if r[\"passed\"])\n",
        "    rate = passed_n / len(results) if results else 0\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Integration results: {passed_n}/{len(results)} passed ({rate:.1%})\")\n",
        "    print(\"=\" * 60)\n",
        "    return {\"pass_rate\": rate, \"passed\": passed_n,\n",
        "            \"failed\": len(results) - passed_n, \"total\": len(results), \"results\": results}\n",
        "\n",
        "test_results = run_integration_tests(endpoint_name, INTEGRATION_TEST_CASES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aV0QnVTc64pJ",
      "metadata": {
        "id": "aV0QnVTc64pJ"
      },
      "source": [
        "#### 8.4.4 Integration Quality Gate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l_4bnuzV64pJ",
      "metadata": {
        "id": "l_4bnuzV64pJ"
      },
      "outputs": [],
      "source": [
        "INTEGRATION_THRESHOLDS = {\n",
        "    \"min_pass_rate\":      0.95,\n",
        "    \"max_latency_ms\":    1000,\n",
        "    \"min_availability\":   0.99,\n",
        "}\n",
        "\n",
        "violations = []\n",
        "if test_results[\"pass_rate\"] < INTEGRATION_THRESHOLDS[\"min_pass_rate\"]:\n",
        "    violations.append(\n",
        "        f\"Integration pass rate {test_results['pass_rate']:.1%} \"\n",
        "        f\"< threshold {INTEGRATION_THRESHOLDS['min_pass_rate']:.1%}\"\n",
        "    )\n",
        "if health_summary[\"success_rate\"] < INTEGRATION_THRESHOLDS[\"min_availability\"]:\n",
        "    violations.append(\n",
        "        f\"Endpoint availability {health_summary['success_rate']:.1%} \"\n",
        "        f\"< threshold {INTEGRATION_THRESHOLDS['min_availability']:.1%}\"\n",
        "    )\n",
        "if health_summary[\"avg_latency_ms\"] and health_summary[\"avg_latency_ms\"] > INTEGRATION_THRESHOLDS[\"max_latency_ms\"]:\n",
        "    violations.append(\n",
        "        f\"Avg latency {health_summary['avg_latency_ms']:.2f} ms \"\n",
        "        f\"> threshold {INTEGRATION_THRESHOLDS['max_latency_ms']} ms\"\n",
        "    )\n",
        "\n",
        "gate_passed = len(violations) == 0\n",
        "print(\"=\" * 60)\n",
        "if gate_passed:\n",
        "    print(\"✓ INTEGRATION QUALITY GATE: PASSED\")\n",
        "else:\n",
        "    print(\"✗ INTEGRATION QUALITY GATE: FAILED\")\n",
        "    for i, v in enumerate(violations, 1):\n",
        "        print(f\"  {i}. {v}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lnmGe6Zd64pK",
      "metadata": {
        "id": "lnmGe6Zd64pK"
      },
      "source": [
        "#### 8.4.5 Publish Custom Integration Metrics to CloudWatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SqbYqJPK64pK",
      "metadata": {
        "id": "SqbYqJPK64pK"
      },
      "outputs": [],
      "source": [
        "metric_data = [\n",
        "    {\n",
        "        \"MetricName\": \"IntegrationTestPassRate\",\n",
        "        \"Value\":      test_results[\"pass_rate\"] * 100,\n",
        "        \"Unit\":       \"Percent\",\n",
        "        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n",
        "    },\n",
        "    {\n",
        "        \"MetricName\": \"HealthCheckSuccessRate\",\n",
        "        \"Value\":      health_summary[\"success_rate\"] * 100,\n",
        "        \"Unit\":       \"Percent\",\n",
        "        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n",
        "    },\n",
        "]\n",
        "if health_summary.get(\"avg_latency_ms\"):\n",
        "    metric_data.append({\n",
        "        \"MetricName\": \"HealthCheckLatencyMs\",\n",
        "        \"Value\":      health_summary[\"avg_latency_ms\"],\n",
        "        \"Unit\":       \"Milliseconds\",\n",
        "        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n",
        "    })\n",
        "\n",
        "try:\n",
        "    cloudwatch_client.put_metric_data(\n",
        "        Namespace=\"VenueSignal/Integration\",\n",
        "        MetricData=metric_data,\n",
        "    )\n",
        "    print(\"✓ Custom metrics published to CloudWatch (VenueSignal/Integration)\")\n",
        "    for m in metric_data:\n",
        "        print(f\"  {m['MetricName']}: {m['Value']:.2f} {m['Unit']}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to publish metrics: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1NgSJSOu64pK",
      "metadata": {
        "id": "1NgSJSOu64pK"
      },
      "source": [
        "#### 8.4.6 Create CloudWatch Alarms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fi_zpder64pK",
      "metadata": {
        "id": "Fi_zpder64pK"
      },
      "outputs": [],
      "source": [
        "def create_alarm(alarm_name, metric_name, namespace, threshold,\n",
        "                 comparison, statistic, period, evaluation_periods,\n",
        "                 alarm_desc, dimensions, treat_missing=\"notBreaching\"):\n",
        "    \"\"\"Helper: create or update a CloudWatch alarm.\"\"\"\n",
        "    try:\n",
        "        cloudwatch_client.put_metric_alarm(\n",
        "            AlarmName=alarm_name,\n",
        "            AlarmDescription=alarm_desc,\n",
        "            ActionsEnabled=True,\n",
        "            MetricName=metric_name,\n",
        "            Namespace=namespace,\n",
        "            Statistic=statistic,\n",
        "            Dimensions=dimensions,\n",
        "            Period=period,\n",
        "            EvaluationPeriods=evaluation_periods,\n",
        "            DatapointsToAlarm=evaluation_periods,\n",
        "            Threshold=threshold,\n",
        "            ComparisonOperator=comparison,\n",
        "            TreatMissingData=treat_missing,\n",
        "        )\n",
        "        print(f\"✓ Alarm created/updated: {alarm_name}\")\n",
        "        return alarm_name\n",
        "    except Exception as exc:\n",
        "        print(f\"✗ Failed to create alarm {alarm_name}: {exc}\")\n",
        "        return None\n",
        "\n",
        "ep_dims = [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}]\n",
        "ep_variant_dims = ep_dims + [{\"Name\": \"VariantName\", \"Value\": variant_name}]\n",
        "\n",
        "created_alarms = []\n",
        "\n",
        "# 1. High 4XX error rate\n",
        "created_alarms.append(create_alarm(\n",
        "    alarm_name=f\"{endpoint_name}-High4XXErrors\",\n",
        "    metric_name=\"Invocation4XXErrors\",\n",
        "    namespace=\"AWS/SageMaker\",\n",
        "    threshold=10,\n",
        "    comparison=\"GreaterThanThreshold\",\n",
        "    statistic=\"Sum\",\n",
        "    period=300,\n",
        "    evaluation_periods=2,\n",
        "    alarm_desc=\"Alert: more than 10 client errors in 5 minutes\",\n",
        "    dimensions=ep_variant_dims,\n",
        "))\n",
        "\n",
        "# 2. High 5XX error rate\n",
        "created_alarms.append(create_alarm(\n",
        "    alarm_name=f\"{endpoint_name}-High5XXErrors\",\n",
        "    metric_name=\"Invocation5XXErrors\",\n",
        "    namespace=\"AWS/SageMaker\",\n",
        "    threshold=5,\n",
        "    comparison=\"GreaterThanThreshold\",\n",
        "    statistic=\"Sum\",\n",
        "    period=300,\n",
        "    evaluation_periods=2,\n",
        "    alarm_desc=\"Alert: more than 5 server errors in 5 minutes\",\n",
        "    dimensions=ep_variant_dims,\n",
        "))\n",
        "\n",
        "# 3. High model latency\n",
        "created_alarms.append(create_alarm(\n",
        "    alarm_name=f\"{endpoint_name}-HighModelLatency\",\n",
        "    metric_name=\"ModelLatency\",\n",
        "    namespace=\"AWS/SageMaker\",\n",
        "    threshold=1_000_000,   # microseconds (SageMaker reports in µs)\n",
        "    comparison=\"GreaterThanThreshold\",\n",
        "    statistic=\"Average\",\n",
        "    period=300,\n",
        "    evaluation_periods=2,\n",
        "    alarm_desc=\"Alert: average model latency exceeds 1 second\",\n",
        "    dimensions=ep_variant_dims,\n",
        "))\n",
        "\n",
        "# 4. High CPU utilisation\n",
        "created_alarms.append(create_alarm(\n",
        "    alarm_name=f\"{endpoint_name}-HighCPU\",\n",
        "    metric_name=\"CPUUtilization\",\n",
        "    namespace=\"/aws/sagemaker/Endpoints\",\n",
        "    threshold=80,\n",
        "    comparison=\"GreaterThanThreshold\",\n",
        "    statistic=\"Average\",\n",
        "    period=300,\n",
        "    evaluation_periods=3,\n",
        "    alarm_desc=\"Alert: endpoint CPU utilisation above 80 %\",\n",
        "    dimensions=ep_dims,\n",
        "))\n",
        "\n",
        "# 5. High memory utilisation\n",
        "created_alarms.append(create_alarm(\n",
        "    alarm_name=f\"{endpoint_name}-HighMemory\",\n",
        "    metric_name=\"MemoryUtilization\",\n",
        "    namespace=\"/aws/sagemaker/Endpoints\",\n",
        "    threshold=85,\n",
        "    comparison=\"GreaterThanThreshold\",\n",
        "    statistic=\"Average\",\n",
        "    period=300,\n",
        "    evaluation_periods=3,\n",
        "    alarm_desc=\"Alert: endpoint memory utilisation above 85 %\",\n",
        "    dimensions=ep_dims,\n",
        "))\n",
        "\n",
        "# 6. Integration test pass-rate drop (custom metric)\n",
        "created_alarms.append(create_alarm(\n",
        "    alarm_name=f\"{endpoint_name}-IntegrationTestFailure\",\n",
        "    metric_name=\"IntegrationTestPassRate\",\n",
        "    namespace=\"VenueSignal/Integration\",\n",
        "    threshold=95,\n",
        "    comparison=\"LessThanThreshold\",\n",
        "    statistic=\"Average\",\n",
        "    period=600,\n",
        "    evaluation_periods=1,\n",
        "    alarm_desc=\"Alert: integration test pass rate below 95 %\",\n",
        "    dimensions=ep_dims,\n",
        "    treat_missing=\"breaching\",\n",
        "))\n",
        "\n",
        "# 7. Model quality: recall drift (populated once MQ schedule runs)\n",
        "mq_alarm_dims = [\n",
        "    {\"Name\": \"Endpoint\",           \"Value\": endpoint_name},\n",
        "    {\"Name\": \"MonitoringSchedule\", \"Value\": mq_schedule_name},\n",
        "]\n",
        "created_alarms.append(create_alarm(\n",
        "    alarm_name=\"VenueSignal-ModelQuality-RecallDrift\",\n",
        "    metric_name=\"recall\",\n",
        "    namespace=\"aws/sagemaker/Endpoints/model-metrics\",\n",
        "    threshold=0.70,\n",
        "    comparison=\"LessThanOrEqualToThreshold\",\n",
        "    statistic=\"Average\",\n",
        "    period=600,\n",
        "    evaluation_periods=1,\n",
        "    alarm_desc=\"Alert: model recall dropped below 0.70 baseline\",\n",
        "    dimensions=mq_alarm_dims,\n",
        "    treat_missing=\"breaching\",\n",
        "))\n",
        "\n",
        "created_alarms = [a for a in created_alarms if a]\n",
        "print(f\"\\n✓ {len(created_alarms)} CloudWatch alarms configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ig5hWw_S64pK",
      "metadata": {
        "id": "Ig5hWw_S64pK"
      },
      "source": [
        "---\n",
        "\n",
        "### 8.5 CloudWatch Dashboard\n",
        "\n",
        "A single comprehensive dashboard that surfaces all four monitoring pillars:\n",
        "endpoint performance, resource utilisation, model quality, and data quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1trSnTJd64pK",
      "metadata": {
        "id": "1trSnTJd64pK"
      },
      "source": [
        "#### 8.5.1 Create Comprehensive Monitoring Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k8JKRg8e64pL",
      "metadata": {
        "id": "k8JKRg8e64pL"
      },
      "outputs": [],
      "source": [
        "dashboard_name = f\"{project_name}-monitoring-dashboard\"\n",
        "\n",
        "def _metric_widget(title, metrics, x, y, w=12, h=6, stat=\"Average\",\n",
        "                   period=300, region=REGION, y_min=0, y_label=None):\n",
        "    props = {\n",
        "        \"title\": title,\n",
        "        \"metrics\": metrics,\n",
        "        \"period\": period,\n",
        "        \"stat\": stat,\n",
        "        \"region\": region,\n",
        "        \"view\": \"timeSeries\",\n",
        "        \"yAxis\": {\"left\": {\"min\": y_min}},\n",
        "    }\n",
        "    if y_label:\n",
        "        props[\"yAxis\"][\"left\"][\"label\"] = y_label\n",
        "    return {\"type\": \"metric\", \"x\": x, \"y\": y, \"width\": w, \"height\": h,\n",
        "            \"properties\": props}\n",
        "\n",
        "dashboard_body = {\n",
        "    \"widgets\": [\n",
        "        # ── Row 0: Section title ──────────────────────────────────────────\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"x\": 0, \"y\": 0, \"width\": 24, \"height\": 2,\n",
        "            \"properties\": {\n",
        "                \"markdown\": (\n",
        "                    \"# VenueSignal — ML Monitoring Dashboard\\n\"\n",
        "                    f\"**Endpoint:** `{endpoint_name}` | \"\n",
        "                    f\"**Region:** `{REGION}` | \"\n",
        "                    \"Metrics refresh every 5 min\"\n",
        "                )\n",
        "            },\n",
        "        },\n",
        "        # ── Row 1: Endpoint invocations & errors ──────────────────────────\n",
        "        _metric_widget(\n",
        "            title=\"Endpoint Invocations\",\n",
        "            metrics=[\n",
        "                [\"AWS/SageMaker\", \"Invocations\",\n",
        "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
        "                 {\"stat\": \"Sum\", \"label\": \"Invocations (Sum)\"}],\n",
        "            ],\n",
        "            stat=\"Sum\", x=0, y=2,\n",
        "        ),\n",
        "        _metric_widget(\n",
        "            title=\"Invocation Errors (4XX / 5XX)\",\n",
        "            metrics=[\n",
        "                [\"AWS/SageMaker\", \"Invocation4XXErrors\",\n",
        "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
        "                 {\"stat\": \"Sum\", \"color\": \"#ff7f0e\", \"label\": \"4XX Client Errors\"}],\n",
        "                [\"AWS/SageMaker\", \"Invocation5XXErrors\",\n",
        "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
        "                 {\"stat\": \"Sum\", \"color\": \"#d62728\", \"label\": \"5XX Server Errors\"}],\n",
        "            ],\n",
        "            stat=\"Sum\", x=12, y=2,\n",
        "        ),\n",
        "        # ── Row 2: Latency ────────────────────────────────────────────────\n",
        "        _metric_widget(\n",
        "            title=\"Model Latency (µs) — Avg & p99\",\n",
        "            metrics=[\n",
        "                [\"AWS/SageMaker\", \"ModelLatency\",\n",
        "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"Avg Latency\"}],\n",
        "                [\"...\", {\"stat\": \"p99\", \"label\": \"p99 Latency\", \"color\": \"#e377c2\"}],\n",
        "            ],\n",
        "            x=0, y=8, y_label=\"Microseconds\",\n",
        "        ),\n",
        "        _metric_widget(\n",
        "            title=\"Overhead Latency (µs)\",\n",
        "            metrics=[\n",
        "                [\"AWS/SageMaker\", \"OverheadLatency\",\n",
        "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"Avg Overhead\"}],\n",
        "            ],\n",
        "            x=12, y=8, y_label=\"Microseconds\",\n",
        "        ),\n",
        "        # ── Row 3: Resource utilisation ────────────────────────────────────\n",
        "        _metric_widget(\n",
        "            title=\"CPU Utilization (%)\",\n",
        "            metrics=[\n",
        "                [\"/aws/sagemaker/Endpoints\", \"CPUUtilization\",\n",
        "                 \"EndpointName\", endpoint_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"CPU %\"}],\n",
        "            ],\n",
        "            x=0, y=14, y_min=0, y_label=\"Percent\",\n",
        "        ),\n",
        "        _metric_widget(\n",
        "            title=\"Memory Utilization (%)\",\n",
        "            metrics=[\n",
        "                [\"/aws/sagemaker/Endpoints\", \"MemoryUtilization\",\n",
        "                 \"EndpointName\", endpoint_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"Memory %\"}],\n",
        "            ],\n",
        "            x=12, y=14, y_min=0, y_label=\"Percent\",\n",
        "        ),\n",
        "        _metric_widget(\n",
        "            title=\"Disk Utilization (%)\",\n",
        "            metrics=[\n",
        "                [\"/aws/sagemaker/Endpoints\", \"DiskUtilization\",\n",
        "                 \"EndpointName\", endpoint_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"Disk %\"}],\n",
        "            ],\n",
        "            x=0, y=20, w=12, y_min=0, y_label=\"Percent\",\n",
        "        ),\n",
        "        # ── Row 4: Model quality metrics (populated by MQ schedule) ───────\n",
        "        _metric_widget(\n",
        "            title=\"Model Quality — Recall (vs baseline)\",\n",
        "            metrics=[\n",
        "                [\"aws/sagemaker/Endpoints/model-metrics\", \"recall\",\n",
        "                 \"Endpoint\", endpoint_name,\n",
        "                 \"MonitoringSchedule\", mq_schedule_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"Recall\"}],\n",
        "            ],\n",
        "            x=12, y=20, y_min=0, y_label=\"Score\",\n",
        "        ),\n",
        "        # ── Row 5: Integration / custom metrics ───────────────────────────\n",
        "        _metric_widget(\n",
        "            title=\"Integration Test Pass Rate (%)\",\n",
        "            metrics=[\n",
        "                [\"VenueSignal/Integration\", \"IntegrationTestPassRate\",\n",
        "                 \"EndpointName\", endpoint_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"Pass Rate %\"}],\n",
        "            ],\n",
        "            stat=\"Average\", x=0, y=26, y_min=0,\n",
        "        ),\n",
        "        _metric_widget(\n",
        "            title=\"Health Check Latency (ms)\",\n",
        "            metrics=[\n",
        "                [\"VenueSignal/Integration\", \"HealthCheckLatencyMs\",\n",
        "                 \"EndpointName\", endpoint_name,\n",
        "                 {\"stat\": \"Average\", \"label\": \"Latency ms\"}],\n",
        "            ],\n",
        "            x=12, y=26, y_min=0, y_label=\"Milliseconds\",\n",
        "        ),\n",
        "    ]\n",
        "}\n",
        "\n",
        "try:\n",
        "    cloudwatch_client.put_dashboard(\n",
        "        DashboardName=dashboard_name,\n",
        "        DashboardBody=json.dumps(dashboard_body),\n",
        "    )\n",
        "    print(f\"✓ CloudWatch dashboard created/updated: {dashboard_name}\")\n",
        "    print(f\"\\nView dashboard at:\")\n",
        "    print(\n",
        "        f\"  https://console.aws.amazon.com/cloudwatch/home\"\n",
        "        f\"?region={REGION}#dashboards:name={dashboard_name}\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error creating dashboard: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uzJlbuZG64pL",
      "metadata": {
        "id": "uzJlbuZG64pL"
      },
      "source": [
        "---\n",
        "\n",
        "### 8.6 Model Performance Tracking\n",
        "\n",
        "Compare training-time performance across all models and save a persistent\n",
        "summary to S3 for the monitoring record.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zH5o9YbL64pL",
      "metadata": {
        "id": "zH5o9YbL64pL"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"MODEL PERFORMANCE TRACKING — Validation Set\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ── Retrieve stored results from Section 6 ─────────────────────────────────\n",
        "%store -r baseline_results\n",
        "%store -r xgb_results_val\n",
        "%store -r xgb_results_test\n",
        "\n",
        "b1 = baseline_results['baseline1']['val']\n",
        "b2 = baseline_results['baseline2']['val']\n",
        "xv = xgb_results_val\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "summary = pd.DataFrame([\n",
        "    {\"Model\": \"Baseline #1 (Heuristic)\",         \"Accuracy\": f\"{b1['accuracy']:.4f}\",\n",
        "     \"Precision\": f\"{b1['precision']:.4f}\",       \"Recall\": f\"{b1['recall']:.4f}\",\n",
        "     \"F1\": f\"{b1['f1']:.4f}\",                    \"RMSE\": f\"{b1['rmse']:.4f}\",\n",
        "     \"Within 1★\": f\"{b1['within_1.0_stars']*100:.2f}%\"},\n",
        "    {\"Model\": \"Baseline #2 (Logistic Reg.)\",      \"Accuracy\": f\"{b2['accuracy']:.4f}\",\n",
        "     \"Precision\": f\"{b2['precision']:.4f}\",       \"Recall\": f\"{b2['recall']:.4f}\",\n",
        "     \"F1\": f\"{b2['f1']:.4f}\",                    \"RMSE\": f\"{b2['rmse']:.4f}\",\n",
        "     \"Within 1★\": f\"{b2['within_1.0_stars']*100:.2f}%\"},\n",
        "    {\"Model\": \"XGBoost (Deployed)\",               \"Accuracy\": f\"{xv['accuracy']:.4f}\",\n",
        "     \"Precision\": f\"{xv['precision']:.4f}\",       \"Recall\": f\"{xv['recall']:.4f}\",\n",
        "     \"F1\": f\"{xv['f1']:.4f}\",                    \"RMSE\": f\"{xv['rmse']:.4f}\",\n",
        "     \"Within 1★\": f\"{xv['within_1.0_stars']*100:.2f}%\"},\n",
        "])\n",
        "display(summary)\n",
        "\n",
        "# ── XGBoost improvements ───────────────────────────────────────────────────\n",
        "def pct_change(new, old):\n",
        "    return (new - old) / old * 100\n",
        "\n",
        "print(\"\\nXGBoost improvements over Baseline #1:\")\n",
        "print(f\"  Accuracy : {pct_change(xv['accuracy'], b1['accuracy']):+.2f}%\")\n",
        "print(f\"  F1-Score : {pct_change(xv['f1'],       b1['f1']):+.2f}%\")\n",
        "print(f\"  RMSE     : {pct_change(b1['rmse'],     xv['rmse']):+.2f}%  (lower is better)\")\n",
        "\n",
        "print(\"\\nXGBoost improvements over Baseline #2:\")\n",
        "print(f\"  Accuracy : {pct_change(xv['accuracy'], b2['accuracy']):+.2f}%\")\n",
        "print(f\"  F1-Score : {pct_change(xv['f1'],       b2['f1']):+.2f}%\")\n",
        "print(f\"  RMSE     : {pct_change(b2['rmse'],     xv['rmse']):+.2f}%  (lower is better)\")\n",
        "\n",
        "# Save to S3 as monitoring artefact\n",
        "summary_local = \"/tmp/model_performance_summary.csv\"\n",
        "summary.to_csv(summary_local, index=False)\n",
        "s3_client.upload_file(\n",
        "    summary_local,\n",
        "    BASE_BUCKET_NAME,\n",
        "    f\"{MONITORING_PREFIX}performance_summary.csv\",\n",
        ")\n",
        "print(f\"\\n✓ Performance summary saved to s3://{MONITORING_DIR}performance_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z_UQmmNp64pM",
      "metadata": {
        "id": "Z_UQmmNp64pM"
      },
      "source": [
        "---\n",
        "\n",
        "### 8.7 Generate Monitoring Reports\n",
        "\n",
        "Generate comprehensive status reports for all monitoring schedules and\n",
        "upload them to S3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XGE7QuWh64pM",
      "metadata": {
        "id": "XGE7QuWh64pM"
      },
      "source": [
        "#### 8.7.1 List All Monitoring Schedules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GAdFjiLV64pM",
      "metadata": {
        "id": "GAdFjiLV64pM"
      },
      "outputs": [],
      "source": [
        "schedules = sagemaker_client.list_monitoring_schedules(\n",
        "    EndpointName=endpoint_name,\n",
        "    MaxResults=100,\n",
        ")\n",
        "print(\"=== Active Monitoring Schedules ===\")\n",
        "for s in schedules[\"MonitoringScheduleSummaries\"]:\n",
        "    print(f\"\\nSchedule : {s['MonitoringScheduleName']}\")\n",
        "    print(f\"  Type    : {s.get('MonitoringType', 'DataQuality')}\")\n",
        "    print(f\"  Status  : {s['MonitoringScheduleStatus']}\")\n",
        "    print(f\"  Created : {s['CreationTime']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GleP5Ae-64pM",
      "metadata": {
        "id": "GleP5Ae-64pM"
      },
      "source": [
        "#### 8.7.2 Check Latest Monitoring Executions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4s1dbM8v64pM",
      "metadata": {
        "id": "4s1dbM8v64pM"
      },
      "outputs": [],
      "source": [
        "def get_latest_execution(schedule_name):\n",
        "    try:\n",
        "        execs = sagemaker_client.list_monitoring_executions(\n",
        "            MonitoringScheduleName=schedule_name,\n",
        "            MaxResults=1,\n",
        "            SortBy=\"CreationTime\",\n",
        "            SortOrder=\"Descending\",\n",
        "        )\n",
        "        summaries = execs.get(\"MonitoringExecutionSummaries\", [])\n",
        "        return summaries[0] if summaries else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "print(\"=== Latest Monitoring Executions ===\")\n",
        "for s in schedules[\"MonitoringScheduleSummaries\"]:\n",
        "    name = s[\"MonitoringScheduleName\"]\n",
        "    execution = get_latest_execution(name)\n",
        "    print(f\"\\n{name}:\")\n",
        "    if execution:\n",
        "        print(f\"  Status   : {execution['MonitoringExecutionStatus']}\")\n",
        "        print(f\"  Scheduled: {execution.get('ScheduledTime', 'N/A')}\")\n",
        "        if \"ProcessingJobArn\" in execution:\n",
        "            print(f\"  Job      : {execution['ProcessingJobArn'].split('/')[-1]}\")\n",
        "        if \"FailureReason\" in execution:\n",
        "            print(f\"  Failure  : {execution['FailureReason']}\")\n",
        "    else:\n",
        "        print(\"  No executions yet — first run at the top of the next hour\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cvxT3Anb64pN",
      "metadata": {
        "id": "cvxT3Anb64pN"
      },
      "source": [
        "#### 8.7.3 Model Quality Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bEBTKGIG64pN",
      "metadata": {
        "id": "bEBTKGIG64pN"
      },
      "outputs": [],
      "source": [
        "print(\"=== Model Quality Monitoring Report ===\")\n",
        "try:\n",
        "    mq_files = S3Downloader.list(mq_results_uri)\n",
        "    if mq_files:\n",
        "        print(f\"Found {len(mq_files)} result file(s) in {mq_results_uri}\")\n",
        "        for fpath in sorted(mq_files, reverse=True):\n",
        "            if \"constraint_violations\" in fpath:\n",
        "                local = S3Downloader.download(fpath, \"/tmp/mq_report\")\n",
        "                with open(local[0]) as fh:\n",
        "                    v = json.load(fh)\n",
        "                viol = v.get(\"violations\", [])\n",
        "                if viol:\n",
        "                    print(f\"\\n⚠ {len(viol)} constraint violation(s) detected:\")\n",
        "                    for item in viol:\n",
        "                        print(f\"  - {item.get('metric_name', 'N/A')}: \"\n",
        "                              f\"{item.get('description', item)}\")\n",
        "                else:\n",
        "                    print(\"\\n✓ No model quality violations detected\")\n",
        "                break\n",
        "    else:\n",
        "        print(\"No model quality results yet — schedule must run at least once\")\n",
        "        print(\"First execution occurs at the top of the next hour.\")\n",
        "except Exception as e:\n",
        "    print(f\"Cannot retrieve model quality results: {e}\")\n",
        "    print(\"This is expected if the monitoring schedule has not yet run.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r5d41U9064pN",
      "metadata": {
        "id": "r5d41U9064pN"
      },
      "source": [
        "#### 8.7.4 Data Quality Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lzhf9W0o64pN",
      "metadata": {
        "id": "Lzhf9W0o64pN"
      },
      "outputs": [],
      "source": [
        "print(\"=== Data Quality Monitoring Report ===\")\n",
        "try:\n",
        "    dq_files = S3Downloader.list(dq_results_uri)\n",
        "    if dq_files:\n",
        "        print(f\"Found {len(dq_files)} result file(s) in {dq_results_uri}\")\n",
        "        for fpath in sorted(dq_files, reverse=True):\n",
        "            if \"constraint_violations\" in fpath:\n",
        "                local = S3Downloader.download(fpath, \"/tmp/dq_report\")\n",
        "                with open(local[0]) as fh:\n",
        "                    v = json.load(fh)\n",
        "                viol = v.get(\"violations\", [])\n",
        "                if viol:\n",
        "                    print(f\"\\n⚠ {len(viol)} data quality violation(s) detected:\")\n",
        "                    for item in viol:\n",
        "                        feat = item.get(\"feature_name\", \"N/A\")\n",
        "                        desc = item.get(\"description\", item)\n",
        "                        print(f\"  - Feature '{feat}': {desc}\")\n",
        "                else:\n",
        "                    print(\"\\n✓ No data quality violations detected\")\n",
        "                break\n",
        "    else:\n",
        "        print(\"No data quality results yet — schedule must run at least once\")\n",
        "        print(\"First execution occurs at the top of the next hour.\")\n",
        "except Exception as e:\n",
        "    print(f\"Cannot retrieve data quality results: {e}\")\n",
        "    print(\"This is expected if the monitoring schedule has not yet run.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "teyxzd3_64pN",
      "metadata": {
        "id": "teyxzd3_64pN"
      },
      "source": [
        "#### 8.7.5 Comprehensive Monitoring Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E_1Z6owi64pN",
      "metadata": {
        "id": "E_1Z6owi64pN"
      },
      "outputs": [],
      "source": [
        "# Retrieve CloudWatch alarms prefixed with the project name\n",
        "try:\n",
        "    alarms_resp = cloudwatch_client.describe_alarms(\n",
        "        AlarmNamePrefix=project_name,\n",
        "        MaxRecords=100,\n",
        "    )\n",
        "    alarm_list = alarms_resp.get(\"MetricAlarms\", [])\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not retrieve alarms: {e}\")\n",
        "    alarm_list = []\n",
        "\n",
        "# Query the latest infrastructure metrics\n",
        "latency_latest  = get_cw_metric(\"ModelLatency\")\n",
        "invoc_latest    = get_cw_metric(\"Invocations\",        statistic=\"Sum\")\n",
        "err4xx_latest   = get_cw_metric(\"Invocation4XXErrors\", statistic=\"Sum\")\n",
        "cpu_latest      = get_cw_metric(\"CPUUtilization\",     namespace=\"/aws/sagemaker/Endpoints\")\n",
        "mem_latest      = get_cw_metric(\"MemoryUtilization\",  namespace=\"/aws/sagemaker/Endpoints\")\n",
        "disk_latest     = get_cw_metric(\"DiskUtilization\",    namespace=\"/aws/sagemaker/Endpoints\")\n",
        "\n",
        "report = {\n",
        "    \"report_timestamp\":       datetime.now(timezone.utc).isoformat(),\n",
        "    \"endpoint_name\":          endpoint_name,\n",
        "    \"monitoring_schedules\": [\n",
        "        {\n",
        "            \"name\":   s[\"MonitoringScheduleName\"],\n",
        "            \"type\":   s.get(\"MonitoringType\", \"DataQuality\"),\n",
        "            \"status\": s[\"MonitoringScheduleStatus\"],\n",
        "        }\n",
        "        for s in schedules[\"MonitoringScheduleSummaries\"]\n",
        "    ],\n",
        "    \"infrastructure_metrics\": {\n",
        "        \"latency_us\":           latency_latest,\n",
        "        \"invocations\":          invoc_latest,\n",
        "        \"errors_4xx\":           err4xx_latest,\n",
        "        \"cpu_utilization_pct\":  cpu_latest,\n",
        "        \"memory_utilization_pct\": mem_latest,\n",
        "        \"disk_utilization_pct\": disk_latest,\n",
        "    },\n",
        "    \"cloudwatch_alarms\": [\n",
        "        {\n",
        "            \"name\":      a[\"AlarmName\"],\n",
        "            \"state\":     a[\"StateValue\"],\n",
        "            \"metric\":    a.get(\"MetricName\", \"Expression\"),\n",
        "            \"threshold\": a.get(\"Threshold\"),\n",
        "        }\n",
        "        for a in alarm_list\n",
        "    ],\n",
        "    \"integration_tests\": {\n",
        "        \"pass_rate\":    test_results[\"pass_rate\"],\n",
        "        \"passed\":       test_results[\"passed\"],\n",
        "        \"failed\":       test_results[\"failed\"],\n",
        "    },\n",
        "    \"health_check\": {\n",
        "        \"success_rate\": health_summary[\"success_rate\"],\n",
        "        \"avg_latency_ms\": health_summary[\"avg_latency_ms\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Persist report locally and upload to S3\n",
        "report_filename = f\"monitoring_report_{datetime.now(timezone.utc):%Y%m%d_%H%M%S}.json\"\n",
        "with open(report_filename, \"w\") as fh:\n",
        "    json.dump(report, fh, indent=2, default=str)\n",
        "\n",
        "report_s3_uri = S3Uploader.upload(report_filename, reports_uri)\n",
        "print(f\"✓ Monitoring report saved to: {report_s3_uri}\")\n",
        "print(\"\\n=== Report Summary ===\")\n",
        "print(json.dumps(report, indent=2, default=str))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nf5MWxqv64pO",
      "metadata": {
        "id": "Nf5MWxqv64pO"
      },
      "source": [
        "---\n",
        "\n",
        "### 8.8 Examine Execution Results\n",
        "\n",
        "The cells below should be run **after** the monitoring schedules have\n",
        "completed their first hourly execution (usually within ~20 minutes of the\n",
        "top of the next hour).  They download constraint-violation reports and\n",
        "analyse the CloudWatch metrics emitted by the model quality job.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AAAjtfXQ64pO",
      "metadata": {
        "id": "AAAjtfXQ64pO"
      },
      "source": [
        "#### 8.8.1 Wait for a Model Quality Execution to Complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tyBqwMQ-64pO",
      "metadata": {
        "id": "tyBqwMQ-64pO"
      },
      "outputs": [],
      "source": [
        "# Poll for the first successful execution — waits up to 90 minutes\n",
        "deadline = time.time() + 90 * 60\n",
        "print(f\"Polling for model quality execution (schedule: {mq_schedule_name})…\")\n",
        "print(\"Note: first execution fires at the top of the next hour + up to 20 min.\")\n",
        "\n",
        "latest_execution = None\n",
        "while time.time() < deadline:\n",
        "    try:\n",
        "        desc = sagemaker_client.describe_monitoring_schedule(\n",
        "            MonitoringScheduleName=mq_schedule_name\n",
        "        )\n",
        "        print(f\"  Schedule status: {desc['MonitoringScheduleStatus']}\")\n",
        "        last_exec = desc.get(\"LastMonitoringExecutionSummary\")\n",
        "        if last_exec:\n",
        "            print(f\"  Last execution : {last_exec['MonitoringExecutionStatus']}\")\n",
        "            if last_exec[\"MonitoringExecutionStatus\"] in (\"Completed\", \"CompletedWithViolations\"):\n",
        "                print(\"✓ Execution completed!\")\n",
        "                # Retrieve the execution object for deeper inspection\n",
        "                execs = xgboost_model_quality_monitor.list_executions()\n",
        "                if execs:\n",
        "                    latest_execution = execs[-1]\n",
        "                break\n",
        "            elif last_exec[\"MonitoringExecutionStatus\"] == \"Failed\":\n",
        "                print(f\"✗ Execution failed: {last_exec.get('FailureReason', 'unknown')}\")\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"  Polling error: {e}\")\n",
        "    sleep(30)\n",
        "\n",
        "if time.time() >= deadline:\n",
        "    print(\"⏱ Timeout: no completed execution within 90 minutes.\")\n",
        "    print(\"  Re-run this cell later or check the SageMaker console.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J6iIgL3K64pO",
      "metadata": {
        "id": "J6iIgL3K64pO"
      },
      "source": [
        "#### 8.8.2 Review Constraint Violations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0Nu7Bld64pO",
      "metadata": {
        "id": "a0Nu7Bld64pO"
      },
      "outputs": [],
      "source": [
        "if latest_execution is not None:\n",
        "    try:\n",
        "        violations = latest_execution.constraint_violations().body_dict.get(\n",
        "            \"violations\", []\n",
        "        )\n",
        "        if violations:\n",
        "            import pandas as pd\n",
        "            pd.options.display.max_colwidth = None\n",
        "            print(f\"⚠ {len(violations)} constraint violation(s):\")\n",
        "            display(pd.json_normalize(violations).head(20))\n",
        "        else:\n",
        "            print(\"✓ No constraint violations in this execution\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve violations: {e}\")\n",
        "else:\n",
        "    print(\"No completed execution available yet — run Section 8.8.1 first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r5zRLHlf64pO",
      "metadata": {
        "id": "r5zRLHlf64pO"
      },
      "source": [
        "#### 8.8.3 Analyse Model Quality CloudWatch Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dX6LTSLg64pP",
      "metadata": {
        "id": "dX6LTSLg64pP"
      },
      "outputs": [],
      "source": [
        "# List all model-quality metrics emitted by the monitor for this schedule\n",
        "cw_namespace = \"aws/sagemaker/Endpoints/model-metrics\"\n",
        "cw_dims = [\n",
        "    {\"Name\": \"Endpoint\",           \"Value\": endpoint_name},\n",
        "    {\"Name\": \"MonitoringSchedule\", \"Value\": mq_schedule_name},\n",
        "]\n",
        "\n",
        "paginator = cloudwatch_client.get_paginator(\"list_metrics\")\n",
        "mq_metric_names = []\n",
        "for page in paginator.paginate(Dimensions=cw_dims, Namespace=cw_namespace):\n",
        "    for metric in page.get(\"Metrics\", []):\n",
        "        mq_metric_names.append(metric[\"MetricName\"])\n",
        "\n",
        "if mq_metric_names:\n",
        "    print(f\"Model quality metrics in CloudWatch ({len(mq_metric_names)} found):\")\n",
        "    for name in mq_metric_names:\n",
        "        print(f\"  {name}\")\n",
        "else:\n",
        "    print(\"No model quality metrics in CloudWatch yet.\")\n",
        "    print(\"They appear after the first successful monitoring execution.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75ea52f8-d1ab-4521-90e1-87337712b49a",
      "metadata": {
        "id": "75ea52f8-d1ab-4521-90e1-87337712b49a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bf9bb1-a864-4d22-9c9c-b4757d565fce",
      "metadata": {
        "id": "72bf9bb1-a864-4d22-9c9c-b4757d565fce"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from datetime import datetime, timezone, timedelta\n",
        "\n",
        "cw = boto3.client(\"cloudwatch\", region_name=REGION)\n",
        "\n",
        "# Check 1: What metrics exist for this endpoint?\n",
        "print(f\"Checking CloudWatch metrics for: {endpoint_name}\\n\")\n",
        "response = cw.list_metrics(\n",
        "    Namespace=\"AWS/SageMaker\",\n",
        "    Dimensions=[{\"Name\": \"EndpointName\", \"Value\": endpoint_name}]\n",
        ")\n",
        "if response[\"Metrics\"]:\n",
        "    print(f\"✓ Found {len(response['Metrics'])} metric(s):\")\n",
        "    for m in response[\"Metrics\"]:\n",
        "        dims = {d[\"Name\"]: d[\"Value\"] for d in m[\"Dimensions\"]}\n",
        "        print(f\"  {m['MetricName']:30} VariantName={dims.get('VariantName','N/A')}\")\n",
        "else:\n",
        "    print(\"✗ NO metrics found in AWS/SageMaker namespace for this endpoint\")\n",
        "    print(\"  The traffic thread is not running or endpoint has never been invoked\")\n",
        "\n",
        "# Check 2: Is the traffic thread alive?\n",
        "import threading\n",
        "print(f\"\\nRunning threads:\")\n",
        "for t in threading.enumerate():\n",
        "    print(f\"  {t.name} (daemon={t.daemon}, alive={t.is_alive()})\")\n",
        "\n",
        "# Check 3: Send 5 test requests right now and verify\n",
        "print(f\"\\nSending 5 test requests to verify endpoint is reachable...\")\n",
        "runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
        "for i in range(5):\n",
        "    try:\n",
        "        payload = (\n",
        "            production_df[xgb_features]\n",
        "            .iloc[i]\n",
        "            .fillna(0)\n",
        "            .to_csv(header=None, index=False)\n",
        "            .strip()\n",
        "        )\n",
        "        resp = runtime.invoke_endpoint(\n",
        "            EndpointName=endpoint_name,\n",
        "            ContentType=\"text/csv\",\n",
        "            Body=payload,\n",
        "            InferenceId=str(i),\n",
        "        )\n",
        "        result = resp[\"Body\"].read().decode().strip()\n",
        "        print(f\"  Request {i}: ✓  prediction={result[:20]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Request {i}: ✗  {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f1b72a-174d-44de-be32-bc9a782d90fe",
      "metadata": {
        "id": "14f1b72a-174d-44de-be32-bc9a782d90fe"
      },
      "source": [
        "#### 8.9 Stop Monitors/Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6aa10e6-53e6-42d1-9f23-c2c8026a9536",
      "metadata": {
        "id": "d6aa10e6-53e6-42d1-9f23-c2c8026a9536"
      },
      "outputs": [],
      "source": [
        "#import threading\n",
        "\n",
        "# List all running threads\n",
        "#for t in threading.enumerate():\n",
        "#    print(t.name, t.daemon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a515e29-3a1a-4647-9dea-f345db9bb0b8",
      "metadata": {
        "id": "9a515e29-3a1a-4647-9dea-f345db9bb0b8"
      },
      "outputs": [],
      "source": [
        "#xgboost_model_quality_monitor.stop_monitoring_schedule()\n",
        "#data_quality_monitor.stop_monitoring_schedule()\n",
        "#print(\"✓ Both monitoring schedules stopped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6c8a57-2600-404f-90ec-6c045557346a",
      "metadata": {
        "id": "7f6c8a57-2600-404f-90ec-6c045557346a"
      },
      "outputs": [],
      "source": [
        "#Review Logs/Errors\n",
        "capture_files = sorted(S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\"))\n",
        "print(f\"Found {len(capture_files)} capture files in new path\")\n",
        "\n",
        "if capture_files:\n",
        "    # Read the most recent file\n",
        "    raw = S3Downloader.read_file(capture_files[-1])\n",
        "    first_record = json.loads(raw.split(\"\\n\")[0])\n",
        "    print(\"\\nCapture record structure:\")\n",
        "    print(json.dumps(first_record, indent=2))\n",
        "\n",
        "    # Show input and output data\n",
        "    print(\"\\n--- Input data (what was sent to endpoint) ---\")\n",
        "    print(first_record.get(\"captureData\", {}).get(\"endpointInput\", {}).get(\"data\", \"N/A\"))\n",
        "    print(\"\\n--- Output data (what endpoint returned) ---\")\n",
        "    print(first_record.get(\"captureData\", {}).get(\"endpointOutput\", {}).get(\"data\", \"N/A\"))\n",
        "    print(\"\\n--- Encoding ---\")\n",
        "    print(\"Input encoding: \", first_record.get(\"captureData\", {}).get(\"endpointInput\", {}).get(\"encoding\", \"N/A\"))\n",
        "    print(\"Output encoding:\", first_record.get(\"captureData\", {}).get(\"endpointOutput\", {}).get(\"encoding\", \"N/A\"))\n",
        "    print(\"\\n--- InferenceId ---\")\n",
        "    print(first_record.get(\"eventMetadata\", {}).get(\"inferenceId\", \"N/A\"))\n",
        "else:\n",
        "    print(\"No capture files found yet — send some traffic first\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a94ca904-056a-4739-b8d6-a04d4e26b8ad",
      "metadata": {
        "id": "a94ca904-056a-4739-b8d6-a04d4e26b8ad"
      },
      "outputs": [],
      "source": [
        "# Review Cloudwatch logs\n",
        "logs_client = boto3.client(\"logs\", region_name=REGION)\n",
        "\n",
        "try:\n",
        "    # SageMaker model monitor logs go to this log group\n",
        "    log_group = f\"/aws/sagemaker/ProcessingJobs\"\n",
        "\n",
        "    # Find the most recent model quality processing job\n",
        "    desc = sagemaker_client.describe_monitoring_schedule(\n",
        "        MonitoringScheduleName=mq_schedule_name\n",
        "    )\n",
        "    last_exec = desc.get(\"LastMonitoringExecutionSummary\", {})\n",
        "    job_arn = last_exec.get(\"ProcessingJobArn\", \"\")\n",
        "    job_name = job_arn.split(\"/\")[-1] if job_arn else None\n",
        "\n",
        "    if job_name:\n",
        "        print(f\"Checking logs for job: {job_name}\")\n",
        "        streams = logs_client.describe_log_streams(\n",
        "        logGroupName=log_group,\n",
        "        logStreamNamePrefix=job_name,\n",
        "    )\n",
        "        if streams[\"logStreams\"]:\n",
        "            stream_name = streams[\"logStreams\"][0][\"logStreamName\"]\n",
        "            events = logs_client.get_log_events(\n",
        "                logGroupName=log_group,\n",
        "                logStreamName=stream_name,\n",
        "                limit=50,\n",
        "            )\n",
        "            print(f\"\\nLast 50 log lines from: {stream_name}\\n\")\n",
        "            for e in events[\"events\"]:\n",
        "                print(e[\"message\"])\n",
        "        else:\n",
        "            print(\"No log streams found for this job\")\n",
        "    else:\n",
        "        print(\"No processing job found in last execution\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching logs: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1qtRWU0e7ENM",
      "metadata": {
        "id": "1qtRWU0e7ENM"
      },
      "source": [
        "## 9. CI/CD Pipeline <a id='section-9'></a>\n",
        "\n",
        "This section implements a **SageMaker Pipeline** to automate the full model lifecycle:\n",
        "\n",
        "| Step | Name | Description |\n",
        "|------|------|-------------|\n",
        "| 1 | Data Validation | Checks incoming data quality — fails pipeline if data is unusable |\n",
        "| 2 | Model Training | Retrains XGBoost on the latest feature data |\n",
        "| 3 | Model Evaluation | Computes F1, ROC-AUC, and MAE against the test set |\n",
        "| 4 | Quality Gate | Conditionally proceeds only if F1 ≥ threshold |\n",
        "| 5a | Model Registration | Registers passing model to SageMaker Model Registry |\n",
        "| 5b | Fail Step | Halts and reports failure if quality gate is not met |\n",
        "\n",
        "**Two execution modes are demonstrated:**\n",
        "- **Success run** — model meets quality threshold, registered to Model Registry\n",
        "- **Failure run** — threshold raised above model performance, pipeline halts at gate\n",
        "\n",
        "**S3 Naming Convention:** All CI/CD artifacts continue to follow the existing `yelp-aai540-group6-{account_id}` pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b6b8ca-2a92-4e39-90a2-e0d9009ee181",
      "metadata": {
        "id": "74b6b8ca-2a92-4e39-90a2-e0d9009ee181"
      },
      "source": [
        "### 9.0 — Cleanup / Reset Section 9\n",
        "\n",
        "Run this cell **before re-running section 9** if a previous attempt left partial resources behind.\n",
        "\n",
        "**What this deletes:**\n",
        "| Resource | Action |\n",
        "|----------|--------|\n",
        "| SageMaker Pipeline | Stopped + deleted |\n",
        "| All pipeline executions | Stopped if still running |\n",
        "| Model Package Group + all versions | All versions deleted, then group deleted |\n",
        "| S3 `cicd/` prefix | All objects cleared |\n",
        "\n",
        "**What this does NOT delete:**\n",
        "- Endpoint (Section 7)\n",
        "- Feature store (Section 5)\n",
        "- Trained model artifacts in `models/` (Section 6)\n",
        "- Monitoring schedules (Section 8)\n",
        "- All other S3 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089d7274-1b2e-471e-ada9-d3ba1bc8e6db",
      "metadata": {
        "id": "089d7274-1b2e-471e-ada9-d3ba1bc8e6db"
      },
      "outputs": [],
      "source": [
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  SECTION 9 CLEANUP — run before re-running section 9 from the top\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "import boto3\n",
        "import time\n",
        "\n",
        "PIPELINE_NAME            = \"venuesignal-cicd-pipeline\"\n",
        "MODEL_PACKAGE_GROUP_NAME = \"VenueSignalModelPackageGroup\"\n",
        "CICD_PREFIX              = \"cicd/\"\n",
        "\n",
        "s3_client  = boto3.client('s3',        region_name=REGION)\n",
        "sm_client  = boto3.client('sagemaker', region_name=REGION)\n",
        "\n",
        "errors = []\n",
        "\n",
        "\n",
        "# ── 1. Stop any running pipeline executions ───────────────────────────────────\n",
        "print(\"[1/4] Stopping any running pipeline executions...\")\n",
        "try:\n",
        "    executions = sm_client.list_pipeline_executions(\n",
        "        PipelineName=PIPELINE_NAME\n",
        "    )['PipelineExecutionSummaries']\n",
        "    running = [e for e in executions\n",
        "               if e['PipelineExecutionStatus'] == 'Executing']\n",
        "    if running:\n",
        "        for e in running:\n",
        "            sm_client.stop_pipeline_execution(\n",
        "                PipelineExecutionArn=e['PipelineExecutionArn']\n",
        "            )\n",
        "            print(f\"  Stopped: {e.get('PipelineExecutionDisplayName', e['PipelineExecutionArn'])}\")\n",
        "        print(f\"  Waiting 15s for executions to stop...\")\n",
        "        time.sleep(15)\n",
        "    else:\n",
        "        print(\"  No running executions found\")\n",
        "except sm_client.exceptions.ResourceNotFound:\n",
        "    print(\"  Pipeline does not exist yet — skipping\")\n",
        "except Exception as e:\n",
        "    print(f\"  ⚠  {e}\")\n",
        "    errors.append(f\"Stop executions: {e}\")\n",
        "\n",
        "\n",
        "# ── 2. Delete the pipeline ────────────────────────────────────────────────────\n",
        "print(\"\\n[2/4] Deleting pipeline...\")\n",
        "try:\n",
        "    sm_client.delete_pipeline(PipelineName=PIPELINE_NAME)\n",
        "    print(f\"  ✓ Deleted pipeline: {PIPELINE_NAME}\")\n",
        "except sm_client.exceptions.ResourceNotFound:\n",
        "    print(f\"  Pipeline '{PIPELINE_NAME}' not found — skipping\")\n",
        "except Exception as e:\n",
        "    print(f\"  ⚠  {e}\")\n",
        "    errors.append(f\"Delete pipeline: {e}\")\n",
        "\n",
        "\n",
        "# ── 3. Delete all model package versions then the group ───────────────────────\n",
        "print(\"\\n[3/4] Deleting model package group and all versions...\")\n",
        "try:\n",
        "    # Must delete all versions before the group\n",
        "    paginator = sm_client.get_paginator('list_model_packages')\n",
        "    pages = paginator.paginate(ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME)\n",
        "    version_count = 0\n",
        "    for page in pages:\n",
        "        for pkg in page['ModelPackageSummaryList']:\n",
        "            sm_client.delete_model_package(\n",
        "                ModelPackageName=pkg['ModelPackageArn']\n",
        "            )\n",
        "            version_count += 1\n",
        "            print(f\"  Deleted version {pkg['ModelPackageVersion']}: {pkg['ModelApprovalStatus']}\")\n",
        "\n",
        "    if version_count == 0:\n",
        "        print(\"  No model versions found\")\n",
        "\n",
        "    # Now delete the group itself\n",
        "    sm_client.delete_model_package_group(\n",
        "        ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME\n",
        "    )\n",
        "    print(f\"  ✓ Deleted model package group: {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "\n",
        "except sm_client.exceptions.ResourceNotFound:\n",
        "    print(f\"  Model package group '{MODEL_PACKAGE_GROUP_NAME}' not found — skipping\")\n",
        "except Exception as e:\n",
        "    print(f\"  ⚠  {e}\")\n",
        "    errors.append(f\"Delete model packages: {e}\")\n",
        "\n",
        "\n",
        "# ── 4. Clear S3 cicd/ prefix ─────────────────────────────────────────────────\n",
        "print(f\"\\n[4/4] Clearing S3 prefix: s3://{BASE_BUCKET_NAME}/{CICD_PREFIX}\")\n",
        "try:\n",
        "    paginator = s3_client.get_paginator('list_objects_v2')\n",
        "    pages = paginator.paginate(Bucket=BASE_BUCKET_NAME, Prefix=CICD_PREFIX)\n",
        "    obj_count = 0\n",
        "    for page in pages:\n",
        "        objects = page.get('Contents', [])\n",
        "        if objects:\n",
        "            s3_client.delete_objects(\n",
        "                Bucket=BASE_BUCKET_NAME,\n",
        "                Delete={'Objects': [{'Key': o['Key']} for o in objects]}\n",
        "            )\n",
        "            obj_count += len(objects)\n",
        "    if obj_count:\n",
        "        print(f\"  ✓ Deleted {obj_count} S3 object(s)\")\n",
        "    else:\n",
        "        print(\"  No S3 objects found under cicd/ — skipping\")\n",
        "except Exception as e:\n",
        "    print(f\"  ⚠  {e}\")\n",
        "    errors.append(f\"Clear S3: {e}\")\n",
        "\n",
        "\n",
        "# ── 5. Clear stale kernel variables ──────────────────────────────────────────\n",
        "print(\"\\n[5/4] Clearing stale kernel variables...\")\n",
        "cicd_vars = [\n",
        "    'pipeline', 'f1_threshold_param', 'sklearn_processor', 'xgb_estimator',\n",
        "    'xgb_container', 'step_validation', 'step_training', 'step_evaluation',\n",
        "    'step_register', 'step_fail', 'step_condition', 'evaluation_report',\n",
        "    'success_execution', 'failure_execution',\n",
        "    'success_execution_arn', 'failure_execution_arn', 'approved_model_arn',\n",
        "    'model_package_group_name', 'model_package_arn'\n",
        "]\n",
        "cleared = []\n",
        "for var in cicd_vars:\n",
        "    if var in globals():\n",
        "        del globals()[var]\n",
        "        cleared.append(var)\n",
        "if cleared:\n",
        "    print(f\"  ✓ Cleared {len(cleared)} kernel variable(s)\")\n",
        "else:\n",
        "    print(\"  No CI/CD kernel variables found\")\n",
        "\n",
        "\n",
        "# ── Summary ───────────────────────────────────────────────────────────────────\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "if not errors:\n",
        "    print(\"CLEANUP COMPLETE — Section 9 is ready for a clean rerun\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    print(\"  Proceed from section 9.2 (Model Package Group creation).\")\n",
        "    print(\"  Section 9.1 imports/config do not need to be rerun.\")\n",
        "else:\n",
        "    print(\"CLEANUP COMPLETE WITH WARNINGS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"  Non-fatal errors (likely resources that didn't exist):\")\n",
        "    for err in errors:\n",
        "        print(f\"    - {err}\")\n",
        "    print()\n",
        "    print(\"  Safe to proceed with rerun from section 9.2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3a645a-0184-482d-8f4e-97ef627d223e",
      "metadata": {
        "id": "7d3a645a-0184-482d-8f4e-97ef627d223e"
      },
      "source": [
        "### 9.1 CI/CD Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e91607-7ea0-46ed-9d6c-c94b406427b5",
      "metadata": {
        "id": "59e91607-7ea0-46ed-9d6c-c94b406427b5"
      },
      "outputs": [],
      "source": [
        "# CI/CD specific imports — builds on Section 1 imports already loaded\n",
        "from sagemaker.workflow.pipeline import Pipeline\n",
        "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
        "from sagemaker.workflow.condition_step import ConditionStep\n",
        "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
        "from sagemaker.workflow.properties import PropertyFile\n",
        "from sagemaker.workflow.functions import JsonGet\n",
        "from sagemaker.workflow.fail_step import FailStep\n",
        "from sagemaker.workflow.parameters import ParameterFloat, ParameterString\n",
        "from sagemaker.sklearn.processing import SKLearnProcessor\n",
        "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
        "from sagemaker.model import ModelPackage\n",
        "from sagemaker.inputs import TrainingInput\n",
        "\n",
        "print(\"CI/CD imports loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef395a20-5687-46b9-ab37-478d97baf2ac",
      "metadata": {
        "id": "ef395a20-5687-46b9-ab37-478d97baf2ac"
      },
      "outputs": [],
      "source": [
        "# ── Restore stored variables from earlier sections ──────────────────────────\n",
        "%store -r account_id\n",
        "%store -r REGION\n",
        "%store -r role\n",
        "%store -r BASE_BUCKET_NAME\n",
        "%store -r MODEL_PREFIX\n",
        "%store -r MONITORING_PREFIX\n",
        "%store -r FEATURE_PREFIX\n",
        "%store -r xgb_model_data\n",
        "%store -r xgb_features\n",
        "%store -r endpoint_name\n",
        "\n",
        "# ── CI/CD-specific S3 paths (same bucket, new prefix) ──────────────────────\n",
        "CICD_PREFIX          = \"cicd/\"\n",
        "CICD_SCRIPTS_PREFIX  = f\"{CICD_PREFIX}scripts\"\n",
        "CICD_EVAL_PREFIX     = f\"{CICD_PREFIX}evaluation\"\n",
        "CICD_LOGS_PREFIX     = f\"{CICD_PREFIX}logs\"\n",
        "\n",
        "CICD_SCRIPTS_S3      = f\"s3://{BASE_BUCKET_NAME}/{CICD_SCRIPTS_PREFIX}\"\n",
        "CICD_EVAL_S3         = f\"s3://{BASE_BUCKET_NAME}/{CICD_EVAL_PREFIX}\"\n",
        "XGB_TRAIN_S3         = f\"s3://{BASE_BUCKET_NAME}/models/xgboost-training/train.csv\"\n",
        "XGB_VAL_S3           = f\"s3://{BASE_BUCKET_NAME}/models/xgboost-training/validation.csv\"\n",
        "XGB_TEST_S3          = f\"s3://{BASE_BUCKET_NAME}/models/xgboost-training/test.csv\"\n",
        "XGB_OUTPUT_S3        = f\"s3://{BASE_BUCKET_NAME}/{MODEL_PREFIX}cicd-runs/\"\n",
        "\n",
        "# ── Pipeline identity ───────────────────────────────────────────────────────\n",
        "PIPELINE_NAME            = \"venuesignal-cicd-pipeline\"\n",
        "MODEL_PACKAGE_GROUP_NAME = \"VenueSignalModelPackageGroup\"\n",
        "\n",
        "# ── Quality gate threshold (pipeline parameter — easy to override at runtime)\n",
        "DEFAULT_F1_THRESHOLD     = 0.70   # Change to trigger failure demo\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CI/CD CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Bucket:              {BASE_BUCKET_NAME}\")\n",
        "print(f\"  CI/CD prefix:        {CICD_PREFIX}\")\n",
        "print(f\"  Scripts S3:          {CICD_SCRIPTS_S3}\")\n",
        "print(f\"  Evaluation S3:       {CICD_EVAL_S3}\")\n",
        "print(f\"  Pipeline name:       {PIPELINE_NAME}\")\n",
        "print(f\"  Model Package Group: {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "print(f\"  F1 gate threshold:   {DEFAULT_F1_THRESHOLD}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db53faf-08c1-45c7-9cdc-5f6b6927ab43",
      "metadata": {
        "id": "4db53faf-08c1-45c7-9cdc-5f6b6927ab43"
      },
      "source": [
        "### 9.2 Create Model Package Group in Model Registry\n",
        "\n",
        "The Model Registry tracks all approved versions of VenueSignal. This only needs to run once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0036624d-4035-43cd-b9da-9ed8b940d62d",
      "metadata": {
        "id": "0036624d-4035-43cd-b9da-9ed8b940d62d"
      },
      "outputs": [],
      "source": [
        "sm_client = boto3.client('sagemaker', region_name=REGION)\n",
        "\n",
        "try:\n",
        "    sm_client.create_model_package_group(\n",
        "        ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
        "        ModelPackageGroupDescription=(\n",
        "            \"VenueSignal XGBoost models for Yelp business rating prediction. \"\n",
        "            \"AAI-540 Group 6. Only F1 >= threshold models are registered.\"\n",
        "        )\n",
        "    )\n",
        "    print(f\"✓ Created Model Package Group: {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "except sm_client.exceptions.ConflictException:\n",
        "    print(f\"✓ Model Package Group already exists: {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "\n",
        "# Verify\n",
        "group_info = sm_client.describe_model_package_group(\n",
        "    ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME\n",
        ")\n",
        "print(f\"  Status: {group_info['ModelPackageGroupStatus']}\")\n",
        "print(f\"  ARN:    {group_info['ModelPackageGroupArn']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c119fbe5-4c6c-46c3-a3fd-e35af815e2b0",
      "metadata": {
        "id": "c119fbe5-4c6c-46c3-a3fd-e35af815e2b0"
      },
      "source": [
        "### 9.3 Write Pipeline Step Scripts\n",
        "\n",
        "Each ProcessingStep runs a self-contained Python script. We write them locally and upload to S3 so SageMaker can execute them on managed infrastructure.\n",
        "\n",
        "#### 9.3.1 Data Validation Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0afc3c9a-ff8d-4970-aaed-612265cf7aa6",
      "metadata": {
        "id": "0afc3c9a-ff8d-4970-aaed-612265cf7aa6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/tmp/cicd_scripts\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27cdbd06-b891-4c93-bd7e-847fb00b4a9a",
      "metadata": {
        "id": "27cdbd06-b891-4c93-bd7e-847fb00b4a9a"
      },
      "outputs": [],
      "source": [
        "%%writefile /tmp/cicd_scripts/data_validation.py\n",
        "\"\"\"\n",
        "VenueSignal CI/CD - Step 1: Data Validation\n",
        "============================================\n",
        "Validates incoming feature data before retraining.\n",
        "Raises an exception (failing the pipeline) if data quality is unacceptable.\n",
        "\n",
        "Checks performed:\n",
        "  1. File exists and is non-empty\n",
        "  2. Required feature columns are present\n",
        "  3. Missing value rate does not exceed threshold\n",
        "  4. Target column distribution is not severely imbalanced\n",
        "  5. Record count meets minimum for reliable training\n",
        "\"\"\"\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ── Thresholds ────────────────────────────────────────────────────────────────\n",
        "MAX_MISSING_RATE   = 0.20   # Fail if > 20% of any column is null\n",
        "MIN_RECORDS        = 1000   # Fail if fewer than 1,000 training records\n",
        "MIN_CLASS_RATE     = 0.05   # Fail if minority class < 5%\n",
        "REQUIRED_FEATURES  = [\n",
        "    'avg_review_stars',\n",
        "    'enhanced_parking_score',\n",
        "    'business_review_count',\n",
        "    'is_highly_rated'\n",
        "]\n",
        "\n",
        "\n",
        "def validate(input_path: str, output_path: str):\n",
        "    print(\"=\" * 60)\n",
        "    print(\"VenueSignal Data Validation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ── 1. Load data ──────────────────────────────────────────────────────────\n",
        "    csv_files = [f for f in os.listdir(input_path) if f.endswith('.csv')]\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in {input_path}\")\n",
        "\n",
        "    df = pd.read_csv(os.path.join(input_path, csv_files[0]))\n",
        "    print(f\"\\n[CHECK 1] File loaded: {csv_files[0]}\")\n",
        "    print(f\"          Shape: {df.shape}\")\n",
        "\n",
        "    errors = []\n",
        "\n",
        "    # ── 2. Minimum record count ───────────────────────────────────────────────\n",
        "    if len(df) < MIN_RECORDS:\n",
        "        errors.append(f\"Record count {len(df):,} is below minimum {MIN_RECORDS:,}\")\n",
        "        print(f\"[CHECK 2] ❌ FAIL — {errors[-1]}\")\n",
        "    else:\n",
        "        print(f\"[CHECK 2] ✓  Record count: {len(df):,} (min: {MIN_RECORDS:,})\")\n",
        "\n",
        "    # ── 3. Required columns ───────────────────────────────────────────────────\n",
        "    missing_cols = [c for c in REQUIRED_FEATURES if c not in df.columns]\n",
        "    if missing_cols:\n",
        "        errors.append(f\"Missing required columns: {missing_cols}\")\n",
        "        print(f\"[CHECK 3] ❌ FAIL — {errors[-1]}\")\n",
        "    else:\n",
        "        print(f\"[CHECK 3] ✓  All required columns present\")\n",
        "\n",
        "    # ── 4. Missing value rate ─────────────────────────────────────────────────\n",
        "    missing_rates = df.isnull().mean()\n",
        "    bad_cols = missing_rates[missing_rates > MAX_MISSING_RATE]\n",
        "    if len(bad_cols) > 0:\n",
        "        errors.append(f\"High null rate columns: {bad_cols.to_dict()}\")\n",
        "        print(f\"[CHECK 4] ❌ FAIL — {errors[-1]}\")\n",
        "    else:\n",
        "        print(f\"[CHECK 4] ✓  Missing rates within threshold (max: {MAX_MISSING_RATE:.0%})\")\n",
        "\n",
        "    # ── 5. Class balance (only if target column exists) ───────────────────────\n",
        "    if 'is_highly_rated' in df.columns:\n",
        "        class_dist = df['is_highly_rated'].value_counts(normalize=True)\n",
        "        minority_rate = class_dist.min()\n",
        "        if minority_rate < MIN_CLASS_RATE:\n",
        "            errors.append(f\"Severe class imbalance — minority class rate: {minority_rate:.2%}\")\n",
        "            print(f\"[CHECK 5] ❌ FAIL — {errors[-1]}\")\n",
        "        else:\n",
        "            print(f\"[CHECK 5] ✓  Class balance OK — minority rate: {minority_rate:.2%}\")\n",
        "            print(f\"             Distribution: {class_dist.to_dict()}\")\n",
        "\n",
        "    # ── Write validation report ───────────────────────────────────────────────\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    report = {\n",
        "        \"status\": \"PASS\" if not errors else \"FAIL\",\n",
        "        \"record_count\": int(len(df)),\n",
        "        \"column_count\": int(len(df.columns)),\n",
        "        \"errors\": errors,\n",
        "        \"missing_rates\": missing_rates.to_dict()\n",
        "    }\n",
        "    report_path = os.path.join(output_path, \"validation_report.json\")\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    print(f\"\\nValidation report written to: {report_path}\")\n",
        "\n",
        "    # ── Fail pipeline if errors found ────────────────────────────────────────\n",
        "    if errors:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"DATA VALIDATION FAILED — pipeline will not proceed\")\n",
        "        print(f\"{'='*60}\")\n",
        "        for e in errors:\n",
        "            print(f\"  ✗ {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DATA VALIDATION PASSED — proceeding to training\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input-path',  default='/opt/ml/processing/input/data')\n",
        "    parser.add_argument('--output-path', default='/opt/ml/processing/output')\n",
        "    args = parser.parse_args()\n",
        "    validate(args.input_path, args.output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c0013b-48e9-486f-a25b-cb51a95d5d25",
      "metadata": {
        "id": "b1c0013b-48e9-486f-a25b-cb51a95d5d25"
      },
      "source": [
        "#### 9.3.2 Model Evaluation Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4cd6e1-1454-4cc8-8b0a-db37aeebedb4",
      "metadata": {
        "id": "ae4cd6e1-1454-4cc8-8b0a-db37aeebedb4"
      },
      "outputs": [],
      "source": [
        "%%writefile /tmp/cicd_scripts/evaluate.py\n",
        "\"\"\"\n",
        "VenueSignal CI/CD - Step 3: Model Evaluation\n",
        "=============================================\n",
        "Loads the trained XGBoost model and evaluates against the test set.\n",
        "Writes a metrics JSON that the ConditionStep reads to decide\n",
        "whether the model is promoted to the Model Registry.\n",
        "\n",
        "Metrics written:\n",
        "  classification: accuracy, precision, recall, f1, roc_auc\n",
        "  regression:     mae, rmse\n",
        "  business:       within_0.5_stars, within_1.0_stars\n",
        "\"\"\"\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
        ")\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "def prob_to_stars(prob):\n",
        "    \"\"\"Convert probability to star rating (1–5 scale).\"\"\"\n",
        "    return 1 + (prob * 4)\n",
        "\n",
        "\n",
        "def evaluate(model_path: str, test_path: str, output_path: str):\n",
        "    print(\"=\" * 60)\n",
        "    print(\"VenueSignal Model Evaluation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ── 1. Load model from tarball ────────────────────────────────────────────\n",
        "    model_tar = os.path.join(model_path, 'model.tar.gz')\n",
        "    with tarfile.open(model_tar) as t:\n",
        "        t.extractall(path=model_path)\n",
        "\n",
        "    # XGBoost SageMaker saves as xgboost-model\n",
        "    model_file = os.path.join(model_path, 'xgboost-model')\n",
        "    booster = xgb.Booster()\n",
        "    booster.load_model(model_file)\n",
        "    print(f\"\\n[1] Model loaded from: {model_file}\")\n",
        "\n",
        "    # ── 2. Load test data ─────────────────────────────────────────────────────\n",
        "    csv_files = [f for f in os.listdir(test_path) if f.endswith('.csv')]\n",
        "    test_df = pd.read_csv(os.path.join(test_path, csv_files[0]))\n",
        "    print(f\"[2] Test data loaded: {test_df.shape}\")\n",
        "\n",
        "    # ── 3. Separate features and targets ─────────────────────────────────────\n",
        "    target_col   = 'is_highly_rated'\n",
        "    stars_col    = 'review_stars'\n",
        "    feature_cols = [c for c in test_df.columns if c not in [target_col, stars_col]]\n",
        "\n",
        "    X_test = test_df[feature_cols].fillna(0)\n",
        "    y_true_class = test_df[target_col].values\n",
        "    y_true_stars = test_df[stars_col].values if stars_col in test_df.columns else None\n",
        "\n",
        "    # ── 4. Predict ────────────────────────────────────────────────────────────\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "    y_prob = booster.predict(dtest)\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    print(f\"[3] Predictions generated for {len(y_pred):,} records\")\n",
        "\n",
        "    # ── 5. Compute metrics ────────────────────────────────────────────────────\n",
        "    accuracy  = float(accuracy_score(y_true_class, y_pred))\n",
        "    precision = float(precision_score(y_true_class, y_pred, zero_division=0))\n",
        "    recall    = float(recall_score(y_true_class, y_pred, zero_division=0))\n",
        "    f1        = float(f1_score(y_true_class, y_pred, zero_division=0))\n",
        "    roc_auc   = float(roc_auc_score(y_true_class, y_prob))\n",
        "\n",
        "    y_pred_stars = prob_to_stars(y_prob)\n",
        "    mae  = float(mean_absolute_error(y_true_stars, y_pred_stars)) if y_true_stars is not None else None\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true_stars, y_pred_stars))) if y_true_stars is not None else None\n",
        "\n",
        "    within_0_5 = float(np.mean(np.abs(y_true_stars - y_pred_stars) <= 0.5)) if y_true_stars is not None else None\n",
        "    within_1_0 = float(np.mean(np.abs(y_true_stars - y_pred_stars) <= 1.0)) if y_true_stars is not None else None\n",
        "\n",
        "    # ── 6. Build metrics report ───────────────────────────────────────────────\n",
        "    # NOTE: The top-level 'f1' key is what the ConditionStep reads via JsonGet.\n",
        "    metrics = {\n",
        "        \"f1\": f1,                        # <── ConditionStep reads this key\n",
        "        \"classification\": {\n",
        "            \"accuracy\":  accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\":    recall,\n",
        "            \"f1\":        f1,\n",
        "            \"roc_auc\":   roc_auc\n",
        "        },\n",
        "        \"regression\": {\n",
        "            \"mae\":  mae,\n",
        "            \"rmse\": rmse\n",
        "        },\n",
        "        \"business\": {\n",
        "            \"within_0.5_stars\": within_0_5,\n",
        "            \"within_1.0_stars\": within_1_0\n",
        "        },\n",
        "        \"test_record_count\": int(len(test_df))\n",
        "    }\n",
        "\n",
        "    # ── 7. Write report ───────────────────────────────────────────────────────\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    report_path = os.path.join(output_path, \"evaluation_report.json\")\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Accuracy:        {accuracy:.4f}\")\n",
        "    print(f\"  Precision:       {precision:.4f}\")\n",
        "    print(f\"  Recall:          {recall:.4f}\")\n",
        "    print(f\"  F1 Score:        {f1:.4f}  ← quality gate checks this\")\n",
        "    print(f\"  ROC-AUC:         {roc_auc:.4f}\")\n",
        "    if mae is not None:\n",
        "        print(f\"  MAE (stars):     {mae:.4f}\")\n",
        "        print(f\"  RMSE (stars):    {rmse:.4f}\")\n",
        "        print(f\"  Within 0.5★:     {within_0_5:.2%}\")\n",
        "        print(f\"  Within 1.0★:     {within_1_0:.2%}\")\n",
        "    print(f\"\\nReport written to: {report_path}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model-path', default='/opt/ml/processing/input/model')\n",
        "    parser.add_argument('--test-path',  default='/opt/ml/processing/input/test')\n",
        "    parser.add_argument('--output-path',default='/opt/ml/processing/output')\n",
        "    args = parser.parse_args()\n",
        "    evaluate(args.model_path, args.test_path, args.output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a51ee7-d93c-43d1-b146-e8d2b5646e99",
      "metadata": {
        "id": "32a51ee7-d93c-43d1-b146-e8d2b5646e99"
      },
      "outputs": [],
      "source": [
        "# Upload scripts to S3 so SageMaker Processing Jobs can access them\n",
        "from sagemaker.s3 import S3Uploader\n",
        "\n",
        "validation_script_s3 = S3Uploader.upload(\n",
        "    '/tmp/cicd_scripts/data_validation.py',\n",
        "    CICD_SCRIPTS_S3\n",
        ")\n",
        "evaluation_script_s3 = S3Uploader.upload(\n",
        "    '/tmp/cicd_scripts/evaluate.py',\n",
        "    CICD_SCRIPTS_S3\n",
        ")\n",
        "\n",
        "print(f\"✓ Validation script: {validation_script_s3}\")\n",
        "print(f\"✓ Evaluation script: {evaluation_script_s3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feffd37-c73a-46e9-a725-f12e089a9206",
      "metadata": {
        "id": "0feffd37-c73a-46e9-a725-f12e089a9206"
      },
      "source": [
        "### 9.4 Define the SageMaker Pipeline\n",
        "\n",
        "The pipeline uses a **`ParameterFloat`** for the F1 threshold so you can override it at runtime without editing the pipeline definition. This is how we trigger the failure demo cleanly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e9ca53-938e-44b3-9100-414a711f11ef",
      "metadata": {
        "id": "e4e9ca53-938e-44b3-9100-414a711f11ef"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "from sagemaker import image_uris\n",
        "\n",
        "# ── Pipeline parameter: override threshold at execution time ─────────────────\n",
        "f1_threshold_param = ParameterFloat(\n",
        "    name=\"F1Threshold\",\n",
        "    default_value=DEFAULT_F1_THRESHOLD\n",
        ")\n",
        "\n",
        "# ── Processors ───────────────────────────────────────────────────────────────\n",
        "# SKLearnProcessor runs the validation and evaluation scripts on managed infra\n",
        "sklearn_processor = SKLearnProcessor(\n",
        "    framework_version='1.0-1',\n",
        "    role=role,\n",
        "    instance_type='ml.m5.large',\n",
        "    instance_count=1,\n",
        "    sagemaker_session=sagemaker_session,\n",
        "    base_job_name='venuesignal-cicd'\n",
        ")\n",
        "\n",
        "# ── XGBoost container (same as Section 6) ────────────────────────────────────\n",
        "xgb_container = image_uris.retrieve(\n",
        "    framework='xgboost',\n",
        "    region=REGION,\n",
        "    version='1.7-1'\n",
        ")\n",
        "\n",
        "print(f\"✓ SKLearnProcessor configured\")\n",
        "print(f\"✓ XGBoost container: {xgb_container}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be2ef6d-9f2b-49d1-b80a-48dc608737c9",
      "metadata": {
        "id": "9be2ef6d-9f2b-49d1-b80a-48dc608737c9"
      },
      "outputs": [],
      "source": [
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  STEP 1 — DATA VALIDATION\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "step_validation = ProcessingStep(\n",
        "    name=\"VenueSignal-DataValidation\",\n",
        "    processor=sklearn_processor,\n",
        "    inputs=[\n",
        "        ProcessingInput(\n",
        "            source=XGB_TRAIN_S3,\n",
        "            destination='/opt/ml/processing/input/data'\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        ProcessingOutput(\n",
        "            source='/opt/ml/processing/output',\n",
        "            destination=f\"{CICD_EVAL_S3}validation/\"\n",
        "        )\n",
        "    ],\n",
        "    code='/tmp/cicd_scripts/data_validation.py'\n",
        ")\n",
        "print(\"✓ Step 1 defined: VenueSignal-DataValidation\")\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  STEP 2 — MODEL TRAINING\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "xgb_estimator = sagemaker.estimator.Estimator(\n",
        "    image_uri=xgb_container,\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type='ml.m5.xlarge',\n",
        "    output_path=XGB_OUTPUT_S3,\n",
        "    sagemaker_session=sagemaker_session,\n",
        "    base_job_name='venuesignal-cicd-train'\n",
        ")\n",
        "xgb_estimator.set_hyperparameters(\n",
        "    objective='binary:logistic',\n",
        "    num_round=100,\n",
        "    max_depth=6,\n",
        "    eta=0.3,\n",
        "    gamma=0,\n",
        "    min_child_weight=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric='auc',\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "step_training = TrainingStep(\n",
        "    name=\"VenueSignal-ModelTraining\",\n",
        "    estimator=xgb_estimator,\n",
        "    inputs={\n",
        "        'train':      TrainingInput(XGB_TRAIN_S3, content_type='text/csv'),\n",
        "        'validation': TrainingInput(XGB_VAL_S3,   content_type='text/csv')\n",
        "    },\n",
        "    depends_on=[step_validation]   # Only runs if validation passes\n",
        ")\n",
        "print(\"✓ Step 2 defined: VenueSignal-ModelTraining\")\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  STEP 3 — MODEL EVALUATION\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "# PropertyFile lets the ConditionStep read the JSON output of this step\n",
        "evaluation_report = PropertyFile(\n",
        "    name=\"VenueSignalEvaluationReport\",\n",
        "    output_name=\"evaluation\",\n",
        "    path=\"evaluation_report.json\"\n",
        ")\n",
        "\n",
        "step_evaluation = ProcessingStep(\n",
        "    name=\"VenueSignal-ModelEvaluation\",\n",
        "    processor=sklearn_processor,\n",
        "    inputs=[\n",
        "        ProcessingInput(\n",
        "            # Pull model artifacts from the training step output\n",
        "            source=step_training.properties.ModelArtifacts.S3ModelArtifacts,\n",
        "            destination='/opt/ml/processing/input/model'\n",
        "        ),\n",
        "        ProcessingInput(\n",
        "            source=XGB_TEST_S3,\n",
        "            destination='/opt/ml/processing/input/test'\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        ProcessingOutput(\n",
        "            output_name='evaluation',\n",
        "            source='/opt/ml/processing/output',\n",
        "            destination=f\"{CICD_EVAL_S3}model-eval/\"\n",
        "        )\n",
        "    ],\n",
        "    code='/tmp/cicd_scripts/evaluate.py',\n",
        "    property_files=[evaluation_report]\n",
        ")\n",
        "print(\"✓ Step 3 defined: VenueSignal-ModelEvaluation\")\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  STEP 4a — REGISTER MODEL (success branch)\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "from sagemaker.workflow.step_collections import RegisterModel\n",
        "\n",
        "step_register = RegisterModel(\n",
        "    name=\"VenueSignal-RegisterModel\",\n",
        "    estimator=xgb_estimator,\n",
        "    model_data=step_training.properties.ModelArtifacts.S3ModelArtifacts,\n",
        "    content_types=[\"text/csv\"],\n",
        "    response_types=[\"text/csv\"],\n",
        "    inference_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"],\n",
        "    transform_instances=[\"ml.m5.xlarge\"],\n",
        "    model_package_group_name=MODEL_PACKAGE_GROUP_NAME,\n",
        "    approval_status=\"PendingManualApproval\",\n",
        "    model_metrics=sagemaker.model_metrics.ModelMetrics(\n",
        "        model_statistics=sagemaker.model_metrics.MetricsSource(\n",
        "            s3_uri=f\"{CICD_EVAL_S3}model-eval/evaluation_report.json\",\n",
        "            content_type=\"application/json\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"✓ Step 4a defined: VenueSignal-RegisterModel\")\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  STEP 4b — FAIL STEP (failure branch)\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "step_fail = FailStep(\n",
        "    name=\"VenueSignal-QualityGateFailed\",\n",
        "    error_message=JsonGet(\n",
        "        step_name=step_evaluation.name,\n",
        "        property_file=evaluation_report,\n",
        "        json_path=\"f1\"        # Surfaces the actual F1 value in the error\n",
        "    )\n",
        ")\n",
        "print(\"✓ Step 4b defined: VenueSignal-QualityGateFailed\")\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  STEP 4 — CONDITION (quality gate)\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "# Reads f1 from the evaluation report JSON and compares to the pipeline parameter\n",
        "condition_f1 = ConditionGreaterThanOrEqualTo(\n",
        "    left=JsonGet(\n",
        "        step_name=step_evaluation.name,\n",
        "        property_file=evaluation_report,\n",
        "        json_path=\"f1\"\n",
        "    ),\n",
        "    right=f1_threshold_param\n",
        ")\n",
        "\n",
        "step_condition = ConditionStep(\n",
        "    name=\"VenueSignal-QualityGate\",\n",
        "    conditions=[condition_f1],\n",
        "    if_steps=[step_register],      # F1 >= threshold → register\n",
        "    else_steps=[step_fail]         # F1 <  threshold → fail\n",
        ")\n",
        "print(\"✓ Step 4 defined:  VenueSignal-QualityGate\")\n",
        "print(f\"                   Gate: F1 >= {{F1Threshold}} (default: {DEFAULT_F1_THRESHOLD})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66212adb-0c72-464c-b39a-d8252b4dd2f6",
      "metadata": {
        "id": "66212adb-0c72-464c-b39a-d8252b4dd2f6"
      },
      "outputs": [],
      "source": [
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "#  ASSEMBLE & REGISTER THE PIPELINE\n",
        "# ════════════════════════════════════════════════════════════════════════════\n",
        "pipeline = Pipeline(\n",
        "    name=PIPELINE_NAME,\n",
        "    parameters=[f1_threshold_param],\n",
        "    steps=[\n",
        "        step_validation,   # Step 1\n",
        "        step_training,     # Step 2  (depends_on step_validation)\n",
        "        step_evaluation,   # Step 3\n",
        "        step_condition     # Step 4  (branches to register or fail)\n",
        "    ],\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "# Upsert: creates the pipeline if it doesn't exist, updates it if it does\n",
        "pipeline_arn = pipeline.upsert(role_arn=role)\n",
        "print(f\"✓ Pipeline registered\")\n",
        "print(f\"  Name: {PIPELINE_NAME}\")\n",
        "print(f\"  ARN:  {pipeline_arn['PipelineArn']}\")\n",
        "print()\n",
        "print(\"DAG structure:\")\n",
        "print(\"  [1] DataValidation\")\n",
        "print(\"       └─► [2] ModelTraining\")\n",
        "print(\"                └─► [3] ModelEvaluation\")\n",
        "print(\"                         └─► [4] QualityGate (F1 >= threshold?)\")\n",
        "print(\"                                  ├─ YES ─► [4a] RegisterModel\")\n",
        "print(\"                                  └─ NO  ─► [4b] QualityGateFailed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd735cd-a8ac-4e63-8879-d34bc2b4e53a",
      "metadata": {
        "id": "2bd735cd-a8ac-4e63-8879-d34bc2b4e53a"
      },
      "source": [
        "### 9.5 Execute Pipeline — Success Case\n",
        "\n",
        "Run with the default threshold (`0.70`). The XGBoost model trained in Section 6 should exceed this, so the pipeline will proceed to Model Registration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c45e4b7-65ec-4a27-a405-45698350323d",
      "metadata": {
        "id": "2c45e4b7-65ec-4a27-a405-45698350323d"
      },
      "outputs": [],
      "source": [
        "# ── Reconnect to existing registered pipeline ─────────────────────────────────\n",
        "# Use this whenever 'pipeline' is not defined in the current kernel session\n",
        "# (e.g. after a kernel restart, or jumping directly to section 9.5)\n",
        "\n",
        "from sagemaker.workflow.pipeline import Pipeline\n",
        "from sagemaker.workflow.parameters import ParameterFloat\n",
        "\n",
        "PIPELINE_NAME        = \"venuesignal-cicd-pipeline\"\n",
        "DEFAULT_F1_THRESHOLD = 0.70\n",
        "\n",
        "# Verify the pipeline exists in AWS before reconnecting\n",
        "try:\n",
        "    desc = sm_client.describe_pipeline(PipelineName=PIPELINE_NAME)\n",
        "    print(f\"✓ Pipeline found in AWS\")\n",
        "    print(f\"  Name:    {desc['PipelineName']}\")\n",
        "    print(f\"  Status:  {desc['PipelineStatus']}\")\n",
        "    print(f\"  Updated: {desc['LastModifiedTime'].strftime('%Y-%m-%d %H:%M UTC')}\")\n",
        "except sm_client.exceptions.ResourceNotFound:\n",
        "    print(\"✗ Pipeline not found in AWS.\")\n",
        "    print(\"  Section 9.4 has not been run yet in any session.\")\n",
        "    print(\"  Please run section 9.4 fully before proceeding.\")\n",
        "    raise\n",
        "\n",
        "# Reconnect — Pipeline() with just a name and session is enough to call .start()\n",
        "pipeline = Pipeline(\n",
        "    name=PIPELINE_NAME,\n",
        "    sagemaker_session=sagemaker_session\n",
        ")\n",
        "\n",
        "# Also rebuild the threshold parameter reference (used in section 9.6 failure demo)\n",
        "f1_threshold_param = ParameterFloat(\n",
        "    name=\"F1Threshold\",\n",
        "    default_value=DEFAULT_F1_THRESHOLD\n",
        ")\n",
        "\n",
        "print()\n",
        "print(f\"✓ pipeline object reconnected — ready to call pipeline.start()\")\n",
        "print(f\"✓ f1_threshold_param rebuilt (default: {DEFAULT_F1_THRESHOLD})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60c10c4-dad3-443e-9f96-8b980e73a69b",
      "metadata": {
        "id": "f60c10c4-dad3-443e-9f96-8b980e73a69b"
      },
      "outputs": [],
      "source": [
        "# ── Start a SUCCESS execution ─────────────────────────────────────────────────\n",
        "# Uses the default F1Threshold = 0.70 (model should pass)\n",
        "\n",
        "success_execution = pipeline.start(\n",
        "    execution_display_name=\"cicd-success-demo\",\n",
        "    execution_description=\"Success run: F1 threshold set below expected model performance\",\n",
        "    parameters={\"F1Threshold\": 0.70}\n",
        ")\n",
        "\n",
        "print(f\"✓ Pipeline execution started\")\n",
        "print(f\"  Execution ARN: {success_execution.arn}\")\n",
        "print(f\"  Execution name: cicd-success-demo\")\n",
        "print(f\"  F1 Threshold: 0.70\")\n",
        "print()\n",
        "print(\"View in SageMaker Studio:\")\n",
        "print(f\"  Pipelines → {PIPELINE_NAME} → cicd-success-demo\")\n",
        "print()\n",
        "print(\"Running wait() to track execution. This takes ~15-25 minutes.\")\n",
        "print(\"You can also open SageMaker Studio to watch the DAG update live.\")\n",
        "print(\"To skip waiting: comment out the next cell and track manually.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7ae6a4-ae50-4e84-bff1-a05f6ca14d33",
      "metadata": {
        "id": "3e7ae6a4-ae50-4e84-bff1-a05f6ca14d33"
      },
      "outputs": [],
      "source": [
        "# ── Wait for success execution to complete ────────────────────────────────────\n",
        "# Comment this out if you want to proceed without waiting\n",
        "\n",
        "print(\"Waiting for pipeline execution to complete...\")\n",
        "print(\"(Each step status will print as they finish)\")\n",
        "print()\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def get_steps(execution):\n",
        "    \"\"\"Safely extract step list regardless of SDK response shape.\"\"\"\n",
        "    raw = execution.list_steps()\n",
        "    if isinstance(raw, list):\n",
        "        return raw\n",
        "    if isinstance(raw, dict):\n",
        "        return raw.get('PipelineExecutionSteps', [])\n",
        "    return []\n",
        "\n",
        "\n",
        "print(\"Waiting for pipeline execution to complete...\")\n",
        "print(\"(Each step status will print as they finish)\")\n",
        "print()\n",
        "\n",
        "seen_steps = set()\n",
        "while True:\n",
        "    for step in get_steps(success_execution):\n",
        "        key = (step['StepName'], step['StepStatus'])\n",
        "        if key not in seen_steps:\n",
        "            seen_steps.add(key)\n",
        "            ts   = datetime.now().strftime('%H:%M:%S')\n",
        "            icon = \"✓\" if step['StepStatus'] == 'Succeeded' \\\n",
        "                   else \"→\" if step['StepStatus'] == 'Executing' \\\n",
        "                   else \"✗\"\n",
        "            print(f\"  [{ts}] {icon} {step['StepName']}: {step['StepStatus']}\")\n",
        "\n",
        "    desc   = sm_client.describe_pipeline_execution(\n",
        "                 PipelineExecutionArn=success_execution.arn\n",
        "             )\n",
        "    status = desc['PipelineExecutionStatus']\n",
        "    if status in ('Succeeded', 'Failed', 'Stopped'):\n",
        "        print()\n",
        "        print(f\"Pipeline execution complete: {status}\")\n",
        "        break\n",
        "    time.sleep(30)\n",
        "\n",
        "success_execution_arn = success_execution.arn\n",
        "%store success_execution_arn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24bffdd-3c8a-430d-9eaf-1b46612c2c82",
      "metadata": {
        "id": "f24bffdd-3c8a-430d-9eaf-1b46612c2c82"
      },
      "outputs": [],
      "source": [
        "# ── Print execution summary ───────────────────────────────────────────────────\n",
        "\n",
        "def get_steps(execution):\n",
        "    \"\"\"Safely extract step list regardless of SDK response shape.\"\"\"\n",
        "    raw = execution.list_steps()\n",
        "    if isinstance(raw, list):\n",
        "        return raw\n",
        "    if isinstance(raw, dict):\n",
        "        return raw.get('PipelineExecutionSteps', [])\n",
        "    return []\n",
        "\n",
        "\n",
        "# ── Overall execution status ──────────────────────────────────────────────────\n",
        "desc   = sm_client.describe_pipeline_execution(\n",
        "             PipelineExecutionArn=success_execution.arn\n",
        "         )\n",
        "status = desc['PipelineExecutionStatus']\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXECUTION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Status:      {status}\")\n",
        "print(f\"  Started:     {desc['CreationTime']}\")\n",
        "print(f\"  Last update: {desc['LastModifiedTime']}\")\n",
        "print()\n",
        "\n",
        "# ── Per-step results ──────────────────────────────────────────────────────────\n",
        "steps = get_steps(success_execution)\n",
        "print(\"  Step results:\")\n",
        "failed_steps = []\n",
        "for step in steps:\n",
        "    icon = \"✓\" if step['StepStatus'] == 'Succeeded' else \"✗\"\n",
        "    print(f\"    {icon} {step['StepName']}: {step['StepStatus']}\")\n",
        "    if step['StepStatus'] in ('Failed', 'Stopped'):\n",
        "        failed_steps.append(step)\n",
        "\n",
        "# ── Failure diagnosis ─────────────────────────────────────────────────────────\n",
        "if failed_steps:\n",
        "    print()\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FAILURE DIAGNOSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    for step in failed_steps:\n",
        "        print(f\"\\n  Failed step: {step['StepName']}\")\n",
        "\n",
        "        # Failure reason from the step itself\n",
        "        reason = step.get('FailureReason', '')\n",
        "        if reason:\n",
        "            print(f\"  Reason:      {reason}\")\n",
        "\n",
        "        # For ProcessingJob steps — get the actual CloudWatch log\n",
        "        metadata = step.get('Metadata', {})\n",
        "\n",
        "        if 'ProcessingJob' in metadata:\n",
        "            job_arn  = metadata['ProcessingJob'].get('Arn', '')\n",
        "            job_name = job_arn.split('/')[-1] if job_arn else ''\n",
        "            if job_name:\n",
        "                print(f\"  Job name:    {job_name}\")\n",
        "                try:\n",
        "                    job_desc = sm_client.describe_processing_job(\n",
        "                        ProcessingJobName=job_name\n",
        "                    )\n",
        "                    print(f\"  Exit msg:    {job_desc.get('ExitMessage', 'N/A')}\")\n",
        "                    print(f\"  Failure:     {job_desc.get('FailureReason', 'N/A')}\")\n",
        "                    print()\n",
        "                    print(\"  To view full logs run:\")\n",
        "                    print(f\"    CloudWatch → Log groups → /aws/sagemaker/ProcessingJobs\")\n",
        "                    print(f\"    Log stream prefix: {job_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Could not retrieve job details: {e}\")\n",
        "\n",
        "        if 'TrainingJob' in metadata:\n",
        "            job_arn  = metadata['TrainingJob'].get('Arn', '')\n",
        "            job_name = job_arn.split('/')[-1] if job_arn else ''\n",
        "            if job_name:\n",
        "                print(f\"  Job name:    {job_name}\")\n",
        "                try:\n",
        "                    job_desc = sm_client.describe_training_job(\n",
        "                        TrainingJobName=job_name\n",
        "                    )\n",
        "                    print(f\"  Exit msg:    {job_desc.get('SecondaryStatus', 'N/A')}\")\n",
        "                    print(f\"  Failure:     {job_desc.get('FailureReason', 'N/A')}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Could not retrieve job details: {e}\")\n",
        "\n",
        "    print()\n",
        "    print(\"  Most likely causes:\")\n",
        "    print(\"    1. Training data not found at XGB_TRAIN_S3 path (check Section 5 ran)\")\n",
        "    print(\"    2. Evaluation script failed to find model.tar.gz (check training completed)\")\n",
        "    print(\"    3. IAM/LabRole permission on S3 path\")\n",
        "    print()\n",
        "    print(\"  Quick path checks:\")\n",
        "    s3 = boto3.client('s3', region_name=REGION)\n",
        "    paths_to_check = [\n",
        "        (BASE_BUCKET_NAME, 'feature-store/training-data/train.csv',      'Training data'),\n",
        "        (BASE_BUCKET_NAME, 'feature-store/training-data/validation.csv', 'Validation data'),\n",
        "        (BASE_BUCKET_NAME, 'feature-store/training-data/test.csv',       'Test data'),\n",
        "    ]\n",
        "    for bucket, key, label in paths_to_check:\n",
        "        try:\n",
        "            s3.head_object(Bucket=bucket, Key=key)\n",
        "            print(f\"    ✓ {label}: s3://{bucket}/{key}\")\n",
        "        except Exception:\n",
        "            print(f\"    ✗ {label} NOT FOUND: s3://{bucket}/{key}\")\n",
        "            print(f\"      → Re-run Section 5 to regenerate training splits\")\n",
        "\n",
        "elif status == 'Succeeded':\n",
        "    print()\n",
        "    # Retrieve evaluation metrics from S3\n",
        "    try:\n",
        "        s3_client = boto3.client('s3', region_name=REGION)\n",
        "        eval_obj  = s3_client.get_object(\n",
        "            Bucket=BASE_BUCKET_NAME,\n",
        "            Key=f\"{CICD_EVAL_PREFIX}model-eval/evaluation_report.json\"\n",
        "        )\n",
        "        eval_metrics = json.loads(eval_obj['Body'].read())\n",
        "        print(\"  Evaluation metrics:\")\n",
        "        print(f\"    F1 Score:  {eval_metrics['f1']:.4f}\")\n",
        "        print(f\"    Accuracy:  {eval_metrics['classification']['accuracy']:.4f}\")\n",
        "        print(f\"    ROC-AUC:   {eval_metrics['classification']['roc_auc']:.4f}\")\n",
        "        if eval_metrics['regression']['mae']:\n",
        "            print(f\"    MAE:       {eval_metrics['regression']['mae']:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  (Could not retrieve evaluation report: {e})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8755e8b4-af57-4b73-8a48-ecb8c506604c",
      "metadata": {
        "id": "8755e8b4-af57-4b73-8a48-ecb8c506604c"
      },
      "source": [
        "### 9.6 Execute Pipeline — Failure Case (Video Demo)\n",
        "\n",
        "We re-run the pipeline with `F1Threshold = 0.99` so the ConditionStep routes to the `FailStep`. This demonstrates the quality gate working as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d725c9-358e-4200-a2df-269a18437a06",
      "metadata": {
        "id": "94d725c9-358e-4200-a2df-269a18437a06"
      },
      "outputs": [],
      "source": [
        "# ── Start a FAILURE execution ─────────────────────────────────────────────────\n",
        "# Override F1Threshold to 0.99 — model cannot reach this, pipeline will fail\n",
        "\n",
        "failure_execution = pipeline.start(\n",
        "    execution_display_name=\"cicd-failure-demo\",\n",
        "    execution_description=\"Failure run: F1 threshold set above model capability to demo quality gate\",\n",
        "    parameters={\"F1Threshold\": 0.99}\n",
        ")\n",
        "\n",
        "print(f\"✓ Failure demo execution started\")\n",
        "print(f\"  Execution ARN: {failure_execution.arn}\")\n",
        "print(f\"  F1 Threshold: 0.99  ← model cannot reach this\")\n",
        "print()\n",
        "print(\"Expected flow:\")\n",
        "print(\"  DataValidation  → Succeeded\")\n",
        "print(\"  ModelTraining   → Succeeded\")\n",
        "print(\"  ModelEvaluation → Succeeded\")\n",
        "print(\"  QualityGate     → Condition evaluates False (F1 < 0.99)\")\n",
        "print(\"  QualityGateFailed → Failed  ← this is what we want to see\")\n",
        "\n",
        "failure_execution_arn = failure_execution.arn\n",
        "%store failure_execution_arn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce9e6b1-f4be-4b11-8e35-3c17cc35a64f",
      "metadata": {
        "id": "0ce9e6b1-f4be-4b11-8e35-3c17cc35a64f"
      },
      "outputs": [],
      "source": [
        "# ── Wait for failure execution ────────────────────────────────────────────────\n",
        "# Fixed: handles list_steps() returning a list OR a dict\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def get_steps(execution):\n",
        "    \"\"\"Safely extract step list regardless of SDK response shape.\"\"\"\n",
        "    raw = execution.list_steps()\n",
        "    if isinstance(raw, list):\n",
        "        return raw\n",
        "    if isinstance(raw, dict):\n",
        "        return raw.get('PipelineExecutionSteps', [])\n",
        "    return []\n",
        "\n",
        "\n",
        "print(\"Waiting for failure demo execution to complete...\")\n",
        "print()\n",
        "\n",
        "seen_steps = set()\n",
        "while True:\n",
        "    for step in get_steps(failure_execution):\n",
        "        key = (step['StepName'], step['StepStatus'])\n",
        "        if key not in seen_steps:\n",
        "            seen_steps.add(key)\n",
        "            ts   = datetime.now().strftime('%H:%M:%S')\n",
        "            icon = \"✓\" if step['StepStatus'] == 'Succeeded' \\\n",
        "                   else \"→\" if step['StepStatus'] == 'Executing' \\\n",
        "                   else \"✗\"\n",
        "            print(f\"  [{ts}] {icon} {step['StepName']}: {step['StepStatus']}\")\n",
        "\n",
        "    desc   = sm_client.describe_pipeline_execution(\n",
        "                 PipelineExecutionArn=failure_execution.arn\n",
        "             )\n",
        "    status = desc['PipelineExecutionStatus']\n",
        "    if status in ('Succeeded', 'Failed', 'Stopped'):\n",
        "        print()\n",
        "        print(f\"Pipeline execution complete: {status}\")\n",
        "        break\n",
        "    time.sleep(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15750b87-ea71-44c0-87d6-71aa4026e5f7",
      "metadata": {
        "id": "15750b87-ea71-44c0-87d6-71aa4026e5f7"
      },
      "outputs": [],
      "source": [
        "# ── Failure execution summary ─────────────────────────────────────────────────\n",
        "# Fixed: handles list_steps() returning a list OR a dict\n",
        "\n",
        "desc   = sm_client.describe_pipeline_execution(\n",
        "             PipelineExecutionArn=failure_execution.arn\n",
        "         )\n",
        "status = desc['PipelineExecutionStatus']\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FAILURE EXECUTION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Status:      {status}\")\n",
        "print(f\"  Started:     {desc['CreationTime']}\")\n",
        "print(f\"  Last update: {desc['LastModifiedTime']}\")\n",
        "print()\n",
        "\n",
        "steps = get_steps(failure_execution)\n",
        "print(\"  Step results:\")\n",
        "for step in steps:\n",
        "    icon = \"✓\" if step['StepStatus'] == 'Succeeded' else \"✗\"\n",
        "    print(f\"    {icon} {step['StepName']}: {step['StepStatus']}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ── Distinguish expected failure (QualityGate) from unexpected failure ─────────\n",
        "step_names   = {s['StepName']: s['StepStatus'] for s in steps}\n",
        "gate_failed  = step_names.get('VenueSignal-QualityGateFailed') == 'Failed'\n",
        "infra_failed = any(\n",
        "    step_names.get(s) == 'Failed'\n",
        "    for s in ['VenueSignal-DataValidation',\n",
        "               'VenueSignal-ModelTraining',\n",
        "               'VenueSignal-ModelEvaluation']\n",
        ")\n",
        "\n",
        "if gate_failed and not infra_failed:\n",
        "    print(\"✓ Quality gate worked as intended.\")\n",
        "    print(\"  All pipeline steps succeeded, but F1 did not reach 0.99.\")\n",
        "    print(\"  The FailStep triggered correctly — model was blocked from registration.\")\n",
        "    print()\n",
        "    print(\"  This is the state to record for your video demonstration.\")\n",
        "    print(\"  In SageMaker Studio the DAG will show:\")\n",
        "    print(\"    ✓ DataValidation\")\n",
        "    print(\"    ✓ ModelTraining\")\n",
        "    print(\"    ✓ ModelEvaluation\")\n",
        "    print(\"    ✓ QualityGate  (condition evaluated — routed to else branch)\")\n",
        "    print(\"    ✗ QualityGateFailed  (red — expected)\")\n",
        "elif infra_failed:\n",
        "    print(\"⚠  An infrastructure step failed before reaching the quality gate.\")\n",
        "    print(\"   This is NOT the intended failure — check the step details above.\")\n",
        "    print(\"   Resolve the underlying issue in the success run first,\")\n",
        "    print(\"   then re-run this failure demo.\")\n",
        "else:\n",
        "    print(f\"  Pipeline status: {status}\")\n",
        "    print(\"  Check SageMaker Studio for the full DAG view.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23b2775c-771a-40b7-99b1-cc89d6cca242",
      "metadata": {
        "id": "23b2775c-771a-40b7-99b1-cc89d6cca242"
      },
      "source": [
        "### 9.7 View Registered Models in Model Registry\n",
        "\n",
        "Only the **success execution** should have registered a model. This cell confirms the registry state and shows how to approve a model for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f38f00a0-80f4-4477-bdff-d281592b9352",
      "metadata": {
        "id": "f38f00a0-80f4-4477-bdff-d281592b9352"
      },
      "outputs": [],
      "source": [
        "# ── List all model versions in the package group ──────────────────────────────\n",
        "print(\"=\" * 60)\n",
        "print(f\"MODEL REGISTRY: {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    model_packages = sm_client.list_model_packages(\n",
        "        ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
        "        SortBy='CreationTime',\n",
        "        SortOrder='Descending'\n",
        "    )\n",
        "\n",
        "    packages = model_packages['ModelPackageSummaryList']\n",
        "    if not packages:\n",
        "        print(\"No model versions registered yet.\")\n",
        "    else:\n",
        "        print(f\"  {len(packages)} model version(s) found:\\n\")\n",
        "        for pkg in packages:\n",
        "            print(f\"  Version:   {pkg['ModelPackageVersion']}\")\n",
        "            print(f\"  Status:    {pkg['ModelApprovalStatus']}\")\n",
        "            print(f\"  Created:   {pkg['CreationTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "            print(f\"  ARN:       {pkg['ModelPackageArn']}\")\n",
        "            print()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Model Registry: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11d9a5e-95ae-47f9-a812-b4992019fd86",
      "metadata": {
        "id": "a11d9a5e-95ae-47f9-a812-b4992019fd86"
      },
      "outputs": [],
      "source": [
        "# ── Approve the latest registered model ───────────────────────────────────────\n",
        "# In production this would be a manual review step.\n",
        "# For the course demo we approve programmatically.\n",
        "\n",
        "try:\n",
        "    model_packages = sm_client.list_model_packages(\n",
        "        ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
        "        ModelApprovalStatus='PendingManualApproval',\n",
        "        SortBy='CreationTime',\n",
        "        SortOrder='Descending'\n",
        "    )\n",
        "\n",
        "    latest = model_packages['ModelPackageSummaryList']\n",
        "    if latest:\n",
        "        model_pkg_arn = latest[0]['ModelPackageArn']\n",
        "        sm_client.update_model_package(\n",
        "            ModelPackageArn=model_pkg_arn,\n",
        "            ModelApprovalStatus='Approved'\n",
        "        )\n",
        "        print(f\"✓ Model approved for deployment\")\n",
        "        print(f\"  ARN:     {model_pkg_arn}\")\n",
        "        print(f\"  Status:  Approved\")\n",
        "        approved_model_arn = model_pkg_arn\n",
        "        %store approved_model_arn\n",
        "    else:\n",
        "        print(\"No models pending approval.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not approve model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c72e841-d28d-480c-b286-62287e216613",
      "metadata": {
        "id": "6c72e841-d28d-480c-b286-62287e216613"
      },
      "source": [
        "### 9.8 Pipeline Execution History\n",
        "\n",
        "Print a summary of all executions — useful for confirming the success/failure pair in your video and design document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a463ae44-6979-4c94-b28a-b08462bbf753",
      "metadata": {
        "id": "a463ae44-6979-4c94-b28a-b08462bbf753"
      },
      "outputs": [],
      "source": [
        "# ── Full execution history for the pipeline ───────────────────────────────────\n",
        "print(\"=\" * 70)\n",
        "print(f\"EXECUTION HISTORY: {PIPELINE_NAME}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "history = sm_client.list_pipeline_executions(\n",
        "    PipelineName=PIPELINE_NAME,\n",
        "    SortBy='CreationTime',\n",
        "    SortOrder='Descending'\n",
        ")\n",
        "\n",
        "for exec_summary in history['PipelineExecutionSummaries']:\n",
        "    status = exec_summary['PipelineExecutionStatus']\n",
        "    icon = \"✅\" if status == 'Succeeded' else \"❌\" if status == 'Failed' else \"⏳\"\n",
        "    name = exec_summary.get('PipelineExecutionDisplayName', 'N/A')\n",
        "    created = exec_summary['StartTime'].strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"  {icon} [{created}] {name}: {status}\")\n",
        "\n",
        "print()\n",
        "print(\"View DAG visually in SageMaker Studio:\")\n",
        "print(f\"  Home → Pipelines → {PIPELINE_NAME} → [select execution] → Graph\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea01837-3c66-4b4f-bc76-1150e7983c20",
      "metadata": {
        "id": "1ea01837-3c66-4b4f-bc76-1150e7983c20"
      },
      "source": [
        "### 9.9 CI/CD Section Summary\n",
        "\n",
        "Print a complete reference summary of all CI/CD resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd05c7b-0806-4ab3-8f84-4081845b1493",
      "metadata": {
        "id": "4dd05c7b-0806-4ab3-8f84-4081845b1493"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CI/CD SECTION — COMPLETE RESOURCE SUMMARY\")\n",
        "print(\"VenueSignal | AAI-540 Group 6\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nS3 Resources (bucket: yelp-aai540-group6-{account_id}):\")\n",
        "print(f\"  Scripts:    s3://{BASE_BUCKET_NAME}/{CICD_SCRIPTS_PREFIX}\")\n",
        "print(f\"  Validation: s3://{BASE_BUCKET_NAME}/{CICD_EVAL_PREFIX}validation/\")\n",
        "print(f\"  Evaluation: s3://{BASE_BUCKET_NAME}/{CICD_EVAL_PREFIX}model-eval/\")\n",
        "print(f\"  Training:   s3://{BASE_BUCKET_NAME}/{MODEL_PREFIX}cicd-runs/\")\n",
        "\n",
        "print(\"\\nSageMaker Pipeline:\")\n",
        "print(f\"  Name:   {PIPELINE_NAME}\")\n",
        "print(f\"  Steps:  DataValidation → ModelTraining → ModelEvaluation → QualityGate\")\n",
        "print(f\"  Gate:   F1 >= F1Threshold parameter (default: {DEFAULT_F1_THRESHOLD})\")\n",
        "\n",
        "print(\"\\nModel Registry:\")\n",
        "print(f\"  Group:  {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "print(f\"  Only models passing the quality gate are registered\")\n",
        "\n",
        "print(\"\\nExecution History:\")\n",
        "try:\n",
        "    history = sm_client.list_pipeline_executions(\n",
        "        PipelineName=PIPELINE_NAME, SortBy='CreationTime', SortOrder='Descending'\n",
        "    )\n",
        "    for e in history['PipelineExecutionSummaries']:\n",
        "        icon = \"✅\" if e['PipelineExecutionStatus'] == 'Succeeded' else \"❌\"\n",
        "        name = e.get('PipelineExecutionDisplayName', 'N/A')\n",
        "        print(f\"  {icon} {name}: {e['PipelineExecutionStatus']}\")\n",
        "except Exception:\n",
        "    print(\"  (Run pipeline executions first)\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82461433-7e80-4fac-9498-a266c765626b",
      "metadata": {
        "id": "82461433-7e80-4fac-9498-a266c765626b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c79afa-6606-4d3e-bf5f-506e56340fee",
      "metadata": {
        "id": "e7c79afa-6606-4d3e-bf5f-506e56340fee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abdb5707-7196-461c-a2a8-d9fa3b634d63",
      "metadata": {
        "id": "abdb5707-7196-461c-a2a8-d9fa3b634d63"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81735014-d95f-49ef-8078-b7234a1cd261",
      "metadata": {
        "id": "81735014-d95f-49ef-8078-b7234a1cd261"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cefea782-c02b-42bc-8ee3-78c9db5e04ac",
      "metadata": {
        "id": "cefea782-c02b-42bc-8ee3-78c9db5e04ac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19a9f26-ac09-474a-905d-35bed69e3c3d",
      "metadata": {
        "id": "a19a9f26-ac09-474a-905d-35bed69e3c3d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "C1CfBiL364pP",
      "metadata": {
        "id": "C1CfBiL364pP"
      },
      "source": [
        "## References\n",
        "\n",
        "- Amazon Web Services. (n.d.). *Amazon SageMaker developer guide*. https://docs.aws.amazon.com/sagemaker/\n",
        "- Amazon Web Services. (n.d.). *AWS SDK for Python (Boto3) documentation*. https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
        "- Anthropic. (2024). *Claude* (Version 4.5 Sonnet) [Large language model]. https://www.anthropic.com/claude\n",
        "- Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. *Proceedings of the 22nd ACM SIGKDD* (pp. 785–794). https://doi.org/10.1145/2939672.2939785\n",
        "- Harris, C. R., et al. (2020). Array programming with NumPy. *Nature, 585*(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2\n",
        "- Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. *Computing in Science & Engineering, 9*(3), 90–95. https://doi.org/10.1109/MCSE.2007.55\n",
        "- Huyen, C. (2022). *Designing machine learning systems: An iterative process for production-ready applications*. O'Reilly Media.\n",
        "- McKinney, W. (2010). Data structures for statistical computing in Python. *Proceedings of the 9th Python in Science Conference* (pp. 51–56).\n",
        "- Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. *JMLR, 12*, 2825–2830.\n",
        "- Waskom, M. L. (2021). seaborn: Statistical data visualization. *JOSS, 6*(60), 3021. https://doi.org/10.21105/joss.03021\n",
        "- Yelp. (n.d.). *Yelp Open Dataset*. https://www.yelp.com/dataset\n",
        "\n",
        "This project utilised Claude (Anthropic) and ChatGPT (OpenAI) for code debugging, documentation assistance, and technical guidance.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}