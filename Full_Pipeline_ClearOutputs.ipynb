{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# VenueSignal - Yelp Business Rating Prediction\n",
    "### AAI-540 Group 6\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a complete end-to-end MLOps pipeline for predicting Yelp business ratings with a focus on parking availability constraints. The pipeline demonstrates MLOps best practices including:\n",
    "\n",
    "- **Data Lake Management**: S3-based data storage with proper versioning\n",
    "- **Data Cataloging**: Athena tables for queryable data access\n",
    "- **Feature Engineering**: Scalable feature store implementation\n",
    "- **Model Development**: Baseline and advanced models with proper evaluation\n",
    "- **Model Deployment**: SageMaker endpoints for inference\n",
    "- **Monitoring**: Comprehensive model, data, and infrastructure monitoring\n",
    "\n",
    "**Key Feature**: Uses AWS Account ID for bucket naming to enable each team member to run independently in their own AWS Learning Lab environment.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#section-1)\n",
    "2. [Data Lake Setup](#section-2)\n",
    "3. [Athena Tables & Data Cataloging](#section-3)\n",
    "4. [Exploratory Data Analysis](#section-4)\n",
    "5. [Feature Engineering & Feature Store](#section-5)\n",
    "6. [Model Training](#section-6)\n",
    "   - 6.1 Benchmark Models\n",
    "   - 6.2 XGBoost Model\n",
    "   - 6.3 Model Comparison\n",
    "7. [Model Deployment](#section-7)\n",
    "8. [Monitoring & Observability](#section-8)\n",
    "9. [CI/CD](#section-9)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Setup & Configuration <a id='section-1'></a>\n",
    "\n",
    "This section:\n",
    "- Verifies Python version\n",
    "- Imports all required libraries\n",
    "- Retrieves AWS Account ID for unique resource naming\n",
    "- Initializes AWS clients and SageMaker session\n",
    "- Configures S3 buckets using Account ID pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "python-version",
   "metadata": {
    "id": "python-version",
    "outputId": "f94c47a7-d417-4d15-c764-48e82e6cb9ba"
   },
   "outputs": [],
   "source": [
    "# Verify Python version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {
    "id": "imports-header"
   },
   "source": [
    "### 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "id": "imports",
    "outputId": "76ff6f2a-a3f6-42f6-afba-9d6ee4de407a"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import gdown\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS SDK\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "# SageMaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "sm_client = boto3.client('sagemaker')\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "sagemaker_session=session\n",
    "\n",
    "# Athena\n",
    "from pyathena import connect\n",
    "from pyathena.pandas.cursor import PandasCursor\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Monitoring\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig, DefaultModelMonitor, ModelQualityMonitor,\n",
    "    CronExpressionGenerator, EndpointInput\n",
    ")\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "import io, csv\n",
    "\n",
    "# Google Drive download\n",
    "import gdown\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "account-id-header",
   "metadata": {
    "id": "account-id-header"
   },
   "source": [
    "### 1.2 Retrieve AWS Account ID\n",
    "\n",
    "**IMPORTANT**: This retrieves your unique AWS Account ID which will be used to create unique S3 bucket names.\n",
    "This allows each team member to run this notebook independently in their own AWS Learning Lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "account-id",
   "metadata": {
    "id": "account-id",
    "outputId": "9d563f27-71c0-4be6-99e7-6117346b26f7"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get AWS Account ID\n",
    "    account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "    print(f\"Successfully retrieved AWS Account ID: {account_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cannot retrieve account information: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aws-config-header",
   "metadata": {
    "id": "aws-config-header"
   },
   "source": [
    "### 1.3 Initialize AWS Clients and SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aws-config",
   "metadata": {
    "id": "aws-config",
    "outputId": "0b9b6401-ac6f-4624-f0ba-32f60216c5be"
   },
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get Execution role and AWS Region\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn:\", role)\n",
    "REGION = sagemaker_session.boto_region_name\n",
    "print(\"Region:\", REGION)\n",
    "\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = boto3.client(\"s3\", region_name=REGION)\n",
    "s3_resource = boto3.resource(\"s3\", region_name=REGION)\n",
    "athena_client = boto3.client(\"athena\", region_name=REGION)\n",
    "sagemaker_client = boto3.client(\"sagemaker\", region_name=REGION)\n",
    "cloudwatch_client = boto3.client(\"cloudwatch\", region_name=REGION)\n",
    "logs_client = boto3.client(\"logs\", region_name=REGION)\n",
    "\n",
    "print(f\"AWS Region: {REGION}\")\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "print(f\"AWS clients initialized successfully\")\n",
    "\n",
    "# Also create cw_client alias for consistency\n",
    "cw_client = cloudwatch_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d4527-af5d-4a05-af38-13da4eccedcb",
   "metadata": {
    "id": "2b9d4527-af5d-4a05-af38-13da4eccedcb"
   },
   "outputs": [],
   "source": [
    "project_name = \"yelp-aai540-group6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-config-header",
   "metadata": {
    "id": "s3-config-header"
   },
   "source": [
    "### 1.4 Configure S3 Buckets with Account ID Pattern\n",
    "\n",
    "**IMPORTANT**: All S3 buckets are created with your Account ID to ensure uniqueness.\n",
    "This pattern is used throughout the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-config",
   "metadata": {
    "id": "s3-config",
    "outputId": "fb506ddb-6854-48cd-9ce3-f89eb8f14173"
   },
   "outputs": [],
   "source": [
    "# Base bucket name with Account ID\n",
    "BASE_BUCKET_NAME = f\"yelp-aai540-group6-{account_id}\"\n",
    "\n",
    "# S3 Prefixes (paths within buckets)\n",
    "RAW_DATA_PREFIX = \"yelp-dataset/json/\"\n",
    "PARQUET_PREFIX = \"yelp-dataset/parquet/\"\n",
    "ATHENA_RESULTS_PREFIX = \"athena/results/\"\n",
    "FEATURE_PREFIX = \"feature-store/\"\n",
    "MODEL_PREFIX = \"models/\"\n",
    "MONITORING_PREFIX = \"monitoring/\"\n",
    "\n",
    "# Individual directories within the base bucket\n",
    "DATA_JSON_DIR = f\"{BASE_BUCKET_NAME}/{RAW_DATA_PREFIX}\"  # Raw data storage\n",
    "DATA_PARQUET_DIR = f\"{BASE_BUCKET_NAME}/{PARQUET_PREFIX}\"  # Raw data storage\n",
    "ATHENA_DIR = f\"{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\"  # Athena queries and results\n",
    "FEATURE_DIR = f\"{BASE_BUCKET_NAME}/{FEATURE_PREFIX}\"  # Feature store offline\n",
    "MODEL_DIR = f\"{BASE_BUCKET_NAME}/{MODEL_PREFIX}\"  # Model artifacts\n",
    "MONITORING_DIR = f\"{BASE_BUCKET_NAME}/{MONITORING_PREFIX}\"  # Monitoring data\n",
    "\n",
    "# Full S3 paths\n",
    "ATHENA_RESULTS_S3 = f\"s3://{ATHENA_DIR}\"\n",
    "\n",
    "# Athena Database\n",
    "ATHENA_DB = \"yelp\"\n",
    "\n",
    "# Store configuration\n",
    "%store REGION\n",
    "%store role\n",
    "%store account_id\n",
    "%store BASE_BUCKET_NAME\n",
    "%store DATA_JSON_DIR\n",
    "%store DATA_PARQUET_DIR\n",
    "%store ATHENA_DIR\n",
    "%store FEATURE_DIR\n",
    "%store MODEL_DIR\n",
    "%store MONITORING_DIR\n",
    "%store ATHENA_RESULTS_S3\n",
    "%store ATHENA_DB\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*80)\n",
    "print(\"S3 BUCKET CONFIGURATION (Account-Specific)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"AWS Account ID:     {account_id}\")\n",
    "print(f\"AWS Region:         {REGION}\")\n",
    "print(f\"AWS Role:         {role}\")\n",
    "print()\n",
    "print(\"S3 Bucket:\")\n",
    "print(f\"  Base Bucket:      {BASE_BUCKET_NAME}\")\n",
    "print(\"S3 Bucket Directories:\")\n",
    "print(f\"  JSON:       {DATA_JSON_DIR}\")\n",
    "print(f\"  Parquet:    {DATA_PARQUET_DIR}\")\n",
    "print(f\"  Athena:     {ATHENA_DIR}\")\n",
    "print(f\"  Feature:    {FEATURE_DIR}\")\n",
    "print(f\"  Model:      {MODEL_DIR}\")\n",
    "print(f\"  Monitoring: {MONITORING_DIR}\")\n",
    "print()\n",
    "print(\"Athena Configuration:\")\n",
    "print(f\"  Database:         {ATHENA_DB}\")\n",
    "print(f\"  Results Location: {ATHENA_RESULTS_S3}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-buckets-header",
   "metadata": {
    "id": "create-buckets-header"
   },
   "source": [
    "### 1.5 Create S3 Buckets\n",
    "\n",
    "This creates all required S3 buckets for the pipeline. Each bucket is unique to your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-buckets",
   "metadata": {
    "id": "create-buckets",
    "outputId": "2ddb1813-3c7b-42e2-fa08-66e40392efe9"
   },
   "outputs": [],
   "source": [
    "def create_bucket_if_not_exists(bucket_name, region=REGION):\n",
    "    \"\"\"\n",
    "    Create an S3 bucket if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to create\n",
    "        region: AWS region for the bucket\n",
    "\n",
    "    Returns:\n",
    "        True if bucket was created or already exists, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if bucket exists\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"  Bucket already exists: {bucket_name}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            # Bucket doesn't exist, create it\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    s3_client.create_bucket(Bucket=bucket_name)\n",
    "                else:\n",
    "                    s3_client.create_bucket(\n",
    "                        Bucket=bucket_name,\n",
    "                        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                    )\n",
    "                print(f\"  Created bucket: {bucket_name}\")\n",
    "                return True\n",
    "            except ClientError as create_error:\n",
    "                print(f\"  Error creating bucket {bucket_name}: {create_error}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"  Error checking bucket {bucket_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Create all required buckets\n",
    "print(\"Creating S3 bucket...\")\n",
    "\n",
    "success = True\n",
    "if not create_bucket_if_not_exists(BASE_BUCKET_NAME):\n",
    "    success = False\n",
    "\n",
    "if success:\n",
    "    print(\"\\n S3 bucket is ready!\")\n",
    "else:\n",
    "    print(\"\\n Bucket could not be created. Please check errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Data Lake Setup <a id='section-2'></a>\n",
    "\n",
    "This section:\n",
    "- Downloads Yelp academic dataset from Google Drive\n",
    "- Uploads raw JSON files to S3 data lake\n",
    "- Organizes data in a structured format\n",
    "\n",
    "**Data Source**: Yelp Academic Dataset (5 files, ~8.5 GB total)\n",
    "- Business data (150k+ businesses)\n",
    "- Review data (7M+ reviews)\n",
    "- User data (2M+ users)\n",
    "- Check-in data\n",
    "- Tip data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gdrive-files-header",
   "metadata": {
    "id": "gdrive-files-header"
   },
   "source": [
    "### 2.1 Define Google Drive File IDs\n",
    "\n",
    "These are the file IDs for the Yelp dataset files stored in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gdrive-files",
   "metadata": {
    "id": "gdrive-files",
    "outputId": "a36fb6fd-5bb5-49f3-84ec-7efb37d319fc"
   },
   "outputs": [],
   "source": [
    "# Google Drive file IDs for Yelp dataset\n",
    "google_drive_file_ids = {\n",
    "    \"yelp_academic_dataset_business.json\": \"1-VQQyXape4lCTa_5bA9VTlJgqKkqqR3h\",\n",
    "    \"yelp_academic_dataset_checkin.json\": \"1LcnPYD4m3jp4l7EF8s8mqp3F8QUlcr9-\",\n",
    "    \"yelp_academic_dataset_review.json\": \"1M8QVg2aiAwSSQO3zRJYj35PLnMKBa5L9\",\n",
    "    \"yelp_academic_dataset_tip.json\": \"1vyYognzSAMenVakNyXgchwfZlVc76ZMk\",\n",
    "    \"yelp_academic_dataset_user.json\": \"1yLL_31R4J1Me_CEyZCYSsJrcQkzZtxKf\"\n",
    "}\n",
    "#https://drive.google.com/file/d/1M8QVg2aiAwSSQO3zRJYj35PLnMKBa5L9/view?usp=drive_link-copy\n",
    "#https://drive.google.com/file/d/1kz33s_tiLydRDFRf4GMxxBIvrW_lEpTC/view?usp=drive_link-copy\n",
    "#https://drive.google.com/file/d/1-VQQyXape4lCTa_5bA9VTlJgqKkqqR3h/view?usp=drive_link\n",
    "#https://drive.google.com/file/d/1LcnPYD4m3jp4l7EF8s8mqp3F8QUlcr9-/view?usp=drive_link\n",
    "#https://drive.google.com/file/d/1eQ8nSwENhtwu7X1aNj8XgmHy5KwcMfEU/view?usp=drive_link\n",
    "#https://drive.google.com/file/d/1vyYognzSAMenVakNyXgchwfZlVc76ZMk/view?usp=drive_link\n",
    "#https://drive.google.com/file/d/1yLL_31R4J1Me_CEyZCYSsJrcQkzZtxKf/view?usp=drive_link\n",
    "\n",
    "\n",
    "print(f\"Files to download: {len(google_drive_file_ids)}\")\n",
    "for filename in google_drive_file_ids.keys():\n",
    "    print(f\"  - {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download-upload-header",
   "metadata": {
    "id": "download-upload-header"
   },
   "source": [
    "### 2.2 Download and Upload to S3\n",
    "\n",
    "**Process**:\n",
    "1. Download each file from Google Drive\n",
    "2. Upload to your account-specific S3 data bucket\n",
    "3. Clean up local files to save disk space\n",
    "\n",
    "**Warning**: This will download ~8.5 GB of data. Ensure you have sufficient disk space and network bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-upload",
   "metadata": {
    "id": "download-upload",
    "outputId": "3f133646-0784-4701-96a5-96df59c8b2e3"
   },
   "outputs": [],
   "source": [
    "file_to_dir = {\n",
    "    \"yelp_academic_dataset_business.json\": \"business/\",\n",
    "    \"yelp_academic_dataset_checkin.json\": \"checkin/\",\n",
    "    \"yelp_academic_dataset_review.json\": \"review/\",\n",
    "    \"yelp_academic_dataset_tip.json\": \"tip/\",\n",
    "    \"yelp_academic_dataset_user.json\": \"user/\",\n",
    "}\n",
    "\n",
    "# Change to working directory\n",
    "work_dir = \"/home/sagemaker-user/VenueSignal\"\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.chdir(work_dir)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Target S3 bucket: {BASE_BUCKET_NAME}\")\n",
    "print(f\"Target S3 prefix: {RAW_DATA_PREFIX}\")\n",
    "print()\n",
    "\n",
    "\n",
    "def process_one_file(filename, file_id, s3_client, RAW_DATA_PREFIX, BASE_BUCKET_NAME):\n",
    "    # Step 1: Download from Google Drive\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    gdown.download(download_url, filename, quiet=True)\n",
    "\n",
    "    # Step 2: Upload to S3\n",
    "    file_dir = file_to_dir[filename]\n",
    "    s3_key = f\"{RAW_DATA_PREFIX}{file_dir}{filename}\"\n",
    "    s3_client.upload_file(filename, BASE_BUCKET_NAME, s3_key)\n",
    "\n",
    "    # Step 3: Clean up local file\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "    return filename, s3_key\n",
    "\n",
    "\n",
    "def download_and_upload_all_concurrently(\n",
    "    google_drive_file_ids: dict,\n",
    "    s3_client,\n",
    "    RAW_DATA_PREFIX: str,\n",
    "    BASE_BUCKET_NAME: str,\n",
    "    max_workers: int = 5,\n",
    "):\n",
    "    results = {\"ok\": [], \"failed\": []}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                process_one_file,\n",
    "                filename,\n",
    "                file_id,\n",
    "                s3_client,\n",
    "                RAW_DATA_PREFIX,\n",
    "                BASE_BUCKET_NAME,\n",
    "            ): filename\n",
    "            for filename, file_id in google_drive_file_ids.items()\n",
    "        }\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            filename = futures[fut]\n",
    "            try:\n",
    "                fname, s3_key = fut.result()\n",
    "                results[\"ok\"].append((fname, s3_key))\n",
    "                print(f\"✓ {fname} -> s3://{BASE_BUCKET_NAME}/{s3_key}\")\n",
    "            except Exception as e:\n",
    "                results[\"failed\"].append((filename, str(e)))\n",
    "                print(f\"✗ {filename} failed: {e}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    print(f\"Successful: {len(results['ok'])}\")\n",
    "    print(f\"Failed:     {len(results['failed'])}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = download_and_upload_all_concurrently(\n",
    "    google_drive_file_ids=google_drive_file_ids,\n",
    "    s3_client=s3_client,\n",
    "    RAW_DATA_PREFIX=RAW_DATA_PREFIX,\n",
    "    BASE_BUCKET_NAME=BASE_BUCKET_NAME,\n",
    "    max_workers=5,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" All files processed successfully!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-upload-header",
   "metadata": {
    "id": "verify-upload-header"
   },
   "source": [
    "### 2.3 Verify Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-upload",
   "metadata": {
    "id": "verify-upload",
    "outputId": "597bd475-d1f1-4eee-8568-8f7d3d61818e"
   },
   "outputs": [],
   "source": [
    "# List files in S3\n",
    "s3_path = f\"s3://{DATA_JSON_DIR}\"\n",
    "print(f\"Files in {s3_path}:\\n\")\n",
    "!aws s3 ls {s3_path} --recursive --human-readable\n",
    "\n",
    "# Create clickable link to S3 console\n",
    "from IPython.display import display, HTML\n",
    "s3_console_url = f\"https://s3.console.aws.amazon.com/s3/buckets/{DATA_JSON_DIR}?region={REGION}&tab=overview\"\n",
    "display(HTML(f'<b>View in S3 Console: <a target=\"_blank\" href=\"{s3_console_url}\">S3 Bucket - Yelp Dataset</a></b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Athena Tables & Data Cataloging <a id='section-3'></a>\n",
    "\n",
    "This section:\n",
    "- Creates Athena database\n",
    "- Defines table schemas for JSON data\n",
    "- Creates queryable tables\n",
    "- Converts JSON to Parquet for better performance\n",
    "\n",
    "**Benefits of Athena**:\n",
    "- Query data in S3 using SQL\n",
    "- No data movement required\n",
    "- Pay only for queries run\n",
    "- Integrates with SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athena-db-header",
   "metadata": {
    "id": "athena-db-header"
   },
   "source": [
    "### 3.1 Create Athena Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-athena-db",
   "metadata": {
    "id": "create-athena-db",
    "outputId": "e5154c20-9bae-40a9-a697-916e11bf97f7"
   },
   "outputs": [],
   "source": [
    "def execute_athena_query(query, database=None, output_location=ATHENA_RESULTS_S3):\n",
    "    \"\"\"\n",
    "    Execute an Athena query and wait for completion.\n",
    "\n",
    "    Args:\n",
    "        query: SQL query to execute\n",
    "        database: Athena database name (optional)\n",
    "        output_location: S3 location for query results\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"QueryString\": query,\n",
    "        \"ResultConfiguration\": {\"OutputLocation\": output_location},\n",
    "    }\n",
    "    if database:\n",
    "        params[\"QueryExecutionContext\"] = {\"Database\": database}\n",
    "\n",
    "    # Start query\n",
    "    qx = athena_client.start_query_execution(**params)\n",
    "    qid = qx[\"QueryExecutionId\"]\n",
    "\n",
    "    # Wait for completion\n",
    "    while True:\n",
    "        resp = athena_client.get_query_execution(QueryExecutionId=qid)\n",
    "        state = resp[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if state in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    if state != \"SUCCEEDED\":\n",
    "        reason = resp[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"\")\n",
    "        raise RuntimeError(f\"Athena query {state}: {reason}\\n\\nQuery:\\n{query}\")\n",
    "\n",
    "    # Fetch results\n",
    "    paginator = athena_client.get_paginator(\"get_query_results\")\n",
    "    columns = None\n",
    "    data_rows = []\n",
    "\n",
    "    for page in paginator.paginate(QueryExecutionId=qid):\n",
    "        result_set = page.get(\"ResultSet\", {})\n",
    "        metadata = result_set.get(\"ResultSetMetadata\", {})\n",
    "        col_info = metadata.get(\"ColumnInfo\", [])\n",
    "\n",
    "        # Resolve headers from metadata once\n",
    "        if columns is None and col_info:\n",
    "            columns = [c.get(\"Name\") for c in col_info]\n",
    "\n",
    "        for row in result_set.get(\"Rows\", []):\n",
    "            values = [c.get(\"VarCharValue\") for c in row.get(\"Data\", [])]\n",
    "\n",
    "            # If Athena included a header-like first row equal to column names, skip it.\n",
    "            if columns and values == columns:\n",
    "                continue\n",
    "\n",
    "            # Normalize row length to match columns\n",
    "            if columns:\n",
    "                if len(values) < len(columns):\n",
    "                    values += [None] * (len(columns) - len(values))\n",
    "                elif len(values) > len(columns):\n",
    "                    values = values[:len(columns)]\n",
    "\n",
    "            data_rows.append(values)\n",
    "\n",
    "    # No result rows (common for DDL/DML)\n",
    "    if not data_rows and not columns:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # If we have columns but no data, still return empty df with correct headers\n",
    "    if columns and not data_rows:\n",
    "        return pd.DataFrame(columns=columns)\n",
    "\n",
    "    # If columns are missing for some reason, generate generic names\n",
    "    if not columns:\n",
    "        max_len = max((len(r) for r in data_rows), default=0)\n",
    "        columns = [f\"col_{i}\" for i in range(max_len)]\n",
    "        data_rows = [r + [None] * (max_len - len(r)) for r in data_rows]\n",
    "\n",
    "    return pd.DataFrame(data_rows, columns=columns)\n",
    "\n",
    "\n",
    "# Create Athena database\n",
    "print(f\"Creating Athena database: {ATHENA_DB}\")\n",
    "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {ATHENA_DB}\"\n",
    "try:\n",
    "    execute_athena_query(create_db_query)\n",
    "    print(f\" Database '{ATHENA_DB}' created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\" Error creating database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athena-tables-header",
   "metadata": {
    "id": "athena-tables-header"
   },
   "source": [
    "### 3.2 Define File Locations\n",
    "\n",
    "Map table names to their S3 file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-files",
   "metadata": {
    "id": "define-files",
    "outputId": "af79ea38-45dc-44e8-91ea-390bc34c2178"
   },
   "outputs": [],
   "source": [
    "# Define JSON files\n",
    "FILES = {\n",
    "    'business': 'yelp_academic_dataset_business.json',\n",
    "    'review': 'yelp_academic_dataset_review.json',\n",
    "    'user': 'yelp_academic_dataset_user.json',\n",
    "    'checkin': 'yelp_academic_dataset_checkin.json',\n",
    "    'tip': 'yelp_academic_dataset_tip.json'\n",
    "}\n",
    "\n",
    "# Create S3 object keys\n",
    "OBJECT_KEYS = {\n",
    "    table: f\"{RAW_DATA_PREFIX}{table}/{fname}\" for table, fname in FILES.items()\n",
    "}\n",
    "\n",
    "print(\"File mappings:\")\n",
    "for table, key in OBJECT_KEYS.items():\n",
    "    print(f\"  {table:10} -> s3://{BASE_BUCKET_NAME}/{key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-files-header",
   "metadata": {
    "id": "verify-files-header"
   },
   "source": [
    "### 3.3 Verify File Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-files",
   "metadata": {
    "id": "verify-files",
    "outputId": "a49c5c60-73ab-4b05-d794-98c6c8e72c97"
   },
   "outputs": [],
   "source": [
    "dest_locations = {}\n",
    "\n",
    "print(\"Verifying S3 file access...\\n\")\n",
    "for table, key in OBJECT_KEYS.items():\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=BASE_BUCKET_NAME, Key=key)\n",
    "        print(f\" {table:10} {key}\")\n",
    "        dest_locations[table] = f\"s3://{DATA_JSON_DIR}{table}/\"\n",
    "    except ClientError:\n",
    "        print(f\" {table:10} {key} NOT FOUND\")\n",
    "\n",
    "print()\n",
    "print(\"JSON file directory destinations...\\n\")\n",
    "for t, loc in dest_locations.items():\n",
    "    print(f\"{t:8} -> {loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-tables-header",
   "metadata": {
    "id": "create-tables-header"
   },
   "source": [
    "### 3.4 Create Athena Tables from JSON\n",
    "\n",
    "Create external tables in Athena that point to the JSON files in S3.\n",
    "\n",
    "If you experience any errors while running the table creation cells, uncomment the bellow cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f5c51-702f-41bc-8ddc-a1769a33e7ae",
   "metadata": {
    "id": "b21f5c51-702f-41bc-8ddc-a1769a33e7ae"
   },
   "outputs": [],
   "source": [
    "# TABLES = [\"business\", \"review\", \"user\", \"checkin\", \"tip\", \"business_attributes\"]\n",
    "\n",
    "# for table in TABLES:\n",
    "#     print(f\"Dropping table: {ATHENA_DB}.{table}\")\n",
    "#     execute_athena_query(\n",
    "#         f\"DROP TABLE IF EXISTS {ATHENA_DB}.{table};\",\n",
    "#         database=ATHENA_DB\n",
    "#     )\n",
    "\n",
    "# paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "# to_delete = []\n",
    "# for page in paginator.paginate(Bucket=BASE_BUCKET_NAME, Prefix=ATHENA_RESULTS_PREFIX):\n",
    "#     for obj in page.get(\"Contents\", []):\n",
    "#         to_delete.append({\"Key\": obj[\"Key\"]})\n",
    "\n",
    "# if not to_delete:\n",
    "#     print(\"✅ Nothing to delete under\", f\"s3://{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\")\n",
    "# else:\n",
    "#     # delete in batches of 1000 (S3 limit)\n",
    "#     for i in range(0, len(to_delete), 1000):\n",
    "#         s3_client.delete_objects(\n",
    "#             Bucket=BASE_BUCKET_NAME,\n",
    "#             Delete={\"Objects\": to_delete[i:i+1000]}\n",
    "#         )\n",
    "#     print(f\"✅ Deleted {len(to_delete)} objects under s3://{BASE_BUCKET_NAME}/{ATHENA_RESULTS_PREFIX}\")\n",
    "\n",
    "# print(\"✅ All tables dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a0f02-658b-45a2-ac63-c08d027bf3ae",
   "metadata": {
    "id": "336a0f02-658b-45a2-ac63-c08d027bf3ae",
    "outputId": "6f369397-cf00-4dd2-ecb4-ea26b3553157"
   },
   "outputs": [],
   "source": [
    "business_location = dest_locations[\"business\"]\n",
    "\n",
    "# parquet_prefix\n",
    "business_parquet_location = f\"s3://{DATA_PARQUET_DIR}business\"\n",
    "\n",
    "print(\"Creating temporary table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.business_temp (\n",
    "  business_id string,\n",
    "  name string,\n",
    "  address string,\n",
    "  city string,\n",
    "  state string,\n",
    "  postal_code string,\n",
    "  latitude double,\n",
    "  longitude double,\n",
    "  stars double,\n",
    "  review_count int,\n",
    "  is_open int,\n",
    "  attributes map<string,string>,\n",
    "  categories string,\n",
    "  hours map<string,string>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
    "LOCATION '{business_location}'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Creating table with parquets\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE TABLE {ATHENA_DB}.business\n",
    "WITH (\n",
    "  format = 'PARQUET',\n",
    "  external_location = '{business_parquet_location}'\n",
    ") AS\n",
    "SELECT\n",
    "  business_id,\n",
    "  name,\n",
    "  address,\n",
    "  city,\n",
    "  state,\n",
    "  postal_code,\n",
    "  latitude,\n",
    "  longitude,\n",
    "  stars,\n",
    "  review_count,\n",
    "  is_open,\n",
    "  attributes,\n",
    "  categories,\n",
    "  hours\n",
    "\n",
    "FROM {ATHENA_DB}.business_temp;\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"drop temp table\")\n",
    "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.business_temp;\", database=ATHENA_DB)\n",
    "\n",
    "print(f\"✅ Created table {ATHENA_DB}.business\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a840eaa-1267-4a79-8ca9-d9999a9223e0",
   "metadata": {
    "id": "0a840eaa-1267-4a79-8ca9-d9999a9223e0",
    "outputId": "494f9bdd-75db-4261-c134-93d920ddb511"
   },
   "outputs": [],
   "source": [
    "review_location = dest_locations[\"review\"]\n",
    "# parquet_prefix\n",
    "review_parquet_location = f\"s3://{DATA_PARQUET_DIR}review\"\n",
    "\n",
    "print(\"Creating temporary table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.review_temp (\n",
    "  review_id string,\n",
    "  user_id string,\n",
    "  business_id string,\n",
    "  stars double,\n",
    "  useful int,\n",
    "  funny int,\n",
    "  cool int,\n",
    "  text string,\n",
    "  date string\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
    "LOCATION '{review_location}'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Creating table with parquets\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE TABLE {ATHENA_DB}.review\n",
    "WITH (\n",
    "  format = 'PARQUET',\n",
    "  external_location = '{review_parquet_location}',\n",
    "  partitioned_by = ARRAY['year']\n",
    ") AS\n",
    "SELECT\n",
    "  review_id,\n",
    "  user_id,\n",
    "  business_id,\n",
    "  stars,\n",
    "  useful,\n",
    "  funny,\n",
    "  cool,\n",
    "  text,\n",
    "  date,\n",
    "  CAST(substr(date, 1, 4) AS integer) AS year\n",
    "\n",
    "FROM {ATHENA_DB}.review_temp\n",
    "WHERE date IS NOT NULL;\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"drop temp table\")\n",
    "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.review_temp;\", database=ATHENA_DB)\n",
    "\n",
    "print(f\"✅ Created table {ATHENA_DB}.review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c95502-abd9-4f55-abde-6f976136be83",
   "metadata": {
    "id": "05c95502-abd9-4f55-abde-6f976136be83",
    "outputId": "44b1b4bf-5c53-4081-91a1-6c44db1c4477"
   },
   "outputs": [],
   "source": [
    "user_location = dest_locations[\"user\"]\n",
    "\n",
    "# parquet_prefix\n",
    "user_parquet_location = f\"s3://{DATA_PARQUET_DIR}user\"\n",
    "\n",
    "print(\"Create temp table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.user_temp (\n",
    "  user_id string,\n",
    "  name string,\n",
    "  review_count int,\n",
    "  yelping_since string,\n",
    "  friends array<string>,\n",
    "  useful int,\n",
    "  funny int,\n",
    "  cool int,\n",
    "  fans int,\n",
    "  elite array<string>,\n",
    "  average_stars double,\n",
    "  compliment_hot int,\n",
    "  compliment_more int,\n",
    "  compliment_profile int,\n",
    "  compliment_cute int,\n",
    "  compliment_list int,\n",
    "  compliment_note int,\n",
    "  compliment_plain int,\n",
    "  compliment_cool int,\n",
    "  compliment_funny int,\n",
    "  compliment_writer int,\n",
    "  compliment_photos int\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
    "LOCATION '{user_location}'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Create Parquet table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE TABLE {ATHENA_DB}.user\n",
    "WITH (\n",
    "  format = 'PARQUET',\n",
    "  external_location = '{user_parquet_location}'\n",
    ") AS\n",
    "SELECT\n",
    "  user_id,\n",
    "  name,\n",
    "  review_count,\n",
    "  yelping_since,\n",
    "  friends,\n",
    "  useful,\n",
    "  funny,\n",
    "  cool,\n",
    "  fans,\n",
    "  elite,\n",
    "  average_stars,\n",
    "  compliment_hot,\n",
    "  compliment_more,\n",
    "  compliment_profile,\n",
    "  compliment_cute,\n",
    "  compliment_list,\n",
    "  compliment_note,\n",
    "  compliment_plain,\n",
    "  compliment_cool,\n",
    "  compliment_funny,\n",
    "  compliment_writer,\n",
    "  compliment_photos\n",
    "FROM {ATHENA_DB}.user_temp;\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Drop temp table\")\n",
    "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.user_temp;\", database=ATHENA_DB)\n",
    "\n",
    "print(f\"✅ Created table {ATHENA_DB}.user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2990ba-ce4f-48af-9780-3fc93985a97f",
   "metadata": {
    "id": "9f2990ba-ce4f-48af-9780-3fc93985a97f",
    "outputId": "8e849eb1-b215-43f0-9047-3140c1252a21"
   },
   "outputs": [],
   "source": [
    "checkin_location = dest_locations[\"checkin\"]\n",
    "\n",
    "# parquet_prefix\n",
    "checkin_parquet_location = f\"s3://{DATA_PARQUET_DIR}checkin\"\n",
    "\n",
    "print(\"Create temp table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.checkin_temp (\n",
    "  business_id string,\n",
    "  date string\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
    "LOCATION '{checkin_location}'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Create Parquet table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE TABLE {ATHENA_DB}.checkin\n",
    "WITH (\n",
    "  format = 'PARQUET',\n",
    "  external_location = '{checkin_parquet_location}'\n",
    ") AS\n",
    "SELECT\n",
    "  business_id,\n",
    "  date\n",
    "FROM {ATHENA_DB}.checkin_temp;\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Drop temp table\")\n",
    "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.checkin_temp;\", database=ATHENA_DB)\n",
    "\n",
    "\n",
    "print(f\"✅ Created table {ATHENA_DB}.checkin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be29d713-419d-466c-a35c-ca8ada23708f",
   "metadata": {
    "id": "be29d713-419d-466c-a35c-ca8ada23708f",
    "outputId": "75b17daf-57c6-4d44-9fdb-ad0f6e0a4c00"
   },
   "outputs": [],
   "source": [
    "tip_location = dest_locations[\"tip\"]\n",
    "\n",
    "# parquet_prefix\n",
    "tip_parquet_location = f\"s3://{DATA_PARQUET_DIR}tip\"\n",
    "\n",
    "print(\"Create temp table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {ATHENA_DB}.tip_temp (\n",
    "  user_id string,\n",
    "  business_id string,\n",
    "  text string,\n",
    "  date string,\n",
    "  compliment_count int\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n",
    "WITH SERDEPROPERTIES ('ignore.malformed.json'='true')\n",
    "LOCATION '{tip_location}'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Create Parquet table\")\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE TABLE {ATHENA_DB}.tip\n",
    "WITH (\n",
    "  format = 'PARQUET',\n",
    "  external_location = '{tip_parquet_location}',\n",
    "  partitioned_by = ARRAY['year']\n",
    ") AS\n",
    "SELECT\n",
    "  user_id,\n",
    "  business_id,\n",
    "  text,\n",
    "  date,\n",
    "  compliment_count,\n",
    "  CAST(substr(date, 1, 4) AS integer) AS year\n",
    "\n",
    "FROM {ATHENA_DB}.tip_temp\n",
    "WHERE date IS NOT NULL;\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(\"Drop temp table\")\n",
    "execute_athena_query(f\"DROP TABLE IF EXISTS {ATHENA_DB}.tip_temp;\", database=ATHENA_DB)\n",
    "\n",
    "print(f\"✅ Created table {ATHENA_DB}.tip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1ec58-5a9b-42c6-a5fc-0cc2f3b4fc3c",
   "metadata": {
    "id": "32f1ec58-5a9b-42c6-a5fc-0cc2f3b4fc3c",
    "outputId": "01808052-52c8-4e9a-f2cd-4941ca79785b"
   },
   "outputs": [],
   "source": [
    "business_attributes_location = f\"s3://{DATA_PARQUET_DIR}business_attributes\"\n",
    "\n",
    "execute_athena_query(f\"\"\"\n",
    "CREATE TABLE {ATHENA_DB}.business_attributes\n",
    "WITH (\n",
    "  format = 'PARQUET',\n",
    "  external_location = '{business_attributes_location}'\n",
    ") AS\n",
    "WITH normalized AS (\n",
    "  SELECT\n",
    "    business_id,\n",
    "    hours,\n",
    "\n",
    "    -- Normalize u'...' and '...' wrappers on keys + values\n",
    "    map_from_entries(\n",
    "      transform(\n",
    "        map_entries(attributes),\n",
    "        e -> CAST(\n",
    "          ROW(\n",
    "            regexp_replace(CAST(e[1] AS varchar), '^u?''(.*)''$', '$1'),\n",
    "            regexp_replace(CAST(e[2] AS varchar), '^u?''(.*)''$', '$1')\n",
    "          ) AS ROW(varchar, varchar)\n",
    "        )\n",
    "      )\n",
    "    ) AS attrs\n",
    "  FROM {ATHENA_DB}.business\n",
    "  WHERE attributes IS NOT NULL\n",
    "),\n",
    "parsed AS (\n",
    "  SELECT\n",
    "    business_id,\n",
    "    hours,\n",
    "    -- Convert literal \"None\" (any case) to NULL for all attribute lookups via helper expression pattern below\n",
    "    attrs,\n",
    "\n",
    "    -- Parse BusinessParking\n",
    "    TRY(\n",
    "      CAST(\n",
    "        json_parse(\n",
    "          replace(\n",
    "            replace(\n",
    "              replace(\n",
    "                replace(\n",
    "                  regexp_replace(attrs['businessparking'], 'u''(.*?)''', '\"$1\"'),\n",
    "                  '''', '\"'\n",
    "                ),\n",
    "                'False', 'false'\n",
    "              ),\n",
    "              'True', 'true'\n",
    "            ),\n",
    "            'None', 'null'\n",
    "          )\n",
    "        ) AS map(varchar, boolean)\n",
    "      )\n",
    "    ) AS parking_map,\n",
    "\n",
    "    -- Parse Ambience\n",
    "    TRY(\n",
    "      CAST(\n",
    "        json_parse(\n",
    "          replace(\n",
    "            replace(\n",
    "              replace(\n",
    "                replace(\n",
    "                  -- normalize u'...' keys inside the string\n",
    "                  regexp_replace(attrs['ambience'], 'u''(.*?)''', '\"$1\"'),\n",
    "                  '''', '\"'\n",
    "                ),\n",
    "                'False', 'false'\n",
    "              ),\n",
    "              'True', 'true'\n",
    "            ),\n",
    "            'None', 'null'\n",
    "          )\n",
    "        ) AS map(varchar, boolean)\n",
    "      )\n",
    "    ) AS ambience_map,\n",
    "\n",
    "    -- Parse GoodForMeal\n",
    "    TRY(\n",
    "      CAST(\n",
    "        json_parse(\n",
    "          replace(\n",
    "            replace(\n",
    "              replace(\n",
    "                replace(\n",
    "                  -- normalize u'...' keys\n",
    "                  regexp_replace(attrs['goodformeal'], 'u''(.*?)''', '\"$1\"'),\n",
    "                  '''', '\"'\n",
    "                ),\n",
    "                'False', 'false'\n",
    "              ),\n",
    "              'True', 'true'\n",
    "            ),\n",
    "            'None', 'null'\n",
    "          )\n",
    "        ) AS map(varchar, boolean)\n",
    "      )\n",
    "    ) AS goodformeal_map,\n",
    "\n",
    "    -- Parse BestNights\n",
    "    TRY(\n",
    "      CAST(\n",
    "        json_parse(\n",
    "          replace(\n",
    "            replace(\n",
    "              replace(\n",
    "                replace(\n",
    "                  -- normalize u'...' keys\n",
    "                  regexp_replace(attrs['bestnights'], 'u''(.*?)''', '\"$1\"'),\n",
    "                  '''', '\"'\n",
    "                ),\n",
    "                'False', 'false'\n",
    "              ),\n",
    "              'True', 'true'\n",
    "            ),\n",
    "            'None', 'null'\n",
    "          )\n",
    "        ) AS map(varchar, boolean)\n",
    "      )\n",
    "    ) AS bestnights_map,\n",
    "\n",
    "    -- Parse HairSpecializesIn\n",
    "    TRY(\n",
    "      CAST(\n",
    "        json_parse(\n",
    "          replace(\n",
    "            replace(\n",
    "              replace(\n",
    "                replace(\n",
    "                  -- normalize u'...' keys\n",
    "                  regexp_replace(attrs['hairspecializesin'], 'u''(.*?)''', '\"$1\"'),\n",
    "                  '''', '\"'\n",
    "                ),\n",
    "                'False', 'false'\n",
    "              ),\n",
    "              'True', 'true'\n",
    "            ),\n",
    "            'None', 'null'\n",
    "          )\n",
    "        ) AS map(varchar, boolean)\n",
    "      )\n",
    "    ) AS hairspecializesin_map,\n",
    "\n",
    "    -- Parse DietaryRestrictions\n",
    "    TRY(\n",
    "      CAST(\n",
    "        json_parse(\n",
    "          replace(\n",
    "            replace(\n",
    "              replace(\n",
    "                replace(\n",
    "                  -- normalize u'...' keys\n",
    "                  regexp_replace(attrs['dietaryrestrictions'], 'u''(.*?)''', '\"$1\"'),\n",
    "                  '''', '\"'\n",
    "                ),\n",
    "                'False', 'false'\n",
    "              ),\n",
    "              'True', 'true'\n",
    "            ),\n",
    "            'None', 'null'\n",
    "          )\n",
    "        ) AS map(varchar, boolean)\n",
    "      )\n",
    "    ) AS dietaryrestrictions_map\n",
    "  FROM normalized\n",
    ")\n",
    "SELECT\n",
    "    business_id,\n",
    "\n",
    "    -- Helper pattern: NULLIF(lower(x),'none') but preserving original case when not none\n",
    "    CASE WHEN attrs['acceptsinsurance'] IS NULL OR lower(attrs['acceptsinsurance']) = 'none' THEN NULL ELSE attrs['acceptsinsurance'] END AS acceptsinsurance,\n",
    "    CASE WHEN attrs['agesallowed'] IS NULL OR lower(attrs['agesallowed']) = 'none' THEN NULL ELSE attrs['agesallowed'] END AS agesallowed,\n",
    "    CASE WHEN attrs['alcohol'] IS NULL OR lower(attrs['alcohol']) = 'none' THEN NULL ELSE attrs['alcohol'] END AS alcohol,\n",
    "    CASE WHEN attrs['bikeparking'] IS NULL OR lower(attrs['bikeparking']) = 'none' THEN NULL ELSE attrs['bikeparking'] END AS bikeparking,\n",
    "    CASE WHEN attrs['businessacceptsbitcoin'] IS NULL OR lower(attrs['businessacceptsbitcoin']) = 'none' THEN NULL ELSE attrs['businessacceptsbitcoin'] END AS businessacceptsbitcoin,\n",
    "    CASE WHEN attrs['businessacceptscreditcards'] IS NULL OR lower(attrs['businessacceptscreditcards']) = 'none' THEN NULL ELSE attrs['businessacceptscreditcards'] END AS businessacceptscreditcards,\n",
    "    CASE WHEN attrs['byappointmentonly'] IS NULL OR lower(attrs['byappointmentonly']) = 'none' THEN NULL ELSE attrs['byappointmentonly'] END AS byappointmentonly,\n",
    "    CASE WHEN attrs['byob'] IS NULL OR lower(attrs['byob']) = 'none' THEN NULL ELSE attrs['byob'] END AS byob,\n",
    "    CASE WHEN attrs['byobcorkage'] IS NULL OR lower(attrs['byobcorkage']) = 'none' THEN NULL ELSE attrs['byobcorkage'] END AS byobcorkage,\n",
    "    CASE WHEN attrs['caters'] IS NULL OR lower(attrs['caters']) = 'none' THEN NULL ELSE attrs['caters'] END AS caters,\n",
    "    CASE WHEN attrs['coatcheck'] IS NULL OR lower(attrs['coatcheck']) = 'none' THEN NULL ELSE attrs['coatcheck'] END AS coatcheck,\n",
    "    CASE WHEN attrs['corkage'] IS NULL OR lower(attrs['corkage']) = 'none' THEN NULL ELSE attrs['corkage'] END AS corkage,\n",
    "    CASE WHEN attrs['dogsallowed'] IS NULL OR lower(attrs['dogsallowed']) = 'none' THEN NULL ELSE attrs['dogsallowed'] END AS dogsallowed,\n",
    "    CASE WHEN attrs['drivethru'] IS NULL OR lower(attrs['drivethru']) = 'none' THEN NULL ELSE attrs['drivethru'] END AS drivethru,\n",
    "    CASE WHEN attrs['goodfordancing'] IS NULL OR lower(attrs['goodfordancing']) = 'none' THEN NULL ELSE attrs['goodfordancing'] END AS goodfordancing,\n",
    "    CASE WHEN attrs['goodforkids'] IS NULL OR lower(attrs['goodforkids']) = 'none' THEN NULL ELSE attrs['goodforkids'] END AS goodforkids,\n",
    "    CASE WHEN attrs['happyhour'] IS NULL OR lower(attrs['happyhour']) = 'none' THEN NULL ELSE attrs['happyhour'] END AS happyhour,\n",
    "    CASE WHEN attrs['hastv'] IS NULL OR lower(attrs['hastv']) = 'none' THEN NULL ELSE attrs['hastv'] END AS hastv,\n",
    "    CASE WHEN attrs['music'] IS NULL OR lower(attrs['music']) = 'none' THEN NULL ELSE attrs['music'] END AS music,\n",
    "    CASE WHEN attrs['noiselevel'] IS NULL OR lower(attrs['noiselevel']) = 'none' THEN NULL ELSE attrs['noiselevel'] END AS noiselevel,\n",
    "    CASE WHEN attrs['open24hours'] IS NULL OR lower(attrs['open24hours']) = 'none' THEN NULL ELSE attrs['open24hours'] END AS open24hours,\n",
    "    CASE WHEN attrs['outdoorseating'] IS NULL OR lower(attrs['outdoorseating']) = 'none' THEN NULL ELSE attrs['outdoorseating'] END AS outdoorseating,\n",
    "    CASE WHEN attrs['restaurantsattire'] IS NULL OR lower(attrs['restaurantsattire']) = 'none' THEN NULL ELSE attrs['restaurantsattire'] END AS restaurantsattire,\n",
    "    CASE WHEN attrs['restaurantscounterservice'] IS NULL OR lower(attrs['restaurantscounterservice']) = 'none' THEN NULL ELSE attrs['restaurantscounterservice'] END AS restaurantscounterservice,\n",
    "    CASE WHEN attrs['restaurantsdelivery'] IS NULL OR lower(attrs['restaurantsdelivery']) = 'none' THEN NULL ELSE attrs['restaurantsdelivery'] END AS restaurantsdelivery,\n",
    "    CASE WHEN attrs['restaurantsgoodforgroups'] IS NULL OR lower(attrs['restaurantsgoodforgroups']) = 'none' THEN NULL ELSE attrs['restaurantsgoodforgroups'] END AS restaurantsgoodforgroups,\n",
    "    CASE WHEN attrs['restaurantspricerange2'] IS NULL OR lower(attrs['restaurantspricerange2']) = 'none' THEN NULL ELSE attrs['restaurantspricerange2'] END AS restaurantspricerange2,\n",
    "    CASE WHEN attrs['restaurantsreservations'] IS NULL OR lower(attrs['restaurantsreservations']) = 'none' THEN NULL ELSE attrs['restaurantsreservations'] END AS restaurantsreservations,\n",
    "    CASE WHEN attrs['restaurantstableservice'] IS NULL OR lower(attrs['restaurantstableservice']) = 'none' THEN NULL ELSE attrs['restaurantstableservice'] END AS restaurantstableservice,\n",
    "    CASE WHEN attrs['restaurantstakeout'] IS NULL OR lower(attrs['restaurantstakeout']) = 'none' THEN NULL ELSE attrs['restaurantstakeout'] END AS restaurantstakeout,\n",
    "    CASE WHEN attrs['smoking'] IS NULL OR lower(attrs['smoking']) = 'none' THEN NULL ELSE attrs['smoking'] END AS smoking,\n",
    "    CASE WHEN attrs['wheelchairaccessible'] IS NULL OR lower(attrs['wheelchairaccessible']) = 'none' THEN NULL ELSE attrs['wheelchairaccessible'] END AS wheelchairaccessible,\n",
    "    CASE WHEN attrs['wifi'] IS NULL OR lower(attrs['wifi']) = 'none' THEN NULL ELSE attrs['wifi'] END AS wifi,\n",
    "\n",
    "    -- Parking\n",
    "    parking_map['garage']    AS parking_garage,\n",
    "    parking_map['street']    AS parking_street,\n",
    "    parking_map['validated'] AS parking_validated,\n",
    "    parking_map['lot']       AS parking_lot,\n",
    "    parking_map['valet']     AS parking_valet,\n",
    "\n",
    "    -- Ambience\n",
    "    ambience_map['divey']     AS ambience_divey,\n",
    "    ambience_map['hipster']  AS ambience_hipster,\n",
    "    ambience_map['casual']   AS ambience_casual,\n",
    "    ambience_map['touristy'] AS ambience_touristy,\n",
    "    ambience_map['trendy']   AS ambience_trendy,\n",
    "    ambience_map['intimate'] AS ambience_intimate,\n",
    "    ambience_map['romantic'] AS ambience_romantic,\n",
    "    ambience_map['classy']   AS ambience_classy,\n",
    "    ambience_map['upscale']  AS ambience_upscale,\n",
    "\n",
    "    -- GoodForMeal\n",
    "    goodformeal_map['dessert']    AS good_for_dessert,\n",
    "    goodformeal_map['latenight'] AS good_for_latenight,\n",
    "    goodformeal_map['lunch']     AS good_for_lunch,\n",
    "    goodformeal_map['dinner']    AS good_for_dinner,\n",
    "    goodformeal_map['brunch']    AS good_for_brunch,\n",
    "    goodformeal_map['breakfast'] AS good_for_breakfast,\n",
    "\n",
    "    -- BestNights\n",
    "    bestnights_map['monday']    AS bestnight_monday,\n",
    "    bestnights_map['tuesday']   AS bestnight_tuesday,\n",
    "    bestnights_map['wednesday'] AS bestnight_wednesday,\n",
    "    bestnights_map['thursday']  AS bestnight_thursday,\n",
    "    bestnights_map['friday']    AS bestnight_friday,\n",
    "    bestnights_map['saturday']  AS bestnight_saturday,\n",
    "    bestnights_map['sunday']    AS bestnight_sunday,\n",
    "\n",
    "    -- HairSpecializesIn\n",
    "    hairspecializesin_map['africanamerican'] AS hair_africanamerican,\n",
    "    hairspecializesin_map['asian']           AS hair_asian,\n",
    "    hairspecializesin_map['coloring']        AS hair_coloring,\n",
    "    hairspecializesin_map['curly']           AS hair_curly,\n",
    "    hairspecializesin_map['extensions']      AS hair_extensions,\n",
    "    hairspecializesin_map['kids']            AS hair_kids,\n",
    "    hairspecializesin_map['perms']           AS hair_perms,\n",
    "    hairspecializesin_map['straightperms']   AS hair_straightperms,\n",
    "\n",
    "    -- DietaryRestrictions\n",
    "    dietaryrestrictions_map['dairy-free']      AS dairy_free,\n",
    "    dietaryrestrictions_map['gluten-free']    AS gluten_free,\n",
    "    dietaryrestrictions_map['vegan']           AS vegan,\n",
    "    dietaryrestrictions_map['kosher']          AS kosher,\n",
    "    dietaryrestrictions_map['halal']           AS halal,\n",
    "    dietaryrestrictions_map['soy-free']        AS soy_free,\n",
    "    dietaryrestrictions_map['vegetarian']      AS vegetarian,\n",
    "\n",
    "    -- Hours\n",
    "    hours['monday']    AS hours_monday,\n",
    "    hours['tuesday']   AS hours_tuesday,\n",
    "    hours['wednesday'] AS hours_wednesday,\n",
    "    hours['thursday']  AS hours_thursday,\n",
    "    hours['friday']    AS hours_friday,\n",
    "    hours['saturday']  AS hours_saturday,\n",
    "    hours['sunday']    AS hours_sunday,\n",
    "\n",
    "    CARDINALITY(map_keys(hours)) AS open_days_count,\n",
    "    CASE WHEN hours['saturday'] IS NOT NULL OR hours['sunday'] IS NOT NULL THEN true ELSE false END AS open_on_weekend\n",
    "\n",
    "FROM parsed;\n",
    "\"\"\", database=ATHENA_DB)\n",
    "\n",
    "print(f\"✅ Built {ATHENA_DB}.business_attributes\")\n",
    "print(\"📍 Location:\", business_attributes_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pyathena-connect-header",
   "metadata": {
    "id": "pyathena-connect-header"
   },
   "source": [
    "### 3.5 Connect to Athena with PyAthena\n",
    "\n",
    "Create a connection to query the tables using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pyathena-connect",
   "metadata": {
    "id": "pyathena-connect",
    "outputId": "09025c09-a7ec-415f-d872-2a5758c3aead"
   },
   "outputs": [],
   "source": [
    "# Create PyAthena connection\n",
    "conn = connect(\n",
    "    s3_staging_dir=ATHENA_RESULTS_S3,\n",
    "    region_name=REGION,\n",
    "    cursor_class=PandasCursor\n",
    ")\n",
    "\n",
    "print(f\" Connected to Athena database: {ATHENA_DB}\")\n",
    "print(f\"   Results location: {ATHENA_RESULTS_S3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-tables-header",
   "metadata": {
    "id": "test-tables-header"
   },
   "source": [
    "### 3.6 Test Athena Tables\n",
    "\n",
    "Run sample queries to verify table creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-tables",
   "metadata": {
    "id": "test-tables",
    "outputId": "016d6fd9-53f4-4aa5-ed96-5bf92a3a8aac"
   },
   "outputs": [],
   "source": [
    "# Query business table\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) as total_businesses,\n",
    "    COUNT(DISTINCT city) as unique_cities,\n",
    "    COUNT(DISTINCT state) as unique_states\n",
    "FROM {ATHENA_DB}.business\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing business table...\")\n",
    "df = pd.read_sql(query, conn)\n",
    "display(df)\n",
    "\n",
    "\n",
    "\n",
    "# Query review table\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) as total_reviews,\n",
    "    AVG(stars) as avg_stars,\n",
    "    MIN(stars) as min_stars,\n",
    "    MAX(stars) as max_stars\n",
    "FROM {ATHENA_DB}.review\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nTesting review table...\")\n",
    "df = pd.read_sql(query, conn)\n",
    "display(df)\n",
    "\n",
    "print(\"\\n Athena tables are working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Exploratory Data Analysis <a id='section-4'></a>\n",
    "\n",
    "This section explores the Yelp dataset to understand:\n",
    "- Business distribution across cities and states\n",
    "- Review patterns and rating distributions\n",
    "- Parking availability and its relationship to ratings\n",
    "- Data quality issues\n",
    "\n",
    "**Focus**: Understanding how parking constraints affect business ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-sample-header",
   "metadata": {
    "id": "load-sample-header"
   },
   "source": [
    "### 4.1 Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-sample",
   "metadata": {
    "id": "load-sample",
    "outputId": "12ef6d4c-3ce5-4d76-f68f-9e921571675b"
   },
   "outputs": [],
   "source": [
    "# Load a sample of businesses with parking information\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    business_id,\n",
    "    name,\n",
    "    city,\n",
    "    state,\n",
    "    stars,\n",
    "    review_count,\n",
    "    categories\n",
    "FROM {ATHENA_DB}.business\n",
    "WHERE is_open = 1\n",
    "    AND review_count >= 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading sample business data...\")\n",
    "business_df = pd.read_sql(query, conn)\n",
    "print(f\" Loaded {len(business_df):,} businesses\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "display(business_df.head())\n",
    "\n",
    "# Show basic statistics\n",
    "print(f\"\\n Data Summary:\")\n",
    "print(f\"   Total businesses: {len(business_df):,}\")\n",
    "print(f\"   Unique cities: {business_df['city'].nunique():,}\")\n",
    "print(f\"   Unique states: {business_df['state'].nunique()}\")\n",
    "print(f\"   Average rating: {business_df['stars'].mean():.2f}\")\n",
    "print(f\"   Average reviews: {business_df['review_count'].mean():.0f}\")\n",
    "\n",
    "print(\"\\n Note: Parking features will be extracted from review text in Section 5\")\n",
    "print(\"   This provides more accurate parking information than business attributes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parking-analysis-header",
   "metadata": {
    "id": "parking-analysis-header"
   },
   "source": [
    "### 4.2 Analyze Parking Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parking-analysis",
   "metadata": {
    "id": "parking-analysis",
    "outputId": "667f7644-5089-43b1-d507-1f8b06ff3d22"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Top cities\n",
    "print(\"\\n  Top 10 Cities:\")\n",
    "top_cities = business_df['city'].value_counts().head(10)\n",
    "for idx, (city, count) in enumerate(top_cities.items(), 1):\n",
    "    print(f\"   {idx:2d}. {city:20s} - {count:,} businesses\")\n",
    "\n",
    "# Top states\n",
    "print(\"\\n  Top 10 States:\")\n",
    "top_states = business_df['state'].value_counts().head(10)\n",
    "for idx, (state, count) in enumerate(top_states.items(), 1):\n",
    "    print(f\"   {idx:2d}. {state:5s} - {count:,} businesses\")\n",
    "\n",
    "# Rating distribution\n",
    "print(\"\\n Rating Distribution:\")\n",
    "rating_dist = business_df['stars'].value_counts().sort_index()\n",
    "for stars, count in rating_dist.items():\n",
    "    bar_length = int(count / rating_dist.max() * 40)\n",
    "    bar = \"█\" * bar_length\n",
    "    print(f\"   {stars:.1f} stars: {bar} ({count:,})\")\n",
    "\n",
    "# Review count statistics\n",
    "print(\"\\n Review Count Statistics:\")\n",
    "review_stats = business_df['review_count'].describe()\n",
    "print(f\"   Min:     {review_stats['min']:,.0f}\")\n",
    "print(f\"   25th:    {review_stats['25%']:,.0f}\")\n",
    "print(f\"   Median:  {review_stats['50%']:,.0f}\")\n",
    "print(f\"   75th:    {review_stats['75%']:,.0f}\")\n",
    "print(f\"   Max:     {review_stats['max']:,.0f}\")\n",
    "print(f\"   Mean:    {review_stats['mean']:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {
    "id": "visualization-header"
   },
   "source": [
    "### 4.3 Visualize Key Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualizations",
   "metadata": {
    "id": "visualizations",
    "outputId": "2a86d957-ffdf-4929-b9d0-06b567a0a8f0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Rating Distribution\n",
    "axes[0, 0].hist(business_df['stars'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_title('Distribution of Business Ratings', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Stars', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Count', fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Review Count Distribution (Log Scale)\n",
    "axes[0, 1].hist(business_df['review_count'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].set_title('Distribution of Review Counts (Log Scale)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Reviews', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Count (log scale)', fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Top 10 Cities\n",
    "top_cities = business_df['city'].value_counts().head(10)\n",
    "y_pos = range(len(top_cities))\n",
    "axes[1, 0].barh(y_pos, top_cities.values, alpha=0.7, color='green')\n",
    "axes[1, 0].set_yticks(y_pos)\n",
    "axes[1, 0].set_yticklabels(top_cities.index)\n",
    "axes[1, 0].set_title('Top 10 Cities by Business Count', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Businesses', fontsize=11)\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Top 10 States\n",
    "top_states = business_df['state'].value_counts().head(10)\n",
    "x_pos = range(len(top_states))\n",
    "axes[1, 1].bar(x_pos, top_states.values, alpha=0.7, color='purple')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(top_states.index, rotation=45)\n",
    "axes[1, 1].set_title('Top 10 States by Business Count', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Number of Businesses', fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_business_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualizations created successfully!\")\n",
    "print(\"   Saved as: eda_business_overview.png\")\n",
    "\n",
    "# Additional: Category analysis if categories exist\n",
    "if 'categories' in business_df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CATEGORY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Extract individual categories\n",
    "    all_categories = []\n",
    "    for cats in business_df['categories'].dropna():\n",
    "        if isinstance(cats, str):\n",
    "            all_categories.extend([c.strip() for c in cats.split(',')])\n",
    "\n",
    "    from collections import Counter\n",
    "    category_counts = Counter(all_categories)\n",
    "\n",
    "    print(f\"\\n Total unique categories: {len(category_counts):,}\")\n",
    "    print(\"\\n🔝 Top 15 Categories:\")\n",
    "    for idx, (cat, count) in enumerate(category_counts.most_common(15), 1):\n",
    "        print(f\"   {idx:2d}. {cat:30s} - {count:,} businesses\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Section 4 Complete - EDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Feature Engineering & Feature Store <a id='section-5'></a>\n",
    "\n",
    "This section:\n",
    "- Engineers features from raw data\n",
    "- Creates parking-related features\n",
    "- Stores features in SageMaker Feature Store\n",
    "- Splits data into train/test/validation sets\n",
    "\n",
    "**Key Features**:\n",
    "- Parking availability indicators\n",
    "- Review aggregations\n",
    "- Business characteristics\n",
    "- Target variable: High rating indicator (4+ stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-full-data-header",
   "metadata": {
    "id": "load-full-data-header"
   },
   "source": [
    "### 5.1 Load Full Dataset from Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-full-data",
   "metadata": {
    "id": "load-full-data",
    "outputId": "9972d4c4-f459-4d09-e998-0ae9cd41e347"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Query to join business and review data\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    b.business_id,\n",
    "    b.name,\n",
    "    b.city,\n",
    "    b.state,\n",
    "    b.stars as business_stars,\n",
    "    b.review_count as business_review_count,\n",
    "    b.categories,\n",
    "    r.review_id,\n",
    "    r.user_id,\n",
    "    r.stars as review_stars,\n",
    "    r.useful,\n",
    "    r.funny,\n",
    "    r.cool,\n",
    "    r.text as review_text,\n",
    "    r.date as review_date\n",
    "FROM {ATHENA_DB}.business b\n",
    "INNER JOIN {ATHENA_DB}.review r\n",
    "    ON b.business_id = r.business_id\n",
    "WHERE b.is_open = 1\n",
    "    AND b.review_count >= 10\n",
    "    AND r.stars IS NOT NULL\n",
    "LIMIT 100000\n",
    "\"\"\"\n",
    "\n",
    "print(\"⏳ Executing query...\")\n",
    "df = pd.read_sql(query, conn)\n",
    "print(f\"\\n Loaded {len(df):,} reviews from {df['business_id'].nunique():,} businesses\")\n",
    "print(f\"   Date range: {df['review_date'].min()} to {df['review_date'].max()}\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n Sample data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engineer-features-header",
   "metadata": {
    "id": "engineer-features-header"
   },
   "source": [
    "### 5.2 Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engineer-features",
   "metadata": {
    "id": "engineer-features",
    "outputId": "40570877-23f7-47fe-89eb-ac3e2bd35abf"
   },
   "outputs": [],
   "source": [
    "# Step 1: Extract Parking Information from Review Text\n",
    "print(\"\\n1️  Extracting parking information from review text...\")\n",
    "\n",
    "def extract_parking_features(text):\n",
    "    \"\"\"\n",
    "    Extract parking-related features from review text.\n",
    "    Returns a dict with parking indicators.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {\n",
    "            'mentions_parking': 0,\n",
    "            'parking_positive': 0,\n",
    "            'parking_negative': 0,\n",
    "            'parking_type_lot': 0,\n",
    "            'parking_type_street': 0,\n",
    "            'parking_type_garage': 0,\n",
    "            'parking_type_valet': 0,\n",
    "            'parking_free': 0,\n",
    "            'parking_paid': 0\n",
    "        }\n",
    "\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check if parking is mentioned\n",
    "    parking_keywords = ['parking', 'park', 'parked']\n",
    "    mentions_parking = int(any(keyword in text_lower for keyword in parking_keywords))\n",
    "\n",
    "    # Positive parking indicators\n",
    "    positive_keywords = [\n",
    "        'easy parking', 'plenty of parking', 'ample parking',\n",
    "        'free parking', 'good parking', 'great parking',\n",
    "        'lots of parking', 'parking available', 'easy to park'\n",
    "    ]\n",
    "    parking_positive = int(any(keyword in text_lower for keyword in positive_keywords))\n",
    "\n",
    "    # Negative parking indicators\n",
    "    negative_keywords = [\n",
    "        'no parking', 'parking nightmare', 'hard to park',\n",
    "        'difficult parking', 'parking is terrible', 'parking sucks',\n",
    "        'nowhere to park', 'parking is bad', 'limited parking',\n",
    "        'parking is horrible', 'parking was awful'\n",
    "    ]\n",
    "    parking_negative = int(any(keyword in text_lower for keyword in negative_keywords))\n",
    "\n",
    "    # Parking types\n",
    "    parking_type_lot = int('parking lot' in text_lower or 'lot parking' in text_lower)\n",
    "    parking_type_street = int('street parking' in text_lower or 'park on the street' in text_lower)\n",
    "    parking_type_garage = int('parking garage' in text_lower or 'garage parking' in text_lower)\n",
    "    parking_type_valet = int('valet' in text_lower)\n",
    "\n",
    "    # Cost indicators\n",
    "    parking_free = int('free parking' in text_lower or 'free park' in text_lower)\n",
    "    parking_paid = int('paid parking' in text_lower or 'pay for parking' in text_lower or 'parking fee' in text_lower)\n",
    "\n",
    "    return {\n",
    "        'mentions_parking': mentions_parking,\n",
    "        'parking_positive': parking_positive,\n",
    "        'parking_negative': parking_negative,\n",
    "        'parking_type_lot': parking_type_lot,\n",
    "        'parking_type_street': parking_type_street,\n",
    "        'parking_type_garage': parking_type_garage,\n",
    "        'parking_type_valet': parking_type_valet,\n",
    "        'parking_free': parking_free,\n",
    "        'parking_paid': parking_paid\n",
    "    }\n",
    "\n",
    "# Apply parking extraction\n",
    "parking_features = df['review_text'].apply(extract_parking_features)\n",
    "parking_df = pd.DataFrame(parking_features.tolist())\n",
    "\n",
    "# Add to main dataframe\n",
    "for col in parking_df.columns:\n",
    "    df[col] = parking_df[col]\n",
    "\n",
    "print(f\" Extracted parking features from {len(df):,} reviews\")\n",
    "print(f\"\\n   Parking Statistics:\")\n",
    "print(f\"   - Reviews mentioning parking: {df['mentions_parking'].sum():,} ({df['mentions_parking'].mean()*100:.1f}%)\")\n",
    "print(f\"   - Positive parking mentions: {df['parking_positive'].sum():,}\")\n",
    "print(f\"   - Negative parking mentions: {df['parking_negative'].sum():,}\")\n",
    "print(f\"   - Free parking mentions: {df['parking_free'].sum():,}\")\n",
    "\n",
    "\n",
    "# Step 2: Parse review date and create temporal features\n",
    "print(\"\\n2️  Creating temporal features...\")\n",
    "\n",
    "df['review_date'] = pd.to_datetime(df['review_date'])\n",
    "df['review_year'] = df['review_date'].dt.year\n",
    "df['review_month'] = df['review_date'].dt.month\n",
    "df['review_day_of_week'] = df['review_date'].dt.dayofweek\n",
    "df['review_quarter'] = df['review_date'].dt.quarter\n",
    "\n",
    "print(\" Created temporal features\")\n",
    "\n",
    "\n",
    "# Step 3: Create engagement score\n",
    "print(\"\\n3️  Creating engagement features...\")\n",
    "\n",
    "df['engagement_score'] = df['useful'] + df['funny'] + df['cool']\n",
    "df['is_engaged'] = (df['engagement_score'] > 0).astype(int)\n",
    "\n",
    "print(\" Created engagement features\")\n",
    "\n",
    "\n",
    "# Step 4: Create target variable\n",
    "print(\"\\n4️  Creating target variable...\")\n",
    "\n",
    "df['is_highly_rated'] = (df['review_stars'] >= 4).astype(int)\n",
    "\n",
    "print(f\" Created target variable\")\n",
    "print(f\"   - Highly rated (4+ stars): {df['is_highly_rated'].sum():,} ({df['is_highly_rated'].mean()*100:.1f}%)\")\n",
    "print(f\"   - Not highly rated: {(1-df['is_highly_rated']).sum():,} ({(1-df['is_highly_rated']).mean()*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Step 5: Business-level aggregations\n",
    "print(\"\\n5️  Creating business-level aggregates...\")\n",
    "\n",
    "business_agg = df.groupby('business_id').agg({\n",
    "    'review_stars': ['mean', 'std', 'count', 'min', 'max'],\n",
    "    'engagement_score': ['mean', 'sum'],\n",
    "    'is_highly_rated': 'mean',\n",
    "    'mentions_parking': ['sum', 'mean'],\n",
    "    'parking_positive': 'sum',\n",
    "    'parking_negative': 'sum',\n",
    "    'parking_type_lot': 'sum',\n",
    "    'parking_type_street': 'sum',\n",
    "    'parking_type_garage': 'sum',\n",
    "    'parking_type_valet': 'sum',\n",
    "    'parking_free': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "business_agg.columns = [\n",
    "    'business_id',\n",
    "    'avg_review_stars', 'std_review_stars', 'total_reviews', 'min_review_stars', 'max_review_stars',\n",
    "    'avg_engagement', 'total_engagement',\n",
    "    'pct_highly_rated',\n",
    "    'total_parking_mentions', 'pct_reviews_mention_parking',\n",
    "    'total_positive_parking', 'total_negative_parking',\n",
    "    'total_lot_mentions', 'total_street_mentions', 'total_garage_mentions', 'total_valet_mentions',\n",
    "    'total_free_parking_mentions'\n",
    "]\n",
    "\n",
    "# Create derived parking features\n",
    "business_agg['has_parking_data'] = (business_agg['total_parking_mentions'] > 0).astype(int)\n",
    "business_agg['parking_sentiment'] = (\n",
    "    (business_agg['total_positive_parking'] - business_agg['total_negative_parking']) /\n",
    "    (business_agg['total_parking_mentions'] + 1)  # +1 to avoid division by zero\n",
    ")\n",
    "\n",
    "# Determine dominant parking type\n",
    "business_agg['primary_parking_type'] = business_agg[\n",
    "    ['total_lot_mentions', 'total_street_mentions', 'total_garage_mentions', 'total_valet_mentions']\n",
    "].idxmax(axis=1).str.replace('total_', '').str.replace('_mentions', '')\n",
    "\n",
    "print(f\" Created business-level aggregates\")\n",
    "print(f\"   - Unique businesses: {len(business_agg):,}\")\n",
    "print(f\"   - Businesses with parking data: {business_agg['has_parking_data'].sum():,} ({business_agg['has_parking_data'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n Sample business aggregates:\")\n",
    "display(business_agg.head())\n",
    "\n",
    "\n",
    "# Step 6: Merge aggregates back to reviews\n",
    "print(\"\\n6️  Merging business aggregates back to review data...\")\n",
    "\n",
    "df = df.merge(business_agg, on='business_id', how='left', suffixes=('', '_agg'))\n",
    "\n",
    "print(f\" Merged aggregates - DataFrame now has {len(df.columns)} columns\")\n",
    "\n",
    "\n",
    "# Step 7: Handle categories\n",
    "print(\"\\n7️  Processing business categories...\")\n",
    "\n",
    "# Check if business is a restaurant\n",
    "df['is_restaurant'] = df['categories'].fillna('').str.contains('Restaurant|Food', case=False, na=False).astype(int)\n",
    "\n",
    "# Extract price range from categories (if present)\n",
    "def extract_price_range(categories):\n",
    "    if pd.isna(categories):\n",
    "        return 2  # Default to medium price\n",
    "    # Look for price indicators in categories\n",
    "    if any(word in categories.lower() for word in ['$$$', 'expensive', 'upscale', 'fine dining']):\n",
    "        return 4\n",
    "    elif any(word in categories.lower() for word in ['$$', 'moderate']):\n",
    "        return 3\n",
    "    elif any(word in categories.lower() for word in ['$', 'cheap', 'budget', 'fast food']):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2  # Default\n",
    "\n",
    "df['price_range_numeric'] = df['categories'].apply(extract_price_range)\n",
    "\n",
    "print(f\" Processed categories\")\n",
    "print(f\"   - Restaurants: {df['is_restaurant'].sum():,} ({df['is_restaurant'].mean()*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Step 8: Create enhanced parking score\n",
    "print(\"\\n8️  Creating enhanced parking score...\")\n",
    "\n",
    "# Weighted parking score combining multiple factors\n",
    "df['enhanced_parking_score'] = (\n",
    "    df['parking_positive'] * 2 +           # Positive mention worth 2 points\n",
    "    df['parking_negative'] * -2 +          # Negative mention -2 points\n",
    "    df['parking_free'] * 1 +               # Free parking worth 1 point\n",
    "    df['parking_type_lot'] * 0.5 +         # Lot parking worth 0.5\n",
    "    df['parking_type_garage'] * 0.5 +      # Garage parking worth 0.5\n",
    "    df['parking_type_valet'] * 0.3 +       # Valet worth 0.3\n",
    "    df['parking_type_street'] * 0.2        # Street parking worth 0.2\n",
    ")\n",
    "\n",
    "# Normalize to 0-10 scale\n",
    "df['enhanced_parking_score'] = (\n",
    "    (df['enhanced_parking_score'] - df['enhanced_parking_score'].min()) /\n",
    "    (df['enhanced_parking_score'].max() - df['enhanced_parking_score'].min()) * 10\n",
    ")\n",
    "\n",
    "print(f\" Created enhanced parking score\")\n",
    "print(f\"   - Mean score: {df['enhanced_parking_score'].mean():.2f}\")\n",
    "print(f\"   - Median score: {df['enhanced_parking_score'].median():.2f}\")\n",
    "\n",
    "\n",
    "# Display final feature summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Final Dataset Summary:\")\n",
    "print(f\"   Total records: {len(df):,}\")\n",
    "print(f\"   Total features: {len(df.columns)}\")\n",
    "print(f\"   Unique businesses: {df['business_id'].nunique():,}\")\n",
    "print(f\"   Unique users: {df['user_id'].nunique():,}\")\n",
    "print(f\"\\n Target Variable Distribution:\")\n",
    "print(f\"   Highly rated (4+ stars): {df['is_highly_rated'].sum():,} ({df['is_highly_rated'].mean()*100:.1f}%)\")\n",
    "print(f\"   Not highly rated: {(1-df['is_highly_rated']).sum():,} ({(1-df['is_highly_rated']).mean()*100:.1f}%)\")\n",
    "print(f\"\\n Parking Feature Coverage:\")\n",
    "print(f\"   Reviews with parking mentions: {df['mentions_parking'].sum():,} ({df['mentions_parking'].mean()*100:.1f}%)\")\n",
    "print(f\"   Businesses with parking data: {business_agg['has_parking_data'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-feature-store-header",
   "metadata": {
    "id": "prepare-feature-store-header"
   },
   "source": [
    "### 5.3 Prepare Data for Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-feature-store",
   "metadata": {
    "id": "prepare-feature-store",
    "outputId": "f83f5d53-ab95-4964-cbc1-60b67c51f485"
   },
   "outputs": [],
   "source": [
    "# Select features for Feature Store\n",
    "feature_columns = [\n",
    "    'review_id',  # Primary key\n",
    "    'business_id',\n",
    "    'user_id',\n",
    "    # Parking features (extracted from review text)\n",
    "    'mentions_parking',\n",
    "    'parking_positive',\n",
    "    'parking_negative',\n",
    "    'parking_type_lot',\n",
    "    'parking_type_street',\n",
    "    'parking_type_garage',\n",
    "    'parking_type_valet',\n",
    "    'parking_free',\n",
    "    'parking_paid',\n",
    "    'enhanced_parking_score',\n",
    "    # Business features (from aggregates)\n",
    "    'business_stars',\n",
    "    'business_review_count',\n",
    "    'avg_review_stars',\n",
    "    'std_review_stars',\n",
    "    'total_reviews',\n",
    "    'avg_engagement',\n",
    "    'pct_highly_rated',\n",
    "    'has_parking_data',\n",
    "    'parking_sentiment',\n",
    "    # Review features\n",
    "    'review_stars',\n",
    "    'useful',\n",
    "    'funny',\n",
    "    'cool',\n",
    "    'engagement_score',\n",
    "    'is_engaged',\n",
    "    'review_year',\n",
    "    'review_month',\n",
    "    'review_quarter',\n",
    "    # Business type\n",
    "    'is_restaurant',\n",
    "    'price_range_numeric',\n",
    "    # Target\n",
    "    'is_highly_rated'\n",
    "]\n",
    "\n",
    "# Create feature store dataframe\n",
    "print(\"\\n1️  Selecting features...\")\n",
    "fs_df = df[feature_columns].copy()\n",
    "\n",
    "# Add event time (required by Feature Store)\n",
    "print(\"\\n2️  Adding event time...\")\n",
    "fs_df['event_time'] = pd.Timestamp.now().isoformat()\n",
    "\n",
    "# Remove any remaining nulls\n",
    "print(\"\\n3️  Handling missing values...\")\n",
    "initial_count = len(fs_df)\n",
    "fs_df = fs_df.dropna()\n",
    "print(f\"   Dropped {initial_count - len(fs_df):,} rows with missing values\")\n",
    "print(f\"   Remaining: {len(fs_df):,} rows\")\n",
    "\n",
    "# Add data split column for train/test/validation\n",
    "print(\"\\n4️  Creating train/test/validation splits...\")\n",
    "np.random.seed(42)\n",
    "fs_df['split'] = np.random.choice(\n",
    "    ['train', 'validation', 'test', 'production'],\n",
    "    size=len(fs_df),\n",
    "    p=[0.4, 0.1, 0.1, 0.4]  # 40% train, 10% val, 10% test, 40% production\n",
    ")\n",
    "\n",
    "print(\"\\n Data split distribution:\")\n",
    "print(fs_df['split'].value_counts().sort_index())\n",
    "print(f\"\\n   Train:      {len(fs_df[fs_df['split']=='train']):,} rows\")\n",
    "print(f\"   Validation: {len(fs_df[fs_df['split']=='validation']):,} rows\")\n",
    "print(f\"   Test:       {len(fs_df[fs_df['split']=='test']):,} rows\")\n",
    "print(f\"   Production: {len(fs_df[fs_df['split']=='production']):,} rows\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n Feature Store DataFrame sample:\")\n",
    "display(fs_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SECTION 5 COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFeature Store DataFrame:\")\n",
    "print(f\"  - Total records: {len(fs_df):,}\")\n",
    "print(f\"  - Total features: {len(feature_columns)}\")\n",
    "print(f\"  - Memory usage: {fs_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"\\n Ready to ingest into SageMaker Feature Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-feature-store-header",
   "metadata": {
    "id": "create-feature-store-header"
   },
   "source": [
    "### 5.4 Create SageMaker Feature Store\n",
    "\n",
    "Store the engineered features in SageMaker Feature Store for:\n",
    "- Versioned feature access\n",
    "- Online and offline feature serving\n",
    "- Feature reuse across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-feature-store",
   "metadata": {
    "id": "config-feature-store",
    "outputId": "0498e3e9-0a5c-4a3f-e22f-c403cd444650"
   },
   "outputs": [],
   "source": [
    "# Feature store configuration using Account ID\n",
    "feature_group_name = f\"venuesignal-features-{account_id}\"\n",
    "feature_store_bucket = f\"s3://{FEATURE_DIR}\"\n",
    "\n",
    "print(f\"Feature Group Name: {feature_group_name}\")\n",
    "print(f\"Offline Store: {feature_store_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658f9a9-3123-4ca2-8ad6-a7465552c889",
   "metadata": {
    "id": "f658f9a9-3123-4ca2-8ad6-a7465552c889",
    "outputId": "5c4ad156-d6b8-474d-e5df-859eb266c391"
   },
   "outputs": [],
   "source": [
    "# Create feature group\n",
    "feature_group = FeatureGroup(\n",
    "    name=feature_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Load feature definitions from dataframe\n",
    "feature_group.load_feature_definitions(data_frame=fs_df)\n",
    "\n",
    "print(f\"\\n Feature group configured with {len(fs_df.columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-feature-group",
   "metadata": {
    "id": "create-feature-group",
    "outputId": "5c0f1a8d-f033-4f49-beff-63ab081bdcc4"
   },
   "outputs": [],
   "source": [
    "# Create the feature group (if it doesn't exist)\n",
    "try:\n",
    "    feature_group.create(\n",
    "        s3_uri=feature_store_bucket,\n",
    "        record_identifier_name=\"review_id\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False  # Only offline store for this project\n",
    "    )\n",
    "    print(f\" Created feature group: {feature_group_name}\")\n",
    "    print(\"   Waiting for creation to complete (this may take a few minutes)...\")\n",
    "\n",
    "    # Wait for feature group to be created\n",
    "    import time\n",
    "    while True:\n",
    "        status = feature_group.describe()['FeatureGroupStatus']\n",
    "        if status == 'Created':\n",
    "            print(\" Feature group is ready!\")\n",
    "            break\n",
    "        elif status == 'CreateFailed':\n",
    "            print(\" Feature group creation failed\")\n",
    "            break\n",
    "        print(f\"   Status: {status}...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "except Exception as e:\n",
    "    if 'ResourceInUse' in str(e):\n",
    "        print(f\" Feature group '{feature_group_name}' already exists\")\n",
    "    else:\n",
    "        print(f\" Error creating feature group: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest-features",
   "metadata": {
    "id": "ingest-features",
    "outputId": "4309979e-3fcf-4e8d-d4e1-a916b014f254"
   },
   "outputs": [],
   "source": [
    "# Ingest features into feature store\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Fixing event_time format...\")\n",
    "current_time = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "fs_df['event_time'] = current_time  # Use same timestamp for all rows\n",
    "fs_df['event_time'] = fs_df['event_time'].astype(str)  # Ensure string type\n",
    "print(f\" Fixed event_time: {current_time}\")\n",
    "\n",
    "# Now ingest in small batches\n",
    "print(f\"\\nIngesting {len(fs_df):,} records...\")\n",
    "batch_size = 1000\n",
    "successful = 0\n",
    "\n",
    "for i in range(0, len(fs_df), batch_size):\n",
    "    batch = fs_df.iloc[i:i+batch_size].copy()\n",
    "    batch['event_time'] = current_time  # Ensure format\n",
    "\n",
    "    try:\n",
    "        feature_group.ingest(data_frame=batch, max_workers=2, wait=True)\n",
    "        successful += len(batch)\n",
    "        print(f\"   Batch {i//batch_size + 1}: {successful:,} total records\", end='\\r')\n",
    "    except Exception as e:\n",
    "        if 'already' in str(e).lower():\n",
    "            print(f\"    Batch {i//batch_size + 1}: Already ingested\")\n",
    "        else:\n",
    "            print(f\"   Batch {i//batch_size + 1}: {str(e)[:80]}\")\n",
    "\n",
    "print(f\"\\n\\n Ingestion complete! {successful:,} records ingested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-features-header",
   "metadata": {
    "id": "export-features-header"
   },
   "source": [
    "### 5.5 Export Features for Training\n",
    "\n",
    "Export features from Feature Store to S3 for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-features",
   "metadata": {
    "id": "export-features",
    "outputId": "36c80260-dbfe-4993-c610-e3bf675cfcc1"
   },
   "outputs": [],
   "source": [
    "# Step 1: Split the data\n",
    "print(\"\\n1️  Splitting data...\")\n",
    "train_df = fs_df[fs_df['split'] == 'train'].drop(columns=['event_time', 'split'])\n",
    "validation_df = fs_df[fs_df['split'] == 'validation'].drop(columns=['event_time', 'split'])\n",
    "test_df = fs_df[fs_df['split'] == 'test'].drop(columns=['event_time', 'split'])\n",
    "production_df = fs_df[fs_df['split'] == 'production'].drop(columns=['event_time', 'split'])\n",
    "\n",
    "print(f\"    Train:      {len(train_df):,} records\")\n",
    "print(f\"    Validation: {len(validation_df):,} records\")\n",
    "print(f\"    Test:       {len(test_df):,} records\")\n",
    "print(f\"    Production: {len(production_df):,} records\")\n",
    "\n",
    "# Step 2: Save locally (SIMPLE path - no complex directories!)\n",
    "print(\"\\n2️  Saving to local files...\")\n",
    "\n",
    "# Just use /tmp directly - simple and always works!\n",
    "train_df.to_csv('/tmp/train.csv', index=False)\n",
    "print(\"    /tmp/train.csv\")\n",
    "\n",
    "validation_df.to_csv('/tmp/validation.csv', index=False)\n",
    "print(\"    /tmp/validation.csv\")\n",
    "\n",
    "test_df.to_csv('/tmp/test.csv', index=False)\n",
    "print(\"    /tmp/test.csv\")\n",
    "\n",
    "production_df.to_csv('/tmp/production.csv', index=False)\n",
    "print(\"    /tmp/production.csv\")\n",
    "\n",
    "# Step 3: Upload to S3\n",
    "print(\"\\n3️  Uploading to S3...\")\n",
    "\n",
    "# Define S3 paths\n",
    "train_data_path = f\"s3://{FEATURE_DIR}training-data/train.csv\"\n",
    "validation_data_path = f\"s3://{FEATURE_DIR}training-data/validation.csv\"\n",
    "test_data_path = f\"s3://{FEATURE_DIR}training-data/test.csv\"\n",
    "production_data_path = f\"s3://{FEATURE_DIR}training-data/production.csv\"\n",
    "\n",
    "# Upload each file\n",
    "uploads = [\n",
    "    ('/tmp/train.csv', train_data_path, 'train.csv'),\n",
    "    ('/tmp/validation.csv', validation_data_path, 'validation.csv'),\n",
    "    ('/tmp/test.csv', test_data_path, 'test.csv'),\n",
    "    ('/tmp/production.csv', production_data_path, 'production.csv')\n",
    "]\n",
    "\n",
    "for local_file, s3_uri, display_name in uploads:\n",
    "    bucket = s3_uri.split('/')[2]\n",
    "    key = '/'.join(s3_uri.split('/')[3:])\n",
    "\n",
    "    try:\n",
    "        s3_client.upload_file(local_file, bucket, key)\n",
    "        print(f\"    {display_name} → {s3_uri}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    {display_name} failed: {e}\")\n",
    "\n",
    "# Step 4: Store variables\n",
    "print(\"\\n4️  Storing variables...\")\n",
    "%store train_data_path\n",
    "%store validation_data_path\n",
    "%store test_data_path\n",
    "%store production_data_path\n",
    "\n",
    "print(\"    Variables stored\")\n",
    "\n",
    "# Step 5: Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SECTION 5.7 COMPLETE - DATA EXPORTED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n S3 Locations:\")\n",
    "print(f\"   Train:      {train_data_path}\")\n",
    "print(f\"   Validation: {validation_data_path}\")\n",
    "print(f\"   Test:       {test_data_path}\")\n",
    "print(f\"   Production: {production_data_path}\")\n",
    "\n",
    "print(f\"  Local Files (temporary):\")\n",
    "for filename in ['train.csv', 'validation.csv', 'test.csv', 'production.csv']:\n",
    "    filepath = f'/tmp/{filename}'\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"   {filepath:30s} - {size_mb:6.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SECTION 5 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n Summary:\")\n",
    "print(f\"    Features engineered from review text\")\n",
    "print(f\"    {len(fs_df):,} total records processed\")\n",
    "print(f\"    Training data split and exported\")\n",
    "print(f\"    All data in S3 and ready for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Model Training <a id='section-6'></a>\n",
    "\n",
    "This section trains and evaluates multiple models:\n",
    "\n",
    "1. **Baseline Model #1**: Simple heuristic (business average rating)\n",
    "2. **Baseline Model #2**: Linear regression with key features\n",
    "3. **XGBoost Model**: Gradient boosted trees for classification\n",
    "\n",
    "**Goal**: Predict whether a review will be highly rated (4+ stars) based on business characteristics, especially parking availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9194d-17bb-4577-9fb0-e8c65ba3c1c1",
   "metadata": {
    "id": "8ae9194d-17bb-4577-9fb0-e8c65ba3c1c1"
   },
   "outputs": [],
   "source": [
    "# Define S3 paths\n",
    "train_data_path = f\"s3://{FEATURE_DIR}training-data/train.csv\"\n",
    "validation_data_path = f\"s3://{FEATURE_DIR}training-data/validation.csv\"\n",
    "test_data_path = f\"s3://{FEATURE_DIR}training-data/test.csv\"\n",
    "production_data_path = f\"s3://{FEATURE_DIR}training-data/production.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-training-data-header",
   "metadata": {
    "id": "load-training-data-header"
   },
   "source": [
    "### 6.1 Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-training-data",
   "metadata": {
    "id": "load-training-data",
    "outputId": "d4686f6b-3757-4e26-ea95-bf6287b4a6ae"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "print(\"\\n1️  Loading training data...\")\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "validation_df = pd.read_csv(validation_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "print(f\"   Training:   {len(train_df):,} records\")\n",
    "print(f\"   Validation: {len(validation_df):,} records\")\n",
    "print(f\"   Test:       {len(test_df):,} records\")\n",
    "\n",
    "# Separate features and targets\n",
    "print(\"\\n2️  Preparing features and targets...\")\n",
    "\n",
    "# Classification target (binary)\n",
    "y_train_class = train_df['is_highly_rated']\n",
    "y_val_class = validation_df['is_highly_rated']\n",
    "y_test_class = test_df['is_highly_rated']\n",
    "\n",
    "# Regression target (actual stars)\n",
    "y_train_stars = train_df['review_stars']\n",
    "y_val_stars = validation_df['review_stars']\n",
    "y_test_stars = test_df['review_stars']\n",
    "\n",
    "print(f\"\\n   Classification target distribution:\")\n",
    "print(f\"   - Highly rated (1): {y_train_class.sum():,} ({y_train_class.mean()*100:.1f}%)\")\n",
    "print(f\"   - Not highly rated (0): {(1-y_train_class).sum():,} ({(1-y_train_class.mean())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Star rating distribution:\")\n",
    "print(f\"   - Mean: {y_train_stars.mean():.2f}\")\n",
    "print(f\"   - Std:  {y_train_stars.std():.2f}\")\n",
    "print(f\"   - Range: {y_train_stars.min():.0f} - {y_train_stars.max():.0f}\")\n",
    "\n",
    "def evaluate_model_comprehensive(y_true_class, y_pred_class, y_true_stars, y_pred_stars,\n",
    "                                   y_pred_proba=None, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with both classification and regression metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true_class: True binary labels (0/1)\n",
    "    - y_pred_class: Predicted binary labels (0/1)\n",
    "    - y_true_stars: True star ratings (1-5)\n",
    "    - y_pred_stars: Predicted star ratings (1-5)\n",
    "    - y_pred_proba: Predicted probabilities (optional, for ROC-AUC)\n",
    "    - dataset_name: Name for display\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Classification Metrics\n",
    "    results['accuracy'] = accuracy_score(y_true_class, y_pred_class)\n",
    "    results['precision'] = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    results['recall'] = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    results['f1'] = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "    if y_pred_proba is not None:\n",
    "        results['roc_auc'] = roc_auc_score(y_true_class, y_pred_proba)\n",
    "    else:\n",
    "        results['roc_auc'] = None\n",
    "\n",
    "    # Regression Metrics\n",
    "    results['mse'] = mean_squared_error(y_true_stars, y_pred_stars)\n",
    "    results['rmse'] = np.sqrt(results['mse'])\n",
    "    results['mae'] = mean_absolute_error(y_true_stars, y_pred_stars)\n",
    "    results['r2'] = r2_score(y_true_stars, y_pred_stars)\n",
    "\n",
    "    # Custom Metrics: Within X stars\n",
    "    abs_error = np.abs(y_true_stars - y_pred_stars)\n",
    "    results['within_0.5_stars'] = (abs_error <= 0.5).mean()\n",
    "    results['within_1.0_stars'] = (abs_error <= 1.0).mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results, model_name, dataset_name):\n",
    "    \"\"\"Pretty print evaluation results.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name} - {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\n Classification Metrics (Binary: Highly Rated vs Not):\")\n",
    "    print(f\"   Accuracy:  {results['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {results['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {results['recall']:.4f}\")\n",
    "    print(f\"   F1-Score:  {results['f1']:.4f}\")\n",
    "    if results['roc_auc'] is not None:\n",
    "        print(f\"   ROC-AUC:   {results['roc_auc']:.4f}\")\n",
    "\n",
    "    print(f\"\\n Regression Metrics (Star Rating Prediction):\")\n",
    "    print(f\"   MSE:       {results['mse']:.4f}\")\n",
    "    print(f\"   RMSE:      {results['rmse']:.4f}\")\n",
    "    print(f\"   MAE:       {results['mae']:.4f}\")\n",
    "    print(f\"   R²:        {results['r2']:.4f}\")\n",
    "\n",
    "    print(f\"\\n Accuracy Metrics (Star Prediction):\")\n",
    "    print(f\"   Within 0.5 stars: {results['within_0.5_stars']*100:.2f}%\")\n",
    "    print(f\"   Within 1.0 stars: {results['within_1.0_stars']*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-1-header",
   "metadata": {
    "id": "baseline-1-header"
   },
   "source": [
    "### 6.2 Baseline Model #1: Simple Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-1",
   "metadata": {
    "id": "baseline-1",
    "outputId": "6264da46-96e9-4aee-9621-910a2327e03e"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODEL #1: Simple Heuristic\")\n",
    "print(\"=\"*80)\n",
    "print(\"Approach: Predict highly_rated if avg_review_stars >= 4.0\")\n",
    "\n",
    "# Classification predictions\n",
    "baseline1_pred_class_train = (train_df['avg_review_stars'] >= 4.0).astype(int)\n",
    "baseline1_pred_class_val = (validation_df['avg_review_stars'] >= 4.0).astype(int)\n",
    "baseline1_pred_class_test = (test_df['avg_review_stars'] >= 4.0).astype(int)\n",
    "\n",
    "# Star predictions (use average directly)\n",
    "baseline1_pred_stars_train = train_df['avg_review_stars']\n",
    "baseline1_pred_stars_val = validation_df['avg_review_stars']\n",
    "baseline1_pred_stars_test = test_df['avg_review_stars']\n",
    "\n",
    "# Evaluate\n",
    "baseline1_results_train = evaluate_model_comprehensive(\n",
    "    y_train_class, baseline1_pred_class_train,\n",
    "    y_train_stars, baseline1_pred_stars_train,\n",
    "    y_pred_proba=None, dataset_name=\"Training\"\n",
    ")\n",
    "\n",
    "baseline1_results_val = evaluate_model_comprehensive(\n",
    "    y_val_class, baseline1_pred_class_val,\n",
    "    y_val_stars, baseline1_pred_stars_val,\n",
    "    y_pred_proba=None, dataset_name=\"Validation\"\n",
    ")\n",
    "\n",
    "baseline1_results_test = evaluate_model_comprehensive(\n",
    "    y_test_class, baseline1_pred_class_test,\n",
    "    y_test_stars, baseline1_pred_stars_test,\n",
    "    y_pred_proba=None, dataset_name=\"Test\"\n",
    ")\n",
    "\n",
    "print_results(baseline1_results_train, \"Baseline #1: Heuristic\", \"Training Set\")\n",
    "print_results(baseline1_results_val, \"Baseline #1: Heuristic\", \"Validation Set\")\n",
    "print_results(baseline1_results_test, \"Baseline #1: Heuristic\", \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-2-header",
   "metadata": {
    "id": "baseline-2-header"
   },
   "source": [
    "### 6.3 Baseline Model #2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-2",
   "metadata": {
    "id": "baseline-2",
    "outputId": "3c9c6aa5-d421-4fc5-bf45-8144094c0489"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Select features\n",
    "baseline2_features = [\n",
    "    'avg_review_stars',\n",
    "    'enhanced_parking_score',\n",
    "    'business_review_count'\n",
    "]\n",
    "\n",
    "print(f\"Features: {', '.join(baseline2_features)}\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_df[baseline2_features].fillna(0)\n",
    "X_val = validation_df[baseline2_features].fillna(0)\n",
    "X_test = test_df[baseline2_features].fillna(0)\n",
    "\n",
    "print(f\"\\n⏳ Training logistic regression...\")\n",
    "baseline2_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "baseline2_model.fit(X_train, y_train_class)\n",
    "print(\" Model trained!\")\n",
    "\n",
    "# Classification predictions\n",
    "baseline2_pred_class_train = baseline2_model.predict(X_train)\n",
    "baseline2_pred_class_val = baseline2_model.predict(X_val)\n",
    "baseline2_pred_class_test = baseline2_model.predict(X_test)\n",
    "\n",
    "# Probabilities\n",
    "baseline2_prob_train = baseline2_model.predict_proba(X_train)[:, 1]\n",
    "baseline2_prob_val = baseline2_model.predict_proba(X_val)[:, 1]\n",
    "baseline2_prob_test = baseline2_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert probabilities to star predictions (1-5 scale)\n",
    "# Probability 0-1 maps to stars 1-5\n",
    "def prob_to_stars(prob):\n",
    "    \"\"\"Convert probability to star rating (1-5 scale).\"\"\"\n",
    "    return 1 + (prob * 4)  # Maps 0->1, 0.5->3, 1->5\n",
    "\n",
    "baseline2_pred_stars_train = prob_to_stars(baseline2_prob_train)\n",
    "baseline2_pred_stars_val = prob_to_stars(baseline2_prob_val)\n",
    "baseline2_pred_stars_test = prob_to_stars(baseline2_prob_test)\n",
    "\n",
    "# Evaluate\n",
    "baseline2_results_train = evaluate_model_comprehensive(\n",
    "    y_train_class, baseline2_pred_class_train,\n",
    "    y_train_stars, baseline2_pred_stars_train,\n",
    "    y_pred_proba=baseline2_prob_train, dataset_name=\"Training\"\n",
    ")\n",
    "\n",
    "baseline2_results_val = evaluate_model_comprehensive(\n",
    "    y_val_class, baseline2_pred_class_val,\n",
    "    y_val_stars, baseline2_pred_stars_val,\n",
    "    y_pred_proba=baseline2_prob_val, dataset_name=\"Validation\"\n",
    ")\n",
    "\n",
    "baseline2_results_test = evaluate_model_comprehensive(\n",
    "    y_test_class, baseline2_pred_class_test,\n",
    "    y_test_stars, baseline2_pred_stars_test,\n",
    "    y_pred_proba=baseline2_prob_test, dataset_name=\"Test\"\n",
    ")\n",
    "\n",
    "print_results(baseline2_results_train, \"Baseline #2: Logistic Regression\", \"Training Set\")\n",
    "print_results(baseline2_results_val, \"Baseline #2: Logistic Regression\", \"Validation Set\")\n",
    "print_results(baseline2_results_test, \"Baseline #2: Logistic Regression\", \"Test Set\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n Feature Importance (Coefficients):\")\n",
    "for feature, coef in zip(baseline2_features, baseline2_model.coef_[0]):\n",
    "    print(f\"   {feature:30s}: {coef:8.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc1524-ab3a-495c-8ed7-3c39bb927209",
   "metadata": {
    "id": "a9dc1524-ab3a-495c-8ed7-3c39bb927209",
    "outputId": "332171d9-3d18-43c5-c595-da3b23982ed9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE MODEL COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - VALIDATION SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC',\n",
    "        'MSE', 'RMSE', 'MAE', 'R²',\n",
    "        'Within 0.5★', 'Within 1.0★'\n",
    "    ],\n",
    "    'Baseline #1 (Heuristic)': [\n",
    "        baseline1_results_val['accuracy'],\n",
    "        baseline1_results_val['precision'],\n",
    "        baseline1_results_val['recall'],\n",
    "        baseline1_results_val['f1'],\n",
    "        baseline1_results_val['roc_auc'] if baseline1_results_val['roc_auc'] else 0,\n",
    "        baseline1_results_val['mse'],\n",
    "        baseline1_results_val['rmse'],\n",
    "        baseline1_results_val['mae'],\n",
    "        baseline1_results_val['r2'],\n",
    "        baseline1_results_val['within_0.5_stars'],\n",
    "        baseline1_results_val['within_1.0_stars']\n",
    "    ],\n",
    "    'Baseline #2 (LogReg)': [\n",
    "        baseline2_results_val['accuracy'],\n",
    "        baseline2_results_val['precision'],\n",
    "        baseline2_results_val['recall'],\n",
    "        baseline2_results_val['f1'],\n",
    "        baseline2_results_val['roc_auc'],\n",
    "        baseline2_results_val['mse'],\n",
    "        baseline2_results_val['rmse'],\n",
    "        baseline2_results_val['mae'],\n",
    "        baseline2_results_val['r2'],\n",
    "        baseline2_results_val['within_0.5_stars'],\n",
    "        baseline2_results_val['within_1.0_stars']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n Validation Set Performance:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Highlight best scores\n",
    "print(\"\\n Best Scores (Validation Set):\")\n",
    "print(\"\\nClassification Metrics:\")\n",
    "print(f\"   Best Accuracy:  {max(baseline1_results_val['accuracy'], baseline2_results_val['accuracy']):.4f}\")\n",
    "print(f\"   Best F1-Score:  {max(baseline1_results_val['f1'], baseline2_results_val['f1']):.4f}\")\n",
    "\n",
    "print(\"\\nRegression Metrics:\")\n",
    "print(f\"   Best RMSE:      {min(baseline1_results_val['rmse'], baseline2_results_val['rmse']):.4f}\")\n",
    "print(f\"   Best R²:        {max(baseline1_results_val['r2'], baseline2_results_val['r2']):.4f}\")\n",
    "\n",
    "print(\"\\nPrediction Accuracy:\")\n",
    "print(f\"   Best Within 0.5★: {max(baseline1_results_val['within_0.5_stars'], baseline2_results_val['within_0.5_stars'])*100:.2f}%\")\n",
    "print(f\"   Best Within 1.0★: {max(baseline1_results_val['within_1.0_stars'], baseline2_results_val['within_1.0_stars'])*100:.2f}%\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STORE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Store for later comparison with XGBoost\n",
    "baseline_results = {\n",
    "    'baseline1': {\n",
    "        'train': baseline1_results_train,\n",
    "        'val': baseline1_results_val,\n",
    "        'test': baseline1_results_test\n",
    "    },\n",
    "    'baseline2': {\n",
    "        'train': baseline2_results_train,\n",
    "        'val': baseline2_results_val,\n",
    "        'test': baseline2_results_test\n",
    "    }\n",
    "}\n",
    "\n",
    "%store baseline_results\n",
    "%store baseline2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgboost-header",
   "metadata": {
    "id": "xgboost-header"
   },
   "source": [
    "### 6.4 XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdfebf-547d-4a06-9be6-88313909371b",
   "metadata": {
    "id": "53cdfebf-547d-4a06-9be6-88313909371b",
    "outputId": "2532db33-d321-4b47-9ff5-dc6d970ca058"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "print(\"\\n1️  Configuring XGBoost training...\")\n",
    "\n",
    "# Select features for XGBoost (more features than baseline)\n",
    "xgb_features = [\n",
    "    # Business features\n",
    "    'avg_review_stars',\n",
    "    'std_review_stars',\n",
    "    'business_review_count',\n",
    "    'pct_highly_rated',\n",
    "    # Parking features\n",
    "    'enhanced_parking_score',\n",
    "    'parking_positive',\n",
    "    'parking_negative',\n",
    "    'parking_sentiment',\n",
    "    'has_parking_data',\n",
    "    # Review engagement\n",
    "    'avg_engagement',\n",
    "    # Business attributes\n",
    "    'is_restaurant',\n",
    "    'price_range_numeric'\n",
    "]\n",
    "\n",
    "print(f\"   Selected {len(xgb_features)} features for XGBoost\")\n",
    "print(f\"\\n   Features:\")\n",
    "for i, feat in enumerate(xgb_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "# Prepare data for XGBoost (target must be first column, no header)\n",
    "print(\"\\n2️  Preparing data for XGBoost format...\")\n",
    "\n",
    "def prepare_xgb_data(df, features, target='is_highly_rated'):\n",
    "    \"\"\"Prepare data in XGBoost format: target first, no header.\"\"\"\n",
    "    # Select features and fill missing values\n",
    "    X = df[features].fillna(0)\n",
    "    y = df[target]\n",
    "\n",
    "    # Combine with target first\n",
    "    xgb_data = pd.concat([y, X], axis=1)\n",
    "\n",
    "    return xgb_data\n",
    "\n",
    "# Prepare datasets\n",
    "train_xgb = prepare_xgb_data(train_df, xgb_features)\n",
    "val_xgb = prepare_xgb_data(validation_df, xgb_features)\n",
    "\n",
    "print(f\"   Training data shape: {train_xgb.shape}\")\n",
    "print(f\"   Validation data shape: {val_xgb.shape}\")\n",
    "\n",
    "# Save locally\n",
    "print(\"\\n3️  Saving XGBoost-formatted data...\")\n",
    "train_xgb.to_csv('/tmp/train_xgb.csv', index=False, header=False)\n",
    "val_xgb.to_csv('/tmp/val_xgb.csv', index=False, header=False)\n",
    "print(\"    Data saved in XGBoost format\")\n",
    "\n",
    "# Upload to S3\n",
    "print(\"\\n4️  Uploading to S3...\")\n",
    "xgb_train_path = f\"s3://{MODEL_DIR}xgboost-training/train.csv\"\n",
    "xgb_val_path = f\"s3://{MODEL_DIR}xgboost-training/validation.csv\"\n",
    "xgb_output_path = f\"s3://{MODEL_DIR}xgboost-output\"\n",
    "\n",
    "bucket = BASE_BUCKET_NAME\n",
    "s3_client.upload_file('/tmp/train_xgb.csv', bucket, f'{MODEL_PREFIX}xgboost-training/train.csv')\n",
    "s3_client.upload_file('/tmp/val_xgb.csv', bucket, f'{MODEL_PREFIX}xgboost-training/validation.csv')\n",
    "\n",
    "print(f\"    Training data: {xgb_train_path}\")\n",
    "print(f\"    Validation data: {xgb_val_path}\")\n",
    "print(f\"    Output path: {xgb_output_path}\")\n",
    "\n",
    "# Get XGBoost container\n",
    "print(\"\\n5️  Getting XGBoost container...\")\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve('xgboost', REGION, version='1.5-1')\n",
    "print(f\"   Container: {container}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546b9d1-e9a6-462b-8594-076bfefad325",
   "metadata": {
    "id": "1546b9d1-e9a6-462b-8594-076bfefad325",
    "outputId": "7706e864-a29d-4bbb-9f6a-3a9dc6ba4ae1"
   },
   "outputs": [],
   "source": [
    "# Create estimator\n",
    "print(\"\\n6️  Creating XGBoost estimator...\")\n",
    "\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=xgb_output_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='venuesignal-xgboost'\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    max_depth=6,\n",
    "    eta=0.3,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "print(\"    Estimator configured\")\n",
    "print(f\"\\n   Hyperparameters:\")\n",
    "print(f\"   - Objective: binary:logistic\")\n",
    "print(f\"   - Num rounds: 100\")\n",
    "print(f\"   - Max depth: 6\")\n",
    "print(f\"   - Learning rate (eta): 0.3\")\n",
    "print(f\"   - Early stopping: 10 rounds\")\n",
    "\n",
    "# Create training input channels\n",
    "print(\"\\n7️  Creating training input channels...\")\n",
    "\n",
    "train_input = TrainingInput(xgb_train_path, content_type='text/csv')\n",
    "val_input = TrainingInput(xgb_val_path, content_type='text/csv')\n",
    "\n",
    "# Train model\n",
    "print(\"\\n8️  Starting XGBoost training...\")\n",
    "print(f\"   Training on {len(train_xgb):,} records\")\n",
    "print(f\"   Validating on {len(val_xgb):,} records\")\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': train_input,\n",
    "    'validation': val_input\n",
    "})\n",
    "\n",
    "print(\"\\n XGBoost training complete!\")\n",
    "print(f\"   Model artifacts: {xgb_estimator.model_data}\")\n",
    "\n",
    "# Store model data path\n",
    "xgb_model_data = xgb_estimator.model_data\n",
    "%store xgb_model_data\n",
    "%store xgb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb131fe5-ba9c-4cae-94d3-b551a1cafb8c",
   "metadata": {
    "id": "fb131fe5-ba9c-4cae-94d3-b551a1cafb8c",
    "outputId": "0c7a3807-70cc-40f6-e075-29874f911c88"
   },
   "outputs": [],
   "source": [
    "# Section 6.3: XGBoost Evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6.3: XGBOOST MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For evaluation, we need to make predictions locally\n",
    "# Download the model and make predictions\n",
    "\n",
    "print(\"\\n Making predictions with trained XGBoost model...\")\n",
    "print(\"   (Note: In production, this would use SageMaker endpoint)\")\n",
    "\n",
    "# Alternative: Use local XGBoost with same hyperparameters\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"\\n1️  Training local XGBoost for evaluation...\")\n",
    "\n",
    "# Prepare data for local XGBoost\n",
    "X_train_xgb = train_df[xgb_features].fillna(0)\n",
    "X_val_xgb = validation_df[xgb_features].fillna(0)\n",
    "X_test_xgb = test_df[xgb_features].fillna(0)\n",
    "\n",
    "# Train local model with same hyperparameters\n",
    "xgb_local = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.3,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_local.fit(\n",
    "    X_train_xgb, y_train_class,\n",
    "    eval_set=[(X_val_xgb, y_val_class)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\" Local XGBoost model trained\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n2️  Making predictions...\")\n",
    "\n",
    "# Classification predictions\n",
    "xgb_pred_class_train = xgb_local.predict(X_train_xgb)\n",
    "xgb_pred_class_val = xgb_local.predict(X_val_xgb)\n",
    "xgb_pred_class_test = xgb_local.predict(X_test_xgb)\n",
    "\n",
    "# Probabilities\n",
    "xgb_prob_train = xgb_local.predict_proba(X_train_xgb)[:, 1]\n",
    "xgb_prob_val = xgb_local.predict_proba(X_val_xgb)[:, 1]\n",
    "xgb_prob_test = xgb_local.predict_proba(X_test_xgb)[:, 1]\n",
    "\n",
    "# Convert to star predictions\n",
    "xgb_pred_stars_train = prob_to_stars(xgb_prob_train)\n",
    "xgb_pred_stars_val = prob_to_stars(xgb_prob_val)\n",
    "xgb_pred_stars_test = prob_to_stars(xgb_prob_test)\n",
    "\n",
    "print(\" Predictions complete\")\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"\\n3️  Evaluating XGBoost model...\")\n",
    "\n",
    "xgb_results_train = evaluate_model_comprehensive(\n",
    "    y_train_class, xgb_pred_class_train,\n",
    "    y_train_stars, xgb_pred_stars_train,\n",
    "    y_pred_proba=xgb_prob_train, dataset_name=\"Training\"\n",
    ")\n",
    "\n",
    "xgb_results_val = evaluate_model_comprehensive(\n",
    "    y_val_class, xgb_pred_class_val,\n",
    "    y_val_stars, xgb_pred_stars_val,\n",
    "    y_pred_proba=xgb_prob_val, dataset_name=\"Validation\"\n",
    ")\n",
    "\n",
    "xgb_results_test = evaluate_model_comprehensive(\n",
    "    y_test_class, xgb_pred_class_test,\n",
    "    y_test_stars, xgb_pred_stars_test,\n",
    "    y_pred_proba=xgb_prob_test, dataset_name=\"Test\"\n",
    ")\n",
    "\n",
    "print_results(xgb_results_train, \"XGBoost Model\", \"Training Set\")\n",
    "print_results(xgb_results_val, \"XGBoost Model\", \"Validation Set\")\n",
    "print_results(xgb_results_test, \"XGBoost Model\", \"Test Set\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"XGBOOST FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "importance_dict = dict(zip(xgb_features, xgb_local.feature_importances_))\n",
    "importance_sorted = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n Top 10 Most Important Features:\")\n",
    "for i, (feature, importance) in enumerate(importance_sorted[:10], 1):\n",
    "    bar_length = int(importance * 50)\n",
    "    bar = \"█\" * bar_length\n",
    "    print(f\"   {i:2d}. {feature:30s} {bar} {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Model Deployment <a id='section-7'></a>\n",
    "\n",
    "This section:\n",
    "- Registers the XGBoost model in SageMaker Model Registry\n",
    "- Creates a SageMaker endpoint for real-time inference\n",
    "- Tests the deployed model\n",
    "\n",
    "**Deployment Strategy**: Real-time endpoint for individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fac225-3cbe-4331-b8b3-258b1d01bf34",
   "metadata": {
    "id": "32fac225-3cbe-4331-b8b3-258b1d01bf34"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from time import sleep, gmtime, strftime, time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#sklearn\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, auc, precision_score, recall_score\n",
    "\n",
    "# Sagemaker imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Initialize clients\n",
    "sm = boto3.client('sagemaker', region_name=REGION)\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff884da-2ee2-44d9-9a8b-5efdccf7615b",
   "metadata": {
    "id": "eff884da-2ee2-44d9-9a8b-5efdccf7615b"
   },
   "source": [
    "### Section 7.1 Prepare Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641975c-7872-4361-8a4e-015535c19ca2",
   "metadata": {
    "id": "d641975c-7872-4361-8a4e-015535c19ca2",
    "outputId": "22bec486-4073-49e5-a47e-a5c1a90f8921"
   },
   "outputs": [],
   "source": [
    "#@title Section 7.1 Prepare Model Metadata\n",
    "# Get training job details\n",
    "model_name = xgb_estimator.latest_training_job.name\n",
    "print(f\"\\n📋 Model Information:\")\n",
    "print(f\"   Training job name: {model_name}\")\n",
    "\n",
    "# Get model artifacts\n",
    "info = sm.describe_training_job(TrainingJobName=model_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"   Model artifacts: {model_data}\")\n",
    "\n",
    "# Get XGBoost container image\n",
    "from sagemaker.image_uris import retrieve\n",
    "image = retrieve('xgboost', REGION, version='1.5-1')\n",
    "print(f\"   Container image: {image}\")\n",
    "\n",
    "# Datacapture URI\n",
    "data_capture_prefix = f\"{MODEL_PREFIX}datacapture\"\n",
    "s3_capture_upload_path = f\"s3://{BASE_BUCKET_NAME}/{data_capture_prefix}\"\n",
    "\n",
    "print(f\"   S3 Datacapture URI: {s3_capture_upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb892608-8915-41bb-81e3-5ddaa0fdc003",
   "metadata": {
    "id": "eb892608-8915-41bb-81e3-5ddaa0fdc003"
   },
   "source": [
    "### Section 7.2: Create SageMaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829242be-c71c-46ab-bca3-aa53331e165d",
   "metadata": {
    "id": "829242be-c71c-46ab-bca3-aa53331e165d",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "f125c7bc-1c5a-446b-e330-c0be944690a9"
   },
   "outputs": [],
   "source": [
    "#@title Section 7.2: Create SageMaker Model\n",
    "print(f\"\\n Creating SageMaker model: {model_name}\")\n",
    "\n",
    "# Define primary container\n",
    "primary_container = {\n",
    "    'Image': image,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Create model in SageMaker Model Registry\n",
    "    create_model_response = sm.create_model(\n",
    "        ModelName=model_name,\n",
    "        ExecutionRoleArn=role,\n",
    "        PrimaryContainer=primary_container\n",
    "    )\n",
    "\n",
    "    print(f\" Model created successfully!\")\n",
    "    print(f\"   Model ARN: {create_model_response['ModelArn']}\")\n",
    "    model_created = True\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if 'already exists' in error_msg.lower():\n",
    "        print(f\"  Model already exists, will use existing model\")\n",
    "        model_created = True\n",
    "    else:\n",
    "        print(f\" Error creating model: {error_msg[:200]}\")\n",
    "        model_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1184b-c3f0-4028-80c7-9332bc6cc273",
   "metadata": {
    "id": "50d1184b-c3f0-4028-80c7-9332bc6cc273"
   },
   "source": [
    "### Section 7.3: Create the custom inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac927167-006d-40d9-8272-9d52695f7ee3",
   "metadata": {
    "id": "ac927167-006d-40d9-8272-9d52695f7ee3",
    "outputId": "44400661-9c93-41ea-dbd2-19ef69c3585e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/tmp/venuesignal_model\", exist_ok=True)\n",
    "\n",
    "inference_script = '''\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from io import StringIO\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the XGBoost model from the model directory.\"\"\"\n",
    "    model_file = os.path.join(model_dir, \"xgboost-model\")\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(model_file)\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Parse input CSV into DMatrix.\"\"\"\n",
    "    if request_content_type == \"text/csv\":\n",
    "        data = np.array([\n",
    "            [float(x) for x in row.split(\",\")]\n",
    "            for row in request_body.strip().split(\"\\\\n\")\n",
    "            if row.strip()\n",
    "        ])\n",
    "        return xgb.DMatrix(data)\n",
    "    raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Run inference.\"\"\"\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(predictions, accept):\n",
    "    \"\"\"\n",
    "    Return predictions as plain text/csv — no charset suffix.\n",
    "    This prevents SageMaker data capture from BASE64-encoding\n",
    "    the output, which would break the Model Quality Monitor.\n",
    "    \"\"\"\n",
    "    result = \"\\\\n\".join(str(float(p)) for p in predictions)\n",
    "    return result, \"text/csv\"\n",
    "'''\n",
    "\n",
    "script_path = \"/tmp/venuesignal_model/inference.py\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(f\"✓ Inference script written to: {script_path}\")\n",
    "print(\"\\nScript forces output content-type to 'text/csv' (no charset)\")\n",
    "print(\"This allows CsvContentTypes to match and prevents BASE64 encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492c3bc-fc2b-4366-8041-6d33ed73351c",
   "metadata": {
    "id": "b492c3bc-fc2b-4366-8041-6d33ed73351c",
    "outputId": "078a8ca6-477a-4d8b-ad22-990de02e6a37"
   },
   "outputs": [],
   "source": [
    "#@title Section 7.3: Create Endpoint Configuration\n",
    "if model_created:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SECTION 7.3: CREATE ENDPOINT CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "    try:\n",
    "        endpoint_config_response = sm.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ProductionVariants=[\n",
    "                {\n",
    "                    'VariantName': 'venuesignal-xgboost',\n",
    "                    'ModelName': model_name,\n",
    "                    'InstanceType': INSTANCE_TYPE,\n",
    "                    'InitialInstanceCount': INSTANCE_COUNT\n",
    "                }\n",
    "            ],\n",
    "            DataCaptureConfig={\n",
    "                'EnableCapture': True,\n",
    "                'InitialSamplingPercentage': 100,\n",
    "                'DestinationS3Uri': s3_capture_upload_path,\n",
    "                'CaptureOptions': [\n",
    "                    {'CaptureMode': 'Input'},\n",
    "                    {'CaptureMode': 'Output'}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\" Endpoint configuration created!\")\n",
    "        print(f\"   Config ARN: {endpoint_config_response['EndpointConfigArn']}\")\n",
    "        endpoint_config_created = True\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\" Error creating endpoint configuration: {error_msg[:200]}\")\n",
    "\n",
    "        if 'AccessDenied' in error_msg or 'not authorized' in error_msg:\n",
    "            print(\"\\n  AWS Learning Lab IAM Restriction\")\n",
    "            print(\"   This is a known Learning Lab limitation.\")\n",
    "            print(\"   Proceeding with alternative deployment methods...\")\n",
    "\n",
    "        endpoint_config_created = False\n",
    "else:\n",
    "    endpoint_config_created = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variant-name-definition",
   "metadata": {
    "id": "variant-name-definition",
    "outputId": "4ff73206-c179-4ab8-f6d2-3dea97da93cc"
   },
   "outputs": [],
   "source": [
    "# Define variant name for monitoring\n",
    "variant_name = 'venuesignal-xgboost'\n",
    "print(f\"Variant name: {variant_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65270a28-7474-457f-b503-cee97d7c81b7",
   "metadata": {
    "id": "65270a28-7474-457f-b503-cee97d7c81b7"
   },
   "source": [
    "### Section 7.4: Deploy Model to Real-Time Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb19211-6cda-4ae8-8acb-73d6c9728f4d",
   "metadata": {
    "id": "9eb19211-6cda-4ae8-8acb-73d6c9728f4d",
    "outputId": "d760184a-4c53-4e43-a621-3e04a68f0d82"
   },
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoostModel\n",
    "\n",
    "# ── Step 1: Delete old endpoint ───────────────────────────\n",
    "print(\"Cleaning up old resources...\")\n",
    "try:\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"  ✓ Deleted endpoint: {endpoint_name}\")\n",
    "    waiter = sagemaker_client.get_waiter(\"endpoint_deleted\")\n",
    "    waiter.wait(EndpointName=endpoint_name)\n",
    "except Exception as e:\n",
    "    print(f\"  Note: {e}\")\n",
    "\n",
    "# ── Step 2: Upload inference script to S3 ─────────────────\n",
    "script_s3_uri = f\"s3://{BASE_BUCKET_NAME}/{MODEL_PREFIX}code/\"\n",
    "S3Uploader.upload(\"/tmp/venuesignal_model/inference.py\", script_s3_uri)\n",
    "print(f\"\\n✓ Inference script uploaded to: {script_s3_uri}\")\n",
    "\n",
    "# ── Step 3: Deploy with custom inference script ───────────\n",
    "print(\"\\nDeploying model with custom inference script...\")\n",
    "print(\"This takes ~5-10 minutes...\")\n",
    "\n",
    "xgb_model = XGBoostModel(\n",
    "    model_data=xgb_model_data,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"/tmp/venuesignal_model\",\n",
    "    framework_version=\"1.7-1\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "endpoint_name = (\n",
    "    f\"venuesignal-endpoint-{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
    ")\n",
    "\n",
    "predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=sagemaker.model_monitor.DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=s3_capture_upload_path,\n",
    "        capture_options=[\"Input\", \"Output\"],\n",
    "        csv_content_types=[\"text/csv\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"✓ Endpoint deployed: {endpoint_name}\")\n",
    "%store endpoint_name\n",
    "\n",
    "# ── Step 4: Send initial traffic ──────────────────────────\n",
    "print(\"\\nSending initial traffic...\")\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
    "for i in range(20):\n",
    "    payload = (\n",
    "        production_df[xgb_features]\n",
    "        .iloc[i % len(production_df)]\n",
    "        .fillna(0)\n",
    "        .to_csv(header=None, index=False)\n",
    "        .strip()\n",
    "    )\n",
    "    runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"text/csv\",\n",
    "        Body=payload,\n",
    "        InferenceId=str(i),\n",
    "    )\n",
    "    sleep(1)\n",
    "print(\"✓ 20 requests sent — endpoint ready\")\n",
    "\n",
    "endpoint_deployed = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa70e7-a3b2-4412-ab35-0011e045aaf6",
   "metadata": {
    "id": "bafa70e7-a3b2-4412-ab35-0011e045aaf6"
   },
   "source": [
    "### Section 7.5: Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2352a-0bfa-4302-869a-08b2155dfc3e",
   "metadata": {
    "id": "57f2352a-0bfa-4302-869a-08b2155dfc3e",
    "outputId": "cb17329d-a98f-4c2b-e69d-b0580ea44cca"
   },
   "outputs": [],
   "source": [
    "#@title Section 7.5: Test Endpoint\n",
    "if endpoint_deployed:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SECTION 7.5: TEST ENDPOINT\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(\"\\n Testing endpoint with sample predictions...\")\n",
    "\n",
    "    # Prepare test sample (features only, no header)\n",
    "    test_sample = test_df[xgb_features].head(10).fillna(0)\n",
    "    test_csv = test_sample.to_csv(header=None, index=False).strip('\\n').split('\\n')\n",
    "\n",
    "    try:\n",
    "        # Test with first sample\n",
    "        invoke_endpoint_response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='text/csv',\n",
    "            Body=test_csv[0]\n",
    "        )\n",
    "\n",
    "        prediction = invoke_endpoint_response['Body'].read().decode('utf-8')\n",
    "        print(f\" Endpoint responding successfully!\")\n",
    "        print(f\"   Sample prediction: {prediction}\")\n",
    "\n",
    "        # Test with multiple samples\n",
    "        print(f\"\\n Testing with {len(test_csv)} samples...\")\n",
    "\n",
    "        body = \"\"\n",
    "        for row in test_csv:\n",
    "            body += row + \"\\n\"\n",
    "\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='text/csv',\n",
    "            Body=body\n",
    "        )\n",
    "\n",
    "        predictions_str = response['Body'].read().decode('utf-8')\n",
    "        predictions = [float(val) for val in predictions_str.strip().split(\"\\n\") if val.strip()]\n",
    "\n",
    "        print(f\" Received {len(predictions)} predictions\")\n",
    "        print(f\"\\n   Sample predictions:\")\n",
    "        for i, pred in enumerate(predictions[:5]):\n",
    "            print(f\"   Sample {i+1}: {pred:.4f} (probability)\")\n",
    "\n",
    "        # Store endpoint info\n",
    "        %store endpoint_name\n",
    "\n",
    "        print(f\"\\n Endpoint deployed and tested successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error invoking endpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756249b5-175b-45ca-881e-047be7fc5196",
   "metadata": {
    "id": "756249b5-175b-45ca-881e-047be7fc5196"
   },
   "source": [
    "### Section 7.6: Full Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5ccc8-08ac-4a0d-93bc-184d5bc6af6b",
   "metadata": {
    "id": "ccb5ccc8-08ac-4a0d-93bc-184d5bc6af6b",
    "outputId": "a807a605-72d8-4232-e404-ac975377bb9e"
   },
   "outputs": [],
   "source": [
    "#@title Section 7.6: Full Test Set Evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 7.6: FULL TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load test data\n",
    "X_test_eval = test_df[xgb_features].fillna(0)\n",
    "y_test_eval = test_df['is_highly_rated']\n",
    "y_test_stars = test_df['review_stars']\n",
    "\n",
    "if endpoint_deployed:\n",
    "    print(\"\\n Generating predictions using deployed endpoint...\")\n",
    "\n",
    "    try:\n",
    "        # Prepare test data\n",
    "        test_csv_full = X_test_eval.to_csv(header=None, index=False).strip('\\n').split('\\n')\n",
    "\n",
    "        # Create request body\n",
    "        body = \"\"\n",
    "        for row in test_csv_full:\n",
    "            body += row + \"\\n\"\n",
    "\n",
    "        # Invoke endpoint\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='text/csv',\n",
    "            Body=body\n",
    "        )\n",
    "\n",
    "        # Parse predictions\n",
    "        predictions_str = response['Body'].read().decode('utf-8')\n",
    "        predictions_proba = np.array([float(val) for val in predictions_str.split().split(\"\\n\") if val.strip()])\n",
    "        predictions_class = (predictions_proba >= 0.5).astype(int)\n",
    "        predictions_stars = prob_to_stars(predictions_proba)\n",
    "\n",
    "        print(f\" Generated {len(predictions_proba):,} predictions from endpoint\")\n",
    "        prediction_source = \"SageMaker Endpoint\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Endpoint invocation failed: {e}\")\n",
    "        print(\"   Falling back to local predictions...\")\n",
    "\n",
    "        predictions_class = xgb_local.predict(X_test_eval)\n",
    "        predictions_proba = xgb_local.predict_proba(X_test_eval)[:, 1]\n",
    "        predictions_stars = prob_to_stars(predictions_proba)\n",
    "        prediction_source = \"Local Model\"\n",
    "else:\n",
    "    print(\"\\n Generating predictions using local model...\")\n",
    "\n",
    "    predictions_class = xgb_local.predict(X_test_eval)\n",
    "    predictions_proba = xgb_local.predict_proba(X_test_eval)[:, 1]\n",
    "    predictions_stars = prob_to_stars(predictions_proba)\n",
    "\n",
    "    print(f\" Generated {len(predictions_proba):,} predictions locally\")\n",
    "    prediction_source = \"Local Model\"\n",
    "\n",
    "# Evaluate\n",
    "print(f\"\\n Test Set Performance (using {prediction_source}):\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "accuracy = accuracy_score(y_test_eval, predictions_class)\n",
    "precision = precision_score(y_test_eval, predictions_class)\n",
    "recall = recall_score(y_test_eval, predictions_class)\n",
    "f1 = f1_score(y_test_eval, predictions_class)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_stars, predictions_stars))\n",
    "mae = np.mean(np.abs(y_test_stars - predictions_stars))\n",
    "within_05 = (np.abs(y_test_stars - predictions_stars) <= 0.5).mean()\n",
    "within_10 = (np.abs(y_test_stars - predictions_stars) <= 1.0).mean()\n",
    "\n",
    "print(f\"\\n   Classification Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\n   Regression Metrics:\")\n",
    "print(f\"   RMSE:      {rmse:.4f} stars\")\n",
    "print(f\"   MAE:       {mae:.4f} stars\")\n",
    "\n",
    "print(f\"\\n   Prediction Accuracy:\")\n",
    "print(f\"   Within 0.5 star: {within_05*100:.2f}%\")\n",
    "print(f\"   Within 1.0 star: {within_10*100:.2f}%\")\n",
    "\n",
    "# Save predictions\n",
    "print(f\"\\n Saving predictions...\")\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'business_id': test_df['business_id'],\n",
    "    'review_id': test_df['review_id'],\n",
    "    'actual_stars': y_test_stars,\n",
    "    'is_highly_rated': y_test_class,\n",
    "    'predicted_stars': predictions_stars,\n",
    "    'predicted_class': predictions_class,\n",
    "    'probability': predictions_proba,\n",
    "    'error': np.abs(y_test_stars - predictions_stars),\n",
    "    'within_1_star': np.abs(y_test_stars - predictions_stars) <= 1.0\n",
    "})\n",
    "\n",
    "# Save locally and to S3\n",
    "predictions_local = '/tmp/test_predictions.csv'\n",
    "predictions_df.to_csv(predictions_local, index=False)\n",
    "\n",
    "predictions_s3 = f\"s3://{MODEL_DIR}predictions/test_predictions.csv\"\n",
    "s3_client.upload_file(predictions_local, BASE_BUCKET_NAME, f'{MODEL_PREFIX}predictions/test_predictions.csv')\n",
    "\n",
    "print(f\"    Saved to: {predictions_s3}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n Sample Predictions:\")\n",
    "display(predictions_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e7262-ca75-465e-a9d4-3139e55fdc3a",
   "metadata": {
    "id": "4f1e7262-ca75-465e-a9d4-3139e55fdc3a"
   },
   "source": [
    "### Section 7.7: Create Model Package Group (Model Registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f658c9-752e-4a95-90ec-77c696c1bae9",
   "metadata": {
    "id": "16f658c9-752e-4a95-90ec-77c696c1bae9",
    "outputId": "4f5cee4f-c3e5-41e5-e148-74f325c4efb3"
   },
   "outputs": [],
   "source": [
    "model_package_group_name = f\"venuesignal-model-group-{account_id}\"\n",
    "model_description = \"VenueSignal model package group: predicts business ratings based on parking features\"\n",
    "\n",
    "print(f\"\\n Creating model package group...\")\n",
    "print(f\"   Name: {model_package_group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c816bd-5b07-45d0-b372-fd476000ac6f",
   "metadata": {
    "id": "63c816bd-5b07-45d0-b372-fd476000ac6f",
    "outputId": "a35082f8-00a7-4913-8734-cea62f8afb98"
   },
   "outputs": [],
   "source": [
    "#@title Section 7.7: Create Model Package Group (Model Registry)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 7.7: CREATE MODEL PACKAGE GROUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    model_package_group_input_dict = {\n",
    "        'ModelPackageGroupName': model_package_group_name,\n",
    "        'ModelPackageGroupDescription': model_description\n",
    "    }\n",
    "\n",
    "    create_model_package_group_response = sm.create_model_package_group(\n",
    "        **model_package_group_input_dict\n",
    "    )\n",
    "\n",
    "    print(f\" Model package group created!\")\n",
    "    print(f\"   ARN: {create_model_package_group_response['ModelPackageGroupArn']}\")\n",
    "\n",
    "    model_package_group_created = True\n",
    "\n",
    "except Exception as e:\n",
    "    if 'already exists' in str(e).lower():\n",
    "        print(f\"  Model package group already exists\")\n",
    "        model_package_group_created = True\n",
    "    else:\n",
    "        print(f\" Error: {e}\")\n",
    "        model_package_group_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314882f-6184-4d1c-8aca-df4de5662f41",
   "metadata": {
    "id": "c314882f-6184-4d1c-8aca-df4de5662f41"
   },
   "source": [
    "### Section 7.8: Register Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b4f00-3d58-485c-96fc-b3fd7101ca0e",
   "metadata": {
    "id": "0a6b4f00-3d58-485c-96fc-b3fd7101ca0e",
    "outputId": "ad6cd6c8-54f7-4b81-df01-2686ebeb3836"
   },
   "outputs": [],
   "source": [
    "#@title Section 7.8: Register Model Version\n",
    "\n",
    "if model_package_group_created:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SECTION 7.8: REGISTER MODEL VERSION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\n Registering model version to package group...\")\n",
    "\n",
    "    try:\n",
    "        # Define inference specification\n",
    "        modelpackage_inference_specification = {\n",
    "            'InferenceSpecification': {\n",
    "                'Containers': [\n",
    "                    {\n",
    "                        'Image': image,\n",
    "                        'ModelDataUrl': info['ModelArtifacts']['S3ModelArtifacts'],\n",
    "                    }\n",
    "                ],\n",
    "                'SupportedContentTypes': ['text/csv'],\n",
    "                'SupportedResponseMIMETypes': ['text/csv'],\n",
    "                'SupportedTransformInstanceTypes': ['ml.m5.xlarge'],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Create model package input\n",
    "        create_model_package_input_dict = {\n",
    "            'ModelPackageGroupName': model_package_group_name,\n",
    "            'ModelPackageDescription': model_description,\n",
    "            'ModelApprovalStatus': 'Approved'\n",
    "        }\n",
    "        create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "\n",
    "        # Register model\n",
    "        create_model_package_response = sm.create_model_package(\n",
    "            **create_model_package_input_dict\n",
    "        )\n",
    "\n",
    "        model_package_arn = create_model_package_response['ModelPackageArn']\n",
    "        print(f\" Model version registered!\")\n",
    "        print(f\"   Model Package ARN: {model_package_arn}\")\n",
    "\n",
    "        # Store for later use\n",
    "        %store model_package_group_name\n",
    "        %store model_package_arn\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error registering model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c97cf-b815-46a1-b844-8dc608ce43c0",
   "metadata": {
    "id": "7d2c97cf-b815-46a1-b844-8dc608ce43c0"
   },
   "source": [
    "### Section 7.9: Deployment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c26d23-d88e-4887-b93a-97a3662fc34c",
   "metadata": {
    "id": "b9c26d23-d88e-4887-b93a-97a3662fc34c",
    "outputId": "f92066c3-5198-4cc0-a041-6e9958e767c8"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEPLOYMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Deployment Status:\")\n",
    "print(f\"\\n1. SageMaker Model:\")\n",
    "if model_created:\n",
    "    print(f\"   Created: {model_name}\")\n",
    "    print(f\"   Location: {model_data}\")\n",
    "else:\n",
    "    print(f\"    Not created\")\n",
    "\n",
    "print(f\"\\n2. Endpoint Configuration:\")\n",
    "if endpoint_config_created:\n",
    "    print(f\"    Created: {endpoint_config_name}\")\n",
    "    print(f\"   Instance: {INSTANCE_TYPE}\")\n",
    "else:\n",
    "    print(f\"    Not created\")\n",
    "\n",
    "print(f\"\\n3. Real-Time Endpoint:\")\n",
    "if endpoint_deployed:\n",
    "    print(f\"    Deployed: {endpoint_name}\")\n",
    "    print(f\"   Status: InService\")\n",
    "    print(f\"   Tested:  Successfully\")\n",
    "else:\n",
    "    print(f\"    Not deployed\")\n",
    "    if not endpoint_config_created:\n",
    "        print(f\"   Reason: Endpoint configuration creation failed (likely Learning Lab restriction)\")\n",
    "\n",
    "print(f\"\\n4. Model Package Group:\")\n",
    "if model_package_group_created:\n",
    "    print(f\"    Created: {model_package_group_name}\")\n",
    "else:\n",
    "    print(f\"    Not created\")\n",
    "\n",
    "print(f\"\\n Test Set Performance:\")\n",
    "print(f\"   Prediction Source: {prediction_source}\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "print(f\"   RMSE:      {rmse:.4f} stars\")\n",
    "print(f\"   Within 1★: {within_10*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n Outputs:\")\n",
    "print(f\"   Predictions: {predictions_s3}\")\n",
    "print(f\"   Model artifacts: {model_data}\")\n",
    "\n",
    "if endpoint_deployed:\n",
    "    print(f\"\\n Endpoint Available:\")\n",
    "    print(f\"   Name: {endpoint_name}\")\n",
    "    print(f\"   Usage: Invoke with CSV data (features only, no header)\")\n",
    "    print(f\"   Returns: Probability scores (0-1)\")\n",
    "else:\n",
    "    print(f\"\\n  Deployment Note:\")\n",
    "    print(f\"   Real-time endpoint could not be deployed (Learning Lab restriction)\")\n",
    "    print(f\"   All predictions generated using local model\")\n",
    "    print(f\"   Performance is identical to what endpoint would provide\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SECTION 7 COMPLETE - MODEL DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deMilUhp64pD",
   "metadata": {
    "id": "deMilUhp64pD"
   },
   "source": [
    "---\n",
    "\n",
    "## 8. Monitoring & Observability <a id='section-8'></a>\n",
    "\n",
    "This section implements comprehensive production monitoring across three pillars:\n",
    "\n",
    "1. **Model Quality Monitoring** (Section 8.2): Tracks prediction accuracy and classification drift against a validated baseline\n",
    "2. **Data Quality Monitoring** (Section 8.3): Detects feature distribution shifts in incoming data\n",
    "3. **Infrastructure Monitoring** (Section 8.4): Monitors endpoint health, latency, and resource utilization\n",
    "4. **CloudWatch Dashboard** (Section 8.5): Centralized visualization of all monitoring signals\n",
    "5. **Monitoring Reports** (Section 8.6): Automated report generation and violation review\n",
    "\n",
    "All monitors are baselined, scheduled to run **hourly**, and publish metrics to CloudWatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oP-WNir464pD",
   "metadata": {
    "id": "oP-WNir464pD"
   },
   "source": [
    "### 8.1 Configure Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K4_FqtS764pD",
   "metadata": {
    "id": "K4_FqtS764pD"
   },
   "outputs": [],
   "source": [
    "# ── Imports ────────────────────────────────────────────────────────────────\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "import csv\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from threading import Thread\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from time import sleep\n",
    "\n",
    "from sagemaker import get_execution_role, image_uris, Session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.model_monitor import (\n",
    "    DefaultModelMonitor,\n",
    "    ModelQualityMonitor,\n",
    "    CronExpressionGenerator,\n",
    "    DataCaptureConfig,\n",
    "    EndpointInput,\n",
    ")\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# ── Core references (already defined in earlier sections) ─────────────────\n",
    "# Confirm key variables are available\n",
    "print(\"Confirming monitoring configuration...\")\n",
    "print(f\"  Endpoint     : {endpoint_name}\")\n",
    "print(f\"  Variant      : {variant_name}\")\n",
    "print(f\"  Bucket       : {BASE_BUCKET_NAME}\")\n",
    "print(f\"  Monitoring   : s3://{MONITORING_DIR}\")\n",
    "print(f\"  Region       : {REGION}\")\n",
    "print(f\"  Role         : {role[:50]}...\")\n",
    "\n",
    "# ── S3 paths for all monitoring artefacts ──────────────────────────────────\n",
    "monitoring_output_path      = f\"s3://{MONITORING_DIR}monitoring-output\"\n",
    "baseline_results_path       = f\"s3://{MONITORING_DIR}baseline-results\"\n",
    "monitoring_reports_path     = f\"s3://{MONITORING_DIR}reports\"\n",
    "reports_uri                 = monitoring_reports_path\n",
    "\n",
    "# Model Quality paths\n",
    "mq_baseline_data_uri    = f\"s3://{BASE_BUCKET_NAME}/{MONITORING_PREFIX}baselining/data\"\n",
    "mq_baseline_results_uri = f\"s3://{BASE_BUCKET_NAME}/{MONITORING_PREFIX}baselining/results\"\n",
    "mq_results_uri          = f\"s3://{BASE_BUCKET_NAME}/{MONITORING_PREFIX}model-quality-results\"\n",
    "ground_truth_upload_path = f\"s3://{MONITORING_DIR}ground_truth_data\"\n",
    "\n",
    "# Data Quality paths\n",
    "dq_baseline_uri  = f\"s3://{MONITORING_DIR}data-quality-baseline\"\n",
    "dq_results_uri   = f\"s3://{MONITORING_DIR}data-quality-results\"\n",
    "\n",
    "# ── Retrieve the model monitor image ──────────────────────────────────────\n",
    "monitor_image_uri = image_uris.retrieve(framework=\"model-monitor\", region=REGION)\n",
    "\n",
    "print(\"\\n✓ Monitoring paths configured\")\n",
    "print(f\"  MQ baseline data   : {mq_baseline_data_uri}\")\n",
    "print(f\"  MQ baseline results: {mq_baseline_results_uri}\")\n",
    "print(f\"  MQ results         : {mq_results_uri}\")\n",
    "print(f\"  Ground truth       : {ground_truth_upload_path}\")\n",
    "print(f\"  DQ baseline        : {dq_baseline_uri}\")\n",
    "print(f\"  DQ results         : {dq_results_uri}\")\n",
    "print(f\"  Reports            : {monitoring_reports_path}\")\n",
    "print(f\"  Monitor image      : {monitor_image_uri}\")\n",
    "\n",
    "%store monitoring_output_path\n",
    "%store monitoring_reports_path\n",
    "%store ground_truth_upload_path\n",
    "%store mq_results_uri\n",
    "%store dq_results_uri\n",
    "%store reports_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gMgQ_Wya64pD",
   "metadata": {
    "id": "gMgQ_Wya64pD"
   },
   "source": [
    "---\n",
    "\n",
    "### 8.2 Model Quality Monitoring\n",
    "\n",
    "Model Quality Monitoring continuously evaluates classification performance\n",
    "(accuracy, precision, recall, F1, AUC) of the live endpoint against a\n",
    "baseline derived from held-out validation predictions.\n",
    "\n",
    "**Process:**\n",
    "1. Upload a baseline dataset of validation predictions\n",
    "2. Run a baselining job to compute statistics and suggest constraints\n",
    "3. Stream live traffic through the endpoint (data capture)\n",
    "4. Upload synthetic ground-truth labels\n",
    "5. Schedule an hourly monitoring job that merges captured inferences with\n",
    "   ground truth and flags constraint violations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4SWxagC564pE",
   "metadata": {
    "id": "4SWxagC564pE"
   },
   "source": [
    "#### 8.2.1 Upload Baseline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MwjbqfS-64pE",
   "metadata": {
    "id": "MwjbqfS-64pE"
   },
   "outputs": [],
   "source": [
    "# predictions_local was written in Section 7.6 (test_predictions.csv)\n",
    "# It contains: review_id, probability, predicted_class, is_highly_rated\n",
    "print(f\"Uploading baseline dataset from: {predictions_local}\")\n",
    "baseline_dataset_uri = S3Uploader.upload(predictions_local, mq_baseline_data_uri)\n",
    "print(f\"✓ Baseline dataset uploaded: {baseline_dataset_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xuxTCuuU64pE",
   "metadata": {
    "id": "xuxTCuuU64pE"
   },
   "source": [
    "#### 8.2.2 Create Model Quality Monitor & Baseline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbbUoT0z64pE",
   "metadata": {
    "id": "HbbUoT0z64pE"
   },
   "outputs": [],
   "source": [
    "# Instantiate the ModelQualityMonitor\n",
    "xgboost_model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "print(\"✓ ModelQualityMonitor created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2O1M22M64pE",
   "metadata": {
    "id": "d2O1M22M64pE"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the baseline suggestion job.\n",
    "# Problem type is BinaryClassification; column names match test_predictions.csv\n",
    "mq_baseline_job_name = (\n",
    "    f\"venuesignal-mq-baseline-{account_id}-\"\n",
    "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
    ")\n",
    "print(f\"Creating model quality baseline: {mq_baseline_job_name}\")\n",
    "print(\"This will take approximately 10-15 minutes…\")\n",
    "\n",
    "xgboost_model_quality_monitor.suggest_baseline(\n",
    "    job_name=mq_baseline_job_name,\n",
    "    baseline_dataset=baseline_dataset_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=mq_baseline_results_uri,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    inference_attribute=\"predicted_class\",\n",
    "    probability_attribute=\"probability\",\n",
    "    ground_truth_attribute=\"is_highly_rated\",\n",
    "    wait=True,\n",
    "    logs=False,\n",
    ")\n",
    "print(f\"\\n✓ Baseline job complete: {mq_baseline_job_name}\")\n",
    "%store mq_baseline_job_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFn1sdJM64pE",
   "metadata": {
    "id": "oFn1sdJM64pE"
   },
   "source": [
    "#### 8.2.3 Review Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u4T_GXG-64pE",
   "metadata": {
    "id": "u4T_GXG-64pE"
   },
   "outputs": [],
   "source": [
    "mq_baseline_job = xgboost_model_quality_monitor.latest_baselining_job\n",
    "\n",
    "# ── Statistics ─────────────────────────────────────────────────────────────\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL QUALITY BASELINE — STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    binary_metrics = mq_baseline_job.baseline_statistics().body_dict[\n",
    "        \"binary_classification_metrics\"\n",
    "    ]\n",
    "    import pandas as pd\n",
    "    print(pd.json_normalize(binary_metrics).T.to_string())\n",
    "except Exception as e:\n",
    "    print(f\"Warning: could not retrieve statistics — {e}\")\n",
    "\n",
    "# ── Constraints ─────────────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL QUALITY BASELINE — SUGGESTED CONSTRAINTS\")\n",
    "print(\"=\" * 70)\n",
    "try:\n",
    "    constraints = mq_baseline_job.suggested_constraints().body_dict[\n",
    "        \"binary_classification_constraints\"\n",
    "    ]\n",
    "    print(pd.DataFrame(constraints).T.to_string())\n",
    "except Exception as e:\n",
    "    print(f\"Warning: could not retrieve constraints — {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ovdJyh64pF",
   "metadata": {
    "id": "71ovdJyh64pF"
   },
   "source": [
    "#### 8.2.4 Generate Live Traffic for Data Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943a2d1-fc49-4155-b55b-bcace220df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a background thread that continuously invokes the endpoint so that\n",
    "# SageMaker captures input/output pairs.  The thread runs indefinitely;\n",
    "# restart the kernel to stop it.\n",
    "\n",
    "def _invoke_endpoint_loop(ep_name, feature_df):\n",
    "    runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
    "    ids = list(range(500))  # must match ground truth IDs exactly (0-499)\n",
    "    while True:\n",
    "        try:\n",
    "            for i in ids:\n",
    "                row = feature_df.iloc[i % len(feature_df)]\n",
    "                buf = io.StringIO()\n",
    "                csv.writer(buf).writerow(row.fillna(0).tolist())\n",
    "                runtime.invoke_endpoint(\n",
    "                    EndpointName=ep_name,\n",
    "                    ContentType=\"text/csv\",\n",
    "                    Body=buf.getvalue().strip(),\n",
    "                    InferenceId=str(i),  # 0-499 matches ground truth\n",
    "                )\n",
    "                sleep(2)\n",
    "        except Exception:\n",
    "            pass  # retry on transient errors\n",
    "\n",
    "traffic_thread = Thread(\n",
    "    target=_invoke_endpoint_loop,\n",
    "    args=(endpoint_name, production_df[xgb_features]),\n",
    "    daemon=True,\n",
    ")\n",
    "traffic_thread.start()\n",
    "print(\"✓ Endpoint traffic thread started (daemon — stops with kernel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VTdTrrKv64pF",
   "metadata": {
    "id": "VTdTrrKv64pF"
   },
   "source": [
    "#### 8.2.5 Verify Captured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CkIMrtiS64pF",
   "metadata": {
    "id": "CkIMrtiS64pF"
   },
   "outputs": [],
   "source": [
    "print(\"Waiting for capture files to appear in S3…\", end=\"\")\n",
    "capture_files = []\n",
    "for _ in range(120):\n",
    "    capture_files = sorted(\n",
    "        S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\")\n",
    "    )\n",
    "    if capture_files:\n",
    "        sample_lines = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")\n",
    "        sample_record = json.loads(sample_lines[0])\n",
    "        if \"inferenceId\" in sample_record.get(\"eventMetadata\", {}):\n",
    "            break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(1)\n",
    "print()\n",
    "\n",
    "if capture_files:\n",
    "    print(f\"✓ Found {len(capture_files)} capture file(s)\")\n",
    "    print(\"\\nLatest capture files:\")\n",
    "    for f in capture_files[-3:]:\n",
    "        print(f\"  {f}\")\n",
    "    print(\"\\nSample capture record (first event):\")\n",
    "    print(json.dumps(sample_record, indent=2)[:800])\n",
    "else:\n",
    "    print(\"⚠ No capture files found yet — ensure data capture is enabled on the endpoint\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gEWsiNYg64pF",
   "metadata": {
    "id": "gEWsiNYg64pF"
   },
   "source": [
    "#### 8.2.6 Generate Synthetic Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mWWihcG64pF",
   "metadata": {
    "id": "4mWWihcG64pF"
   },
   "outputs": [],
   "source": [
    "# In production, ground truth arrives asynchronously (actual outcomes).\n",
    "# Here we simulate it with random labels (70 % positive) for demonstration.\n",
    "\n",
    "def _ground_truth_record(inference_id):\n",
    "    random.seed(inference_id)\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"1\" if random.random() < 0.7 else \"0\",\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\"eventId\": str(inference_id)},\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "def _upload_ground_truth(records, upload_time):\n",
    "    body = \"\\n\".join(json.dumps(r) for r in records)\n",
    "    target = f\"{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    S3Uploader.upload_string_as_file_body(body, target)\n",
    "    print(f\"  Uploaded {len(records)} ground-truth records → {target}\")\n",
    "\n",
    "def _ground_truth_loop(n=500):\n",
    "    while True:\n",
    "        records = [_ground_truth_record(i) for i in range(n)]\n",
    "        _upload_ground_truth(records, datetime.utcnow())\n",
    "        sleep(3600)  # re-upload once per hour\n",
    "\n",
    "gt_thread = Thread(target=_ground_truth_loop, daemon=True)\n",
    "gt_thread.start()\n",
    "\n",
    "# Upload one batch immediately so the first monitoring job has data\n",
    "_upload_ground_truth([_ground_truth_record(i) for i in range(500)], datetime.utcnow())\n",
    "print(\"\\n✓ Ground-truth thread started; initial batch uploaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OmE9jpiz64pG",
   "metadata": {
    "id": "OmE9jpiz64pG"
   },
   "source": [
    "#### 8.2.7 Create Hourly Model Quality Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141d595-2144-4a47-9a09-b4d574e7af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any previously failing schedule first\n",
    "try:\n",
    "    sagemaker_client.delete_monitoring_schedule(\n",
    "        MonitoringScheduleName=mq_schedule_name\n",
    "    )\n",
    "    print(f\"✓ Deleted old schedule: {mq_schedule_name}\")\n",
    "    time.sleep(15)\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "# EndpointInput — single probability output column\n",
    "mq_endpoint_input = EndpointInput(\n",
    "    endpoint_name=endpoint_name,\n",
    "    probability_attribute=\"_c0\",\n",
    "    probability_threshold_attribute=0.5,\n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")\n",
    "\n",
    "mq_schedule_name = (\n",
    "    f\"venuesignal-mq-schedule-{account_id}-\"\n",
    "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
    ")\n",
    "print(f\"Creating model quality schedule: {mq_schedule_name}\")\n",
    "\n",
    "try:\n",
    "    xgboost_model_quality_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=mq_schedule_name,\n",
    "        endpoint_input=mq_endpoint_input,\n",
    "        output_s3_uri=mq_results_uri,\n",
    "        problem_type=\"BinaryClassification\",\n",
    "        ground_truth_input=ground_truth_upload_path,\n",
    "        constraints=mq_baseline_job.suggested_constraints(),\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"✓ Model quality schedule created: {mq_schedule_name}\")\n",
    "    print(f\"  Frequency : Hourly\")\n",
    "    print(f\"  Results   : {mq_results_uri}\")\n",
    "    %store mq_schedule_name\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nzSHPjdI64pG",
   "metadata": {
    "id": "nzSHPjdI64pG"
   },
   "source": [
    "#### 8.2.8 Verify Model Quality Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "szeujJ9R64pG",
   "metadata": {
    "id": "szeujJ9R64pG"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    desc = sagemaker_client.describe_monitoring_schedule(\n",
    "        MonitoringScheduleName=mq_schedule_name\n",
    "    )\n",
    "    status = desc[\"MonitoringScheduleStatus\"]\n",
    "    print(f\"Schedule : {mq_schedule_name}\")\n",
    "    print(f\"Status   : {status}\")\n",
    "    if status == \"Scheduled\":\n",
    "        print(\"✓ Model quality schedule is active and running hourly\")\n",
    "    elif \"FailureReason\" in desc:\n",
    "        print(f\"✗ Failure: {desc['FailureReason']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Could not describe schedule: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AHgPwEZr64pG",
   "metadata": {
    "id": "AHgPwEZr64pG"
   },
   "source": [
    "---\n",
    "\n",
    "### 8.3 Data Quality Monitoring\n",
    "\n",
    "Data Quality Monitoring detects feature-level drift between the baseline\n",
    "training distribution and incoming inference data.  It monitors statistics\n",
    "(mean, standard deviation, missing-value rates, etc.) for every feature\n",
    "and raises violations when values exceed the baselined thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dUPvgvmf64pG",
   "metadata": {
    "id": "dUPvgvmf64pG"
   },
   "source": [
    "#### 8.3.1 Verify Data Capture is Enabled on the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gnsaLJMu64pG",
   "metadata": {
    "id": "gnsaLJMu64pG"
   },
   "outputs": [],
   "source": [
    "# Data capture must be enabled on the endpoint config; it was configured\n",
    "# in Section 7.3.  This cell verifies the configuration.\n",
    "try:\n",
    "    ep_desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    config_name = ep_desc[\"EndpointConfigName\"]\n",
    "    config_desc = sagemaker_client.describe_endpoint_config(\n",
    "        EndpointConfigName=config_name\n",
    "    )\n",
    "    if \"DataCaptureConfig\" in config_desc:\n",
    "        dc = config_desc[\"DataCaptureConfig\"]\n",
    "        print(f\"✓ Data capture is enabled on endpoint: {endpoint_name}\")\n",
    "        print(f\"  Capture destination : {dc.get('DestinationS3Uri', 'N/A')}\")\n",
    "        print(f\"  Capture percentage  : {dc.get('InitialSamplingPercentage', 100)}%\")\n",
    "        data_capture_enabled = True\n",
    "    else:\n",
    "        print(\"⚠ Data capture is NOT enabled on this endpoint configuration.\")\n",
    "        print(\"  Ensure Section 7.3 was executed with DataCaptureConfig.\")\n",
    "        data_capture_enabled = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking endpoint config: {e}\")\n",
    "    data_capture_enabled = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JXTQEW6e64pH",
   "metadata": {
    "id": "JXTQEW6e64pH"
   },
   "source": [
    "#### 8.3.2 Configure Data Quality Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2DZ6e2G64pH",
   "metadata": {
    "id": "j2DZ6e2G64pH"
   },
   "outputs": [],
   "source": [
    "# training_data_uri was defined in Section 6 and stored via %store\n",
    "# It points to the training CSV used to fit the XGBoost model.\n",
    "print(f\"Training data URI : {train_data_path}\")\n",
    "print(f\"DQ baseline URI   : {dq_baseline_uri}\")\n",
    "print(f\"DQ results URI    : {dq_results_uri}\")\n",
    "\n",
    "# Verify the training CSV is accessible\n",
    "try:\n",
    "    bucket = train_data_path.split('/')[2]\n",
    "    key    = '/'.join(train_data_path.split('/')[3:])\n",
    "    s3_client.head_object(Bucket=bucket, Key=key)\n",
    "    print(\"✓ Training data verified in S3\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Training data not found — {e}\")\n",
    "    print(\"  Update train_data_path if the file has moved.\")\n",
    "\n",
    "%store dq_baseline_uri\n",
    "%store dq_results_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rmeV1k9_64pH",
   "metadata": {
    "id": "rmeV1k9_64pH"
   },
   "source": [
    "#### 8.3.3 Create Data Quality Monitor & Baseline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u-yYjDW464pH",
   "metadata": {
    "id": "u-yYjDW464pH"
   },
   "outputs": [],
   "source": [
    "data_quality_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session,  # use the module-level session object\n",
    ")\n",
    "print(\"✓ DefaultModelMonitor created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bSkSy-4X64pH",
   "metadata": {
    "id": "bSkSy-4X64pH"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dq_baseline_job_name = (\n",
    "    f\"venuesignal-dq-baseline-{account_id}-\"\n",
    "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
    ")\n",
    "print(f\"Creating data quality baseline: {dq_baseline_job_name}\")\n",
    "print(\"This will take approximately 15-20 minutes…\")\n",
    "\n",
    "data_quality_monitor.suggest_baseline(\n",
    "    job_name=dq_baseline_job_name,\n",
    "    baseline_dataset=train_data_path,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=dq_baseline_uri,\n",
    "    wait=True,\n",
    "    logs=False,\n",
    ")\n",
    "print(f\"\\n✓ Data quality baseline job complete: {dq_baseline_job_name}\")\n",
    "%store dq_baseline_job_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LXAK_RXc64pH",
   "metadata": {
    "id": "LXAK_RXc64pH"
   },
   "source": [
    "#### 8.3.4 Review Data Quality Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3LY1h36G64pH",
   "metadata": {
    "id": "3LY1h36G64pH"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY BASELINE REVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dq_baseline_job = data_quality_monitor.latest_baselining_job\n",
    "if dq_baseline_job is None:\n",
    "    print(\"✗ No baseline job found — run Section 8.3.3 first\")\n",
    "else:\n",
    "    print(f\"✓ Baseline job : {dq_baseline_job.job_name}\")\n",
    "\n",
    "    stats_file       = f\"{dq_baseline_uri}/statistics.json\"\n",
    "    constraints_file = f\"{dq_baseline_uri}/constraints.json\"\n",
    "\n",
    "    try:\n",
    "        local_stats = S3Downloader.download(stats_file, \"/tmp/dq_stats\")\n",
    "        with open(local_stats[0]) as fh:\n",
    "            dq_stats = json.load(fh)\n",
    "\n",
    "        def _fmt(val, decimals=4):\n",
    "            \"\"\"Format a numeric value; return 'N/A' if missing or non-numeric.\"\"\"\n",
    "            try:\n",
    "                return f\"{float(val):.{decimals}f}\"\n",
    "            except (TypeError, ValueError):\n",
    "                return \"N/A\"\n",
    "\n",
    "        features = dq_stats.get(\"features\", [])\n",
    "        print(f\"\\nTotal features analysed: {len(features)}\")\n",
    "        print(\"\\nFirst 8 features:\")\n",
    "        for feat in features[:8]:\n",
    "            name  = feat[\"name\"]\n",
    "            ftype = feat.get(\"inferred_type\", \"unknown\")\n",
    "            if \"numerical_statistics\" in feat:\n",
    "                ns = feat[\"numerical_statistics\"]\n",
    "                print(\n",
    "                    f\"  {name:<35} [{ftype}]  \"\n",
    "                    f\"mean={_fmt(ns.get('mean'))}  \"\n",
    "                    f\"std={_fmt(ns.get('stdDev'))}\"\n",
    "                )\n",
    "            else:\n",
    "                ss = feat.get(\"string_statistics\", {})\n",
    "                print(\n",
    "                    f\"  {name:<35} [{ftype}]  \"\n",
    "                    f\"distinct={ss.get('distinct_count', 'N/A')}\"\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not download statistics file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GE-Ojr2R64pI",
   "metadata": {
    "id": "GE-Ojr2R64pI"
   },
   "source": [
    "#### 8.3.5 Create Hourly Data Quality Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_0THtd2764pI",
   "metadata": {
    "id": "_0THtd2764pI"
   },
   "outputs": [],
   "source": [
    "dq_schedule_prefix = \"venuesignal\"\n",
    "dq_schedule_name = (\n",
    "    f\"{dq_schedule_prefix}-dq-schedule-\"\n",
    "    f\"{datetime.now(timezone.utc):%Y%m%d-%H%M%S}\"\n",
    ")\n",
    "print(f\"Creating data quality schedule: {dq_schedule_name}\")\n",
    "\n",
    "try:\n",
    "    data_quality_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=dq_schedule_name,\n",
    "        endpoint_input=endpoint_name,\n",
    "        output_s3_uri=dq_results_uri,\n",
    "        statistics=data_quality_monitor.baseline_statistics(),\n",
    "        constraints=data_quality_monitor.suggested_constraints(),\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"✓ Data quality schedule created: {dq_schedule_name}\")\n",
    "    print(f\"  Frequency : Hourly (top of the hour)\")\n",
    "    print(f\"  Results   : {dq_results_uri}\")\n",
    "    %store dq_schedule_name\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating data quality schedule: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_31uSGI664pI",
   "metadata": {
    "id": "_31uSGI664pI"
   },
   "source": [
    "#### 8.3.6 Verify Data Quality Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JXWc3pbt64pI",
   "metadata": {
    "id": "JXWc3pbt64pI"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    desc = sagemaker_client.describe_monitoring_schedule(\n",
    "        MonitoringScheduleName=dq_schedule_name\n",
    "    )\n",
    "    status = desc[\"MonitoringScheduleStatus\"]\n",
    "    print(f\"Schedule : {dq_schedule_name}\")\n",
    "    print(f\"Status   : {status}\")\n",
    "    if status == \"Scheduled\":\n",
    "        print(\"✓ Data quality schedule is active and running hourly\")\n",
    "    elif \"FailureReason\" in desc:\n",
    "        print(f\"✗ Failure: {desc['FailureReason']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Could not describe schedule: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eQCPgLHl64pI",
   "metadata": {
    "id": "eQCPgLHl64pI"
   },
   "source": [
    "---\n",
    "\n",
    "### 8.4 Infrastructure Monitoring\n",
    "\n",
    "Infrastructure monitoring uses CloudWatch to track endpoint-level hardware\n",
    "and performance metrics (CPU, memory, disk, latency, error rates) and runs\n",
    "integration tests to validate end-to-end system health.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4qlAvSyl64pI",
   "metadata": {
    "id": "4qlAvSyl64pI"
   },
   "source": [
    "#### 8.4.1 Query SageMaker Endpoint Metrics from CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45IKhmds64pI",
   "metadata": {
    "id": "45IKhmds64pI"
   },
   "outputs": [],
   "source": [
    "def get_cw_metric(metric_name, namespace=\"AWS/SageMaker\", statistic=\"Average\", hours=1):\n",
    "    \"\"\"Return the most recent datapoint for an endpoint metric.\"\"\"\n",
    "    cw  = cloudwatch_client\n",
    "    end = datetime.now(timezone.utc)\n",
    "    start = end - timedelta(hours=hours)\n",
    "    dims = [\n",
    "        {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "        {\"Name\": \"VariantName\",  \"Value\": variant_name},\n",
    "    ]\n",
    "    try:\n",
    "        resp = cw.get_metric_statistics(\n",
    "            Namespace=namespace,\n",
    "            MetricName=metric_name,\n",
    "            Dimensions=dims,\n",
    "            StartTime=start,\n",
    "            EndTime=end,\n",
    "            Period=300,\n",
    "            Statistics=[statistic],\n",
    "        )\n",
    "        pts = resp.get(\"Datapoints\", [])\n",
    "        if pts:\n",
    "            return sorted(pts, key=lambda x: x[\"Timestamp\"])[-1][statistic]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print(\"Querying endpoint metrics for the last hour…\")\n",
    "print(\"=\" * 60)\n",
    "latency      = get_cw_metric(\"ModelLatency\")\n",
    "invocations  = get_cw_metric(\"Invocations\",          statistic=\"Sum\")\n",
    "errors_4xx   = get_cw_metric(\"Invocation4XXErrors\",   statistic=\"Sum\")\n",
    "errors_5xx   = get_cw_metric(\"Invocation5XXErrors\",   statistic=\"Sum\")\n",
    "overhead     = get_cw_metric(\"OverheadLatency\")\n",
    "\n",
    "cpu    = get_cw_metric(\"CPUUtilization\",    namespace=\"/aws/sagemaker/Endpoints\")\n",
    "memory = get_cw_metric(\"MemoryUtilization\", namespace=\"/aws/sagemaker/Endpoints\")\n",
    "disk   = get_cw_metric(\"DiskUtilization\",   namespace=\"/aws/sagemaker/Endpoints\")\n",
    "\n",
    "print(f\"  Avg Model Latency   : {latency/1000:.2f} ms\"  if latency     else \"  Avg Model Latency   : N/A (no data yet)\")\n",
    "print(f\"  Total Invocations   : {invocations:.0f}\"      if invocations else \"  Total Invocations   : N/A\")\n",
    "print(f\"  4XX Errors          : {errors_4xx:.0f}\"       if errors_4xx  else \"  4XX Errors          : N/A\")\n",
    "print(f\"  5XX Errors          : {errors_5xx:.0f}\"       if errors_5xx  else \"  5XX Errors          : N/A\")\n",
    "print(f\"  Overhead Latency    : {overhead/1000:.2f} ms\" if overhead    else \"  Overhead Latency    : N/A\")\n",
    "print(f\"  CPU Utilization     : {cpu:.2f}%\"             if cpu         else \"  CPU Utilization     : N/A\")\n",
    "print(f\"  Memory Utilization  : {memory:.2f}%\"          if memory      else \"  Memory Utilization  : N/A\")\n",
    "print(f\"  Disk Utilization    : {disk:.2f}%\"            if disk        else \"  Disk Utilization    : N/A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fn_qk1ii64pJ",
   "metadata": {
    "id": "fn_qk1ii64pJ"
   },
   "source": [
    "#### 8.4.2 Endpoint Health Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7yed0gP_64pJ",
   "metadata": {
    "id": "7yed0gP_64pJ"
   },
   "outputs": [],
   "source": [
    "def endpoint_health_check(ep_name, payload):\n",
    "    \"\"\"Single health check; returns latency and HTTP status.\"\"\"\n",
    "    runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        resp = runtime.invoke_endpoint(\n",
    "            EndpointName=ep_name,\n",
    "            ContentType=\"text/csv\",\n",
    "            Body=payload,\n",
    "            InferenceId=str(uuid.uuid4()),\n",
    "        )\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"latency_ms\": (time.time() - t0) * 1000,\n",
    "            \"http_status\": resp[\"ResponseMetadata\"][\"HTTPStatusCode\"],\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        }\n",
    "    except Exception as exc:\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(exc),\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        }\n",
    "\n",
    "def run_health_check_batch(ep_name, payload, n=10):\n",
    "    \"\"\"Run n health checks and return a summary dict.\"\"\"\n",
    "    results = []\n",
    "    print(f\"Running {n} health checks on {ep_name}…\")\n",
    "    for i in range(n):\n",
    "        results.append(endpoint_health_check(ep_name, payload))\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Completed {i+1}/{n}\")\n",
    "        sleep(1)\n",
    "\n",
    "    healthy = [r for r in results if r[\"status\"] == \"healthy\"]\n",
    "    success_rate = len(healthy) / len(results)\n",
    "    avg_latency  = np.mean([r[\"latency_ms\"] for r in healthy]) if healthy else None\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✓ Health Check Summary\")\n",
    "    print(f\"  Success rate : {success_rate:.1%}\")\n",
    "    print(f\"  Avg latency  : {avg_latency:.2f} ms\" if avg_latency else \"  Avg latency  : N/A\")\n",
    "    print(f\"  Failures     : {len(results) - len(healthy)}\")\n",
    "    print(\"=\" * 60)\n",
    "    return {\n",
    "        \"total\": len(results),\n",
    "        \"successful\": len(healthy),\n",
    "        \"failed\": len(results) - len(healthy),\n",
    "        \"success_rate\": success_rate,\n",
    "        \"avg_latency_ms\": avg_latency,\n",
    "    }\n",
    "\n",
    "# Use a single production row as health-check payload\n",
    "hc_payload = production_df[xgb_features].iloc[0].fillna(0).to_csv(header=None, index=False).strip()\n",
    "health_summary = run_health_check_batch(endpoint_name, hc_payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QRPTneYb64pJ",
   "metadata": {
    "id": "QRPTneYb64pJ"
   },
   "source": [
    "#### 8.4.3 Integration Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WmVssAkD64pJ",
   "metadata": {
    "id": "WmVssAkD64pJ"
   },
   "outputs": [],
   "source": [
    "INTEGRATION_TEST_CASES = [\n",
    "    {\n",
    "        \"name\": \"High-rated business with ample parking\",\n",
    "        \"input\": production_df[xgb_features].iloc[0].fillna(0)\n",
    "                   .to_csv(header=None, index=False).strip(),\n",
    "        \"expected_range\": (0.5, 1.0),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Low parking availability\",\n",
    "        \"input\": production_df[xgb_features].iloc[50].fillna(0)\n",
    "                   .to_csv(header=None, index=False).strip(),\n",
    "        \"expected_range\": (0.0, 1.0),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Average business profile\",\n",
    "        \"input\": production_df[xgb_features].iloc[100].fillna(0)\n",
    "                   .to_csv(header=None, index=False).strip(),\n",
    "        \"expected_range\": (0.0, 1.0),\n",
    "    },\n",
    "]\n",
    "\n",
    "def run_integration_tests(ep_name, test_cases):\n",
    "    \"\"\"Run integration tests and return pass-rate summary.\"\"\"\n",
    "    runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n",
    "    results = []\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Running integration tests…\")\n",
    "    print(\"=\" * 60)\n",
    "    for tc in test_cases:\n",
    "        try:\n",
    "            resp = runtime.invoke_endpoint(\n",
    "                EndpointName=ep_name,\n",
    "                ContentType=\"text/csv\",\n",
    "                Body=tc[\"input\"],\n",
    "            )\n",
    "            # XGBoost may return multiple newline-separated scores;\n",
    "            # take the first value only\n",
    "            raw = resp[\"Body\"].read().decode().strip()\n",
    "            probability = float(raw.split(\"\\n\")[0].strip())\n",
    "            lo, hi = tc[\"expected_range\"]\n",
    "            passed = lo <= probability <= hi\n",
    "            print(f\"{'✓ PASS' if passed else '✗ FAIL'}  {tc['name']}\")\n",
    "            print(f\"       probability={probability:.4f}  expected=[{lo}, {hi}]\")\n",
    "            results.append({\"name\": tc[\"name\"], \"passed\": passed, \"value\": probability})\n",
    "        except Exception as exc:\n",
    "            print(f\"✗ ERROR  {tc['name']}: {exc}\")\n",
    "            results.append({\"name\": tc[\"name\"], \"passed\": False, \"error\": str(exc)})\n",
    "\n",
    "    passed_n = sum(1 for r in results if r[\"passed\"])\n",
    "    rate = passed_n / len(results) if results else 0\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Integration results: {passed_n}/{len(results)} passed ({rate:.1%})\")\n",
    "    print(\"=\" * 60)\n",
    "    return {\"pass_rate\": rate, \"passed\": passed_n,\n",
    "            \"failed\": len(results) - passed_n, \"total\": len(results), \"results\": results}\n",
    "\n",
    "test_results = run_integration_tests(endpoint_name, INTEGRATION_TEST_CASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aV0QnVTc64pJ",
   "metadata": {
    "id": "aV0QnVTc64pJ"
   },
   "source": [
    "#### 8.4.4 Integration Quality Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l_4bnuzV64pJ",
   "metadata": {
    "id": "l_4bnuzV64pJ"
   },
   "outputs": [],
   "source": [
    "INTEGRATION_THRESHOLDS = {\n",
    "    \"min_pass_rate\":      0.95,\n",
    "    \"max_latency_ms\":    1000,\n",
    "    \"min_availability\":   0.99,\n",
    "}\n",
    "\n",
    "violations = []\n",
    "if test_results[\"pass_rate\"] < INTEGRATION_THRESHOLDS[\"min_pass_rate\"]:\n",
    "    violations.append(\n",
    "        f\"Integration pass rate {test_results['pass_rate']:.1%} \"\n",
    "        f\"< threshold {INTEGRATION_THRESHOLDS['min_pass_rate']:.1%}\"\n",
    "    )\n",
    "if health_summary[\"success_rate\"] < INTEGRATION_THRESHOLDS[\"min_availability\"]:\n",
    "    violations.append(\n",
    "        f\"Endpoint availability {health_summary['success_rate']:.1%} \"\n",
    "        f\"< threshold {INTEGRATION_THRESHOLDS['min_availability']:.1%}\"\n",
    "    )\n",
    "if health_summary[\"avg_latency_ms\"] and health_summary[\"avg_latency_ms\"] > INTEGRATION_THRESHOLDS[\"max_latency_ms\"]:\n",
    "    violations.append(\n",
    "        f\"Avg latency {health_summary['avg_latency_ms']:.2f} ms \"\n",
    "        f\"> threshold {INTEGRATION_THRESHOLDS['max_latency_ms']} ms\"\n",
    "    )\n",
    "\n",
    "gate_passed = len(violations) == 0\n",
    "print(\"=\" * 60)\n",
    "if gate_passed:\n",
    "    print(\"✓ INTEGRATION QUALITY GATE: PASSED\")\n",
    "else:\n",
    "    print(\"✗ INTEGRATION QUALITY GATE: FAILED\")\n",
    "    for i, v in enumerate(violations, 1):\n",
    "        print(f\"  {i}. {v}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lnmGe6Zd64pK",
   "metadata": {
    "id": "lnmGe6Zd64pK"
   },
   "source": [
    "#### 8.4.5 Publish Custom Integration Metrics to CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SqbYqJPK64pK",
   "metadata": {
    "id": "SqbYqJPK64pK"
   },
   "outputs": [],
   "source": [
    "metric_data = [\n",
    "    {\n",
    "        \"MetricName\": \"IntegrationTestPassRate\",\n",
    "        \"Value\":      test_results[\"pass_rate\"] * 100,\n",
    "        \"Unit\":       \"Percent\",\n",
    "        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n",
    "    },\n",
    "    {\n",
    "        \"MetricName\": \"HealthCheckSuccessRate\",\n",
    "        \"Value\":      health_summary[\"success_rate\"] * 100,\n",
    "        \"Unit\":       \"Percent\",\n",
    "        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n",
    "    },\n",
    "]\n",
    "if health_summary.get(\"avg_latency_ms\"):\n",
    "    metric_data.append({\n",
    "        \"MetricName\": \"HealthCheckLatencyMs\",\n",
    "        \"Value\":      health_summary[\"avg_latency_ms\"],\n",
    "        \"Unit\":       \"Milliseconds\",\n",
    "        \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n",
    "    })\n",
    "\n",
    "try:\n",
    "    cloudwatch_client.put_metric_data(\n",
    "        Namespace=\"VenueSignal/Integration\",\n",
    "        MetricData=metric_data,\n",
    "    )\n",
    "    print(\"✓ Custom metrics published to CloudWatch (VenueSignal/Integration)\")\n",
    "    for m in metric_data:\n",
    "        print(f\"  {m['MetricName']}: {m['Value']:.2f} {m['Unit']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to publish metrics: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1NgSJSOu64pK",
   "metadata": {
    "id": "1NgSJSOu64pK"
   },
   "source": [
    "#### 8.4.6 Create CloudWatch Alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fi_zpder64pK",
   "metadata": {
    "id": "Fi_zpder64pK"
   },
   "outputs": [],
   "source": [
    "def create_alarm(alarm_name, metric_name, namespace, threshold,\n",
    "                 comparison, statistic, period, evaluation_periods,\n",
    "                 alarm_desc, dimensions, treat_missing=\"notBreaching\"):\n",
    "    \"\"\"Helper: create or update a CloudWatch alarm.\"\"\"\n",
    "    try:\n",
    "        cloudwatch_client.put_metric_alarm(\n",
    "            AlarmName=alarm_name,\n",
    "            AlarmDescription=alarm_desc,\n",
    "            ActionsEnabled=True,\n",
    "            MetricName=metric_name,\n",
    "            Namespace=namespace,\n",
    "            Statistic=statistic,\n",
    "            Dimensions=dimensions,\n",
    "            Period=period,\n",
    "            EvaluationPeriods=evaluation_periods,\n",
    "            DatapointsToAlarm=evaluation_periods,\n",
    "            Threshold=threshold,\n",
    "            ComparisonOperator=comparison,\n",
    "            TreatMissingData=treat_missing,\n",
    "        )\n",
    "        print(f\"✓ Alarm created/updated: {alarm_name}\")\n",
    "        return alarm_name\n",
    "    except Exception as exc:\n",
    "        print(f\"✗ Failed to create alarm {alarm_name}: {exc}\")\n",
    "        return None\n",
    "\n",
    "ep_dims = [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}]\n",
    "ep_variant_dims = ep_dims + [{\"Name\": \"VariantName\", \"Value\": variant_name}]\n",
    "\n",
    "created_alarms = []\n",
    "\n",
    "# 1. High 4XX error rate\n",
    "created_alarms.append(create_alarm(\n",
    "    alarm_name=f\"{endpoint_name}-High4XXErrors\",\n",
    "    metric_name=\"Invocation4XXErrors\",\n",
    "    namespace=\"AWS/SageMaker\",\n",
    "    threshold=10,\n",
    "    comparison=\"GreaterThanThreshold\",\n",
    "    statistic=\"Sum\",\n",
    "    period=300,\n",
    "    evaluation_periods=2,\n",
    "    alarm_desc=\"Alert: more than 10 client errors in 5 minutes\",\n",
    "    dimensions=ep_variant_dims,\n",
    "))\n",
    "\n",
    "# 2. High 5XX error rate\n",
    "created_alarms.append(create_alarm(\n",
    "    alarm_name=f\"{endpoint_name}-High5XXErrors\",\n",
    "    metric_name=\"Invocation5XXErrors\",\n",
    "    namespace=\"AWS/SageMaker\",\n",
    "    threshold=5,\n",
    "    comparison=\"GreaterThanThreshold\",\n",
    "    statistic=\"Sum\",\n",
    "    period=300,\n",
    "    evaluation_periods=2,\n",
    "    alarm_desc=\"Alert: more than 5 server errors in 5 minutes\",\n",
    "    dimensions=ep_variant_dims,\n",
    "))\n",
    "\n",
    "# 3. High model latency\n",
    "created_alarms.append(create_alarm(\n",
    "    alarm_name=f\"{endpoint_name}-HighModelLatency\",\n",
    "    metric_name=\"ModelLatency\",\n",
    "    namespace=\"AWS/SageMaker\",\n",
    "    threshold=1_000_000,   # microseconds (SageMaker reports in µs)\n",
    "    comparison=\"GreaterThanThreshold\",\n",
    "    statistic=\"Average\",\n",
    "    period=300,\n",
    "    evaluation_periods=2,\n",
    "    alarm_desc=\"Alert: average model latency exceeds 1 second\",\n",
    "    dimensions=ep_variant_dims,\n",
    "))\n",
    "\n",
    "# 4. High CPU utilisation\n",
    "created_alarms.append(create_alarm(\n",
    "    alarm_name=f\"{endpoint_name}-HighCPU\",\n",
    "    metric_name=\"CPUUtilization\",\n",
    "    namespace=\"/aws/sagemaker/Endpoints\",\n",
    "    threshold=80,\n",
    "    comparison=\"GreaterThanThreshold\",\n",
    "    statistic=\"Average\",\n",
    "    period=300,\n",
    "    evaluation_periods=3,\n",
    "    alarm_desc=\"Alert: endpoint CPU utilisation above 80 %\",\n",
    "    dimensions=ep_dims,\n",
    "))\n",
    "\n",
    "# 5. High memory utilisation\n",
    "created_alarms.append(create_alarm(\n",
    "    alarm_name=f\"{endpoint_name}-HighMemory\",\n",
    "    metric_name=\"MemoryUtilization\",\n",
    "    namespace=\"/aws/sagemaker/Endpoints\",\n",
    "    threshold=85,\n",
    "    comparison=\"GreaterThanThreshold\",\n",
    "    statistic=\"Average\",\n",
    "    period=300,\n",
    "    evaluation_periods=3,\n",
    "    alarm_desc=\"Alert: endpoint memory utilisation above 85 %\",\n",
    "    dimensions=ep_dims,\n",
    "))\n",
    "\n",
    "# 6. Integration test pass-rate drop (custom metric)\n",
    "created_alarms.append(create_alarm(\n",
    "    alarm_name=f\"{endpoint_name}-IntegrationTestFailure\",\n",
    "    metric_name=\"IntegrationTestPassRate\",\n",
    "    namespace=\"VenueSignal/Integration\",\n",
    "    threshold=95,\n",
    "    comparison=\"LessThanThreshold\",\n",
    "    statistic=\"Average\",\n",
    "    period=600,\n",
    "    evaluation_periods=1,\n",
    "    alarm_desc=\"Alert: integration test pass rate below 95 %\",\n",
    "    dimensions=ep_dims,\n",
    "    treat_missing=\"breaching\",\n",
    "))\n",
    "\n",
    "# 7. Model quality: recall drift (populated once MQ schedule runs)\n",
    "mq_alarm_dims = [\n",
    "    {\"Name\": \"Endpoint\",           \"Value\": endpoint_name},\n",
    "    {\"Name\": \"MonitoringSchedule\", \"Value\": mq_schedule_name},\n",
    "]\n",
    "created_alarms.append(create_alarm(\n",
    "    alarm_name=\"VenueSignal-ModelQuality-RecallDrift\",\n",
    "    metric_name=\"recall\",\n",
    "    namespace=\"aws/sagemaker/Endpoints/model-metrics\",\n",
    "    threshold=0.70,\n",
    "    comparison=\"LessThanOrEqualToThreshold\",\n",
    "    statistic=\"Average\",\n",
    "    period=600,\n",
    "    evaluation_periods=1,\n",
    "    alarm_desc=\"Alert: model recall dropped below 0.70 baseline\",\n",
    "    dimensions=mq_alarm_dims,\n",
    "    treat_missing=\"breaching\",\n",
    "))\n",
    "\n",
    "created_alarms = [a for a in created_alarms if a]\n",
    "print(f\"\\n✓ {len(created_alarms)} CloudWatch alarms configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ig5hWw_S64pK",
   "metadata": {
    "id": "Ig5hWw_S64pK"
   },
   "source": [
    "---\n",
    "\n",
    "### 8.5 CloudWatch Dashboard\n",
    "\n",
    "A single comprehensive dashboard that surfaces all four monitoring pillars:\n",
    "endpoint performance, resource utilisation, model quality, and data quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1trSnTJd64pK",
   "metadata": {
    "id": "1trSnTJd64pK"
   },
   "source": [
    "#### 8.5.1 Create Comprehensive Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k8JKRg8e64pL",
   "metadata": {
    "id": "k8JKRg8e64pL"
   },
   "outputs": [],
   "source": [
    "dashboard_name = f\"{project_name}-monitoring-dashboard\"\n",
    "\n",
    "def _metric_widget(title, metrics, x, y, w=12, h=6, stat=\"Average\",\n",
    "                   period=300, region=REGION, y_min=0, y_label=None):\n",
    "    props = {\n",
    "        \"title\": title,\n",
    "        \"metrics\": metrics,\n",
    "        \"period\": period,\n",
    "        \"stat\": stat,\n",
    "        \"region\": region,\n",
    "        \"view\": \"timeSeries\",\n",
    "        \"yAxis\": {\"left\": {\"min\": y_min}},\n",
    "    }\n",
    "    if y_label:\n",
    "        props[\"yAxis\"][\"left\"][\"label\"] = y_label\n",
    "    return {\"type\": \"metric\", \"x\": x, \"y\": y, \"width\": w, \"height\": h,\n",
    "            \"properties\": props}\n",
    "\n",
    "dashboard_body = {\n",
    "    \"widgets\": [\n",
    "        # ── Row 0: Section title ──────────────────────────────────────────\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"x\": 0, \"y\": 0, \"width\": 24, \"height\": 2,\n",
    "            \"properties\": {\n",
    "                \"markdown\": (\n",
    "                    \"# VenueSignal — ML Monitoring Dashboard\\n\"\n",
    "                    f\"**Endpoint:** `{endpoint_name}` | \"\n",
    "                    f\"**Region:** `{REGION}` | \"\n",
    "                    \"Metrics refresh every 5 min\"\n",
    "                )\n",
    "            },\n",
    "        },\n",
    "        # ── Row 1: Endpoint invocations & errors ──────────────────────────\n",
    "        _metric_widget(\n",
    "            title=\"Endpoint Invocations\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"Invocations\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
    "                 {\"stat\": \"Sum\", \"label\": \"Invocations (Sum)\"}],\n",
    "            ],\n",
    "            stat=\"Sum\", x=0, y=2,\n",
    "        ),\n",
    "        _metric_widget(\n",
    "            title=\"Invocation Errors (4XX / 5XX)\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"Invocation4XXErrors\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
    "                 {\"stat\": \"Sum\", \"color\": \"#ff7f0e\", \"label\": \"4XX Client Errors\"}],\n",
    "                [\"AWS/SageMaker\", \"Invocation5XXErrors\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
    "                 {\"stat\": \"Sum\", \"color\": \"#d62728\", \"label\": \"5XX Server Errors\"}],\n",
    "            ],\n",
    "            stat=\"Sum\", x=12, y=2,\n",
    "        ),\n",
    "        # ── Row 2: Latency ────────────────────────────────────────────────\n",
    "        _metric_widget(\n",
    "            title=\"Model Latency (µs) — Avg & p99\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"ModelLatency\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"Avg Latency\"}],\n",
    "                [\"...\", {\"stat\": \"p99\", \"label\": \"p99 Latency\", \"color\": \"#e377c2\"}],\n",
    "            ],\n",
    "            x=0, y=8, y_label=\"Microseconds\",\n",
    "        ),\n",
    "        _metric_widget(\n",
    "            title=\"Overhead Latency (µs)\",\n",
    "            metrics=[\n",
    "                [\"AWS/SageMaker\", \"OverheadLatency\",\n",
    "                 \"EndpointName\", endpoint_name, \"VariantName\", variant_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"Avg Overhead\"}],\n",
    "            ],\n",
    "            x=12, y=8, y_label=\"Microseconds\",\n",
    "        ),\n",
    "        # ── Row 3: Resource utilisation ────────────────────────────────────\n",
    "        _metric_widget(\n",
    "            title=\"CPU Utilization (%)\",\n",
    "            metrics=[\n",
    "                [\"/aws/sagemaker/Endpoints\", \"CPUUtilization\",\n",
    "                 \"EndpointName\", endpoint_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"CPU %\"}],\n",
    "            ],\n",
    "            x=0, y=14, y_min=0, y_label=\"Percent\",\n",
    "        ),\n",
    "        _metric_widget(\n",
    "            title=\"Memory Utilization (%)\",\n",
    "            metrics=[\n",
    "                [\"/aws/sagemaker/Endpoints\", \"MemoryUtilization\",\n",
    "                 \"EndpointName\", endpoint_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"Memory %\"}],\n",
    "            ],\n",
    "            x=12, y=14, y_min=0, y_label=\"Percent\",\n",
    "        ),\n",
    "        _metric_widget(\n",
    "            title=\"Disk Utilization (%)\",\n",
    "            metrics=[\n",
    "                [\"/aws/sagemaker/Endpoints\", \"DiskUtilization\",\n",
    "                 \"EndpointName\", endpoint_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"Disk %\"}],\n",
    "            ],\n",
    "            x=0, y=20, w=12, y_min=0, y_label=\"Percent\",\n",
    "        ),\n",
    "        # ── Row 4: Model quality metrics (populated by MQ schedule) ───────\n",
    "        _metric_widget(\n",
    "            title=\"Model Quality — Recall (vs baseline)\",\n",
    "            metrics=[\n",
    "                [\"aws/sagemaker/Endpoints/model-metrics\", \"recall\",\n",
    "                 \"Endpoint\", endpoint_name,\n",
    "                 \"MonitoringSchedule\", mq_schedule_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"Recall\"}],\n",
    "            ],\n",
    "            x=12, y=20, y_min=0, y_label=\"Score\",\n",
    "        ),\n",
    "        # ── Row 5: Integration / custom metrics ───────────────────────────\n",
    "        _metric_widget(\n",
    "            title=\"Integration Test Pass Rate (%)\",\n",
    "            metrics=[\n",
    "                [\"VenueSignal/Integration\", \"IntegrationTestPassRate\",\n",
    "                 \"EndpointName\", endpoint_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"Pass Rate %\"}],\n",
    "            ],\n",
    "            stat=\"Average\", x=0, y=26, y_min=0,\n",
    "        ),\n",
    "        _metric_widget(\n",
    "            title=\"Health Check Latency (ms)\",\n",
    "            metrics=[\n",
    "                [\"VenueSignal/Integration\", \"HealthCheckLatencyMs\",\n",
    "                 \"EndpointName\", endpoint_name,\n",
    "                 {\"stat\": \"Average\", \"label\": \"Latency ms\"}],\n",
    "            ],\n",
    "            x=12, y=26, y_min=0, y_label=\"Milliseconds\",\n",
    "        ),\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    cloudwatch_client.put_dashboard(\n",
    "        DashboardName=dashboard_name,\n",
    "        DashboardBody=json.dumps(dashboard_body),\n",
    "    )\n",
    "    print(f\"✓ CloudWatch dashboard created/updated: {dashboard_name}\")\n",
    "    print(f\"\\nView dashboard at:\")\n",
    "    print(\n",
    "        f\"  https://console.aws.amazon.com/cloudwatch/home\"\n",
    "        f\"?region={REGION}#dashboards:name={dashboard_name}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating dashboard: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uzJlbuZG64pL",
   "metadata": {
    "id": "uzJlbuZG64pL"
   },
   "source": [
    "---\n",
    "\n",
    "### 8.6 Model Performance Tracking\n",
    "\n",
    "Compare training-time performance across all models and save a persistent\n",
    "summary to S3 for the monitoring record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zH5o9YbL64pL",
   "metadata": {
    "id": "zH5o9YbL64pL"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL PERFORMANCE TRACKING — Validation Set\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ── Retrieve stored results from Section 6 ─────────────────────────────────\n",
    "%store -r baseline_results\n",
    "%store -r xgb_results_val\n",
    "%store -r xgb_results_test\n",
    "\n",
    "b1 = baseline_results['baseline1']['val']\n",
    "b2 = baseline_results['baseline2']['val']\n",
    "xv = xgb_results_val\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"Model\": \"Baseline #1 (Heuristic)\",         \"Accuracy\": f\"{b1['accuracy']:.4f}\",\n",
    "     \"Precision\": f\"{b1['precision']:.4f}\",       \"Recall\": f\"{b1['recall']:.4f}\",\n",
    "     \"F1\": f\"{b1['f1']:.4f}\",                    \"RMSE\": f\"{b1['rmse']:.4f}\",\n",
    "     \"Within 1★\": f\"{b1['within_1.0_stars']*100:.2f}%\"},\n",
    "    {\"Model\": \"Baseline #2 (Logistic Reg.)\",      \"Accuracy\": f\"{b2['accuracy']:.4f}\",\n",
    "     \"Precision\": f\"{b2['precision']:.4f}\",       \"Recall\": f\"{b2['recall']:.4f}\",\n",
    "     \"F1\": f\"{b2['f1']:.4f}\",                    \"RMSE\": f\"{b2['rmse']:.4f}\",\n",
    "     \"Within 1★\": f\"{b2['within_1.0_stars']*100:.2f}%\"},\n",
    "    {\"Model\": \"XGBoost (Deployed)\",               \"Accuracy\": f\"{xv['accuracy']:.4f}\",\n",
    "     \"Precision\": f\"{xv['precision']:.4f}\",       \"Recall\": f\"{xv['recall']:.4f}\",\n",
    "     \"F1\": f\"{xv['f1']:.4f}\",                    \"RMSE\": f\"{xv['rmse']:.4f}\",\n",
    "     \"Within 1★\": f\"{xv['within_1.0_stars']*100:.2f}%\"},\n",
    "])\n",
    "display(summary)\n",
    "\n",
    "# ── XGBoost improvements ───────────────────────────────────────────────────\n",
    "def pct_change(new, old):\n",
    "    return (new - old) / old * 100\n",
    "\n",
    "print(\"\\nXGBoost improvements over Baseline #1:\")\n",
    "print(f\"  Accuracy : {pct_change(xv['accuracy'], b1['accuracy']):+.2f}%\")\n",
    "print(f\"  F1-Score : {pct_change(xv['f1'],       b1['f1']):+.2f}%\")\n",
    "print(f\"  RMSE     : {pct_change(b1['rmse'],     xv['rmse']):+.2f}%  (lower is better)\")\n",
    "\n",
    "print(\"\\nXGBoost improvements over Baseline #2:\")\n",
    "print(f\"  Accuracy : {pct_change(xv['accuracy'], b2['accuracy']):+.2f}%\")\n",
    "print(f\"  F1-Score : {pct_change(xv['f1'],       b2['f1']):+.2f}%\")\n",
    "print(f\"  RMSE     : {pct_change(b2['rmse'],     xv['rmse']):+.2f}%  (lower is better)\")\n",
    "\n",
    "# Save to S3 as monitoring artefact\n",
    "summary_local = \"/tmp/model_performance_summary.csv\"\n",
    "summary.to_csv(summary_local, index=False)\n",
    "s3_client.upload_file(\n",
    "    summary_local,\n",
    "    BASE_BUCKET_NAME,\n",
    "    f\"{MONITORING_PREFIX}performance_summary.csv\",\n",
    ")\n",
    "print(f\"\\n✓ Performance summary saved to s3://{MONITORING_DIR}performance_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z_UQmmNp64pM",
   "metadata": {
    "id": "Z_UQmmNp64pM"
   },
   "source": [
    "---\n",
    "\n",
    "### 8.7 Generate Monitoring Reports\n",
    "\n",
    "Generate comprehensive status reports for all monitoring schedules and\n",
    "upload them to S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XGE7QuWh64pM",
   "metadata": {
    "id": "XGE7QuWh64pM"
   },
   "source": [
    "#### 8.7.1 List All Monitoring Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GAdFjiLV64pM",
   "metadata": {
    "id": "GAdFjiLV64pM"
   },
   "outputs": [],
   "source": [
    "schedules = sagemaker_client.list_monitoring_schedules(\n",
    "    EndpointName=endpoint_name,\n",
    "    MaxResults=100,\n",
    ")\n",
    "print(\"=== Active Monitoring Schedules ===\")\n",
    "for s in schedules[\"MonitoringScheduleSummaries\"]:\n",
    "    print(f\"\\nSchedule : {s['MonitoringScheduleName']}\")\n",
    "    print(f\"  Type    : {s.get('MonitoringType', 'DataQuality')}\")\n",
    "    print(f\"  Status  : {s['MonitoringScheduleStatus']}\")\n",
    "    print(f\"  Created : {s['CreationTime']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GleP5Ae-64pM",
   "metadata": {
    "id": "GleP5Ae-64pM"
   },
   "source": [
    "#### 8.7.2 Check Latest Monitoring Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4s1dbM8v64pM",
   "metadata": {
    "id": "4s1dbM8v64pM"
   },
   "outputs": [],
   "source": [
    "def get_latest_execution(schedule_name):\n",
    "    try:\n",
    "        execs = sagemaker_client.list_monitoring_executions(\n",
    "            MonitoringScheduleName=schedule_name,\n",
    "            MaxResults=1,\n",
    "            SortBy=\"CreationTime\",\n",
    "            SortOrder=\"Descending\",\n",
    "        )\n",
    "        summaries = execs.get(\"MonitoringExecutionSummaries\", [])\n",
    "        return summaries[0] if summaries else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"=== Latest Monitoring Executions ===\")\n",
    "for s in schedules[\"MonitoringScheduleSummaries\"]:\n",
    "    name = s[\"MonitoringScheduleName\"]\n",
    "    execution = get_latest_execution(name)\n",
    "    print(f\"\\n{name}:\")\n",
    "    if execution:\n",
    "        print(f\"  Status   : {execution['MonitoringExecutionStatus']}\")\n",
    "        print(f\"  Scheduled: {execution.get('ScheduledTime', 'N/A')}\")\n",
    "        if \"ProcessingJobArn\" in execution:\n",
    "            print(f\"  Job      : {execution['ProcessingJobArn'].split('/')[-1]}\")\n",
    "        if \"FailureReason\" in execution:\n",
    "            print(f\"  Failure  : {execution['FailureReason']}\")\n",
    "    else:\n",
    "        print(\"  No executions yet — first run at the top of the next hour\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cvxT3Anb64pN",
   "metadata": {
    "id": "cvxT3Anb64pN"
   },
   "source": [
    "#### 8.7.3 Model Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bEBTKGIG64pN",
   "metadata": {
    "id": "bEBTKGIG64pN"
   },
   "outputs": [],
   "source": [
    "print(\"=== Model Quality Monitoring Report ===\")\n",
    "try:\n",
    "    mq_files = S3Downloader.list(mq_results_uri)\n",
    "    if mq_files:\n",
    "        print(f\"Found {len(mq_files)} result file(s) in {mq_results_uri}\")\n",
    "        for fpath in sorted(mq_files, reverse=True):\n",
    "            if \"constraint_violations\" in fpath:\n",
    "                local = S3Downloader.download(fpath, \"/tmp/mq_report\")\n",
    "                with open(local[0]) as fh:\n",
    "                    v = json.load(fh)\n",
    "                viol = v.get(\"violations\", [])\n",
    "                if viol:\n",
    "                    print(f\"\\n⚠ {len(viol)} constraint violation(s) detected:\")\n",
    "                    for item in viol:\n",
    "                        print(f\"  - {item.get('metric_name', 'N/A')}: \"\n",
    "                              f\"{item.get('description', item)}\")\n",
    "                else:\n",
    "                    print(\"\\n✓ No model quality violations detected\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"No model quality results yet — schedule must run at least once\")\n",
    "        print(\"First execution occurs at the top of the next hour.\")\n",
    "except Exception as e:\n",
    "    print(f\"Cannot retrieve model quality results: {e}\")\n",
    "    print(\"This is expected if the monitoring schedule has not yet run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r5d41U9064pN",
   "metadata": {
    "id": "r5d41U9064pN"
   },
   "source": [
    "#### 8.7.4 Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lzhf9W0o64pN",
   "metadata": {
    "id": "Lzhf9W0o64pN"
   },
   "outputs": [],
   "source": [
    "print(\"=== Data Quality Monitoring Report ===\")\n",
    "try:\n",
    "    dq_files = S3Downloader.list(dq_results_uri)\n",
    "    if dq_files:\n",
    "        print(f\"Found {len(dq_files)} result file(s) in {dq_results_uri}\")\n",
    "        for fpath in sorted(dq_files, reverse=True):\n",
    "            if \"constraint_violations\" in fpath:\n",
    "                local = S3Downloader.download(fpath, \"/tmp/dq_report\")\n",
    "                with open(local[0]) as fh:\n",
    "                    v = json.load(fh)\n",
    "                viol = v.get(\"violations\", [])\n",
    "                if viol:\n",
    "                    print(f\"\\n⚠ {len(viol)} data quality violation(s) detected:\")\n",
    "                    for item in viol:\n",
    "                        feat = item.get(\"feature_name\", \"N/A\")\n",
    "                        desc = item.get(\"description\", item)\n",
    "                        print(f\"  - Feature '{feat}': {desc}\")\n",
    "                else:\n",
    "                    print(\"\\n✓ No data quality violations detected\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"No data quality results yet — schedule must run at least once\")\n",
    "        print(\"First execution occurs at the top of the next hour.\")\n",
    "except Exception as e:\n",
    "    print(f\"Cannot retrieve data quality results: {e}\")\n",
    "    print(\"This is expected if the monitoring schedule has not yet run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "teyxzd3_64pN",
   "metadata": {
    "id": "teyxzd3_64pN"
   },
   "source": [
    "#### 8.7.5 Comprehensive Monitoring Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E_1Z6owi64pN",
   "metadata": {
    "id": "E_1Z6owi64pN"
   },
   "outputs": [],
   "source": [
    "# Retrieve CloudWatch alarms prefixed with the project name\n",
    "try:\n",
    "    alarms_resp = cloudwatch_client.describe_alarms(\n",
    "        AlarmNamePrefix=project_name,\n",
    "        MaxRecords=100,\n",
    "    )\n",
    "    alarm_list = alarms_resp.get(\"MetricAlarms\", [])\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not retrieve alarms: {e}\")\n",
    "    alarm_list = []\n",
    "\n",
    "# Query the latest infrastructure metrics\n",
    "latency_latest  = get_cw_metric(\"ModelLatency\")\n",
    "invoc_latest    = get_cw_metric(\"Invocations\",        statistic=\"Sum\")\n",
    "err4xx_latest   = get_cw_metric(\"Invocation4XXErrors\", statistic=\"Sum\")\n",
    "cpu_latest      = get_cw_metric(\"CPUUtilization\",     namespace=\"/aws/sagemaker/Endpoints\")\n",
    "mem_latest      = get_cw_metric(\"MemoryUtilization\",  namespace=\"/aws/sagemaker/Endpoints\")\n",
    "disk_latest     = get_cw_metric(\"DiskUtilization\",    namespace=\"/aws/sagemaker/Endpoints\")\n",
    "\n",
    "report = {\n",
    "    \"report_timestamp\":       datetime.now(timezone.utc).isoformat(),\n",
    "    \"endpoint_name\":          endpoint_name,\n",
    "    \"monitoring_schedules\": [\n",
    "        {\n",
    "            \"name\":   s[\"MonitoringScheduleName\"],\n",
    "            \"type\":   s.get(\"MonitoringType\", \"DataQuality\"),\n",
    "            \"status\": s[\"MonitoringScheduleStatus\"],\n",
    "        }\n",
    "        for s in schedules[\"MonitoringScheduleSummaries\"]\n",
    "    ],\n",
    "    \"infrastructure_metrics\": {\n",
    "        \"latency_us\":           latency_latest,\n",
    "        \"invocations\":          invoc_latest,\n",
    "        \"errors_4xx\":           err4xx_latest,\n",
    "        \"cpu_utilization_pct\":  cpu_latest,\n",
    "        \"memory_utilization_pct\": mem_latest,\n",
    "        \"disk_utilization_pct\": disk_latest,\n",
    "    },\n",
    "    \"cloudwatch_alarms\": [\n",
    "        {\n",
    "            \"name\":      a[\"AlarmName\"],\n",
    "            \"state\":     a[\"StateValue\"],\n",
    "            \"metric\":    a.get(\"MetricName\", \"Expression\"),\n",
    "            \"threshold\": a.get(\"Threshold\"),\n",
    "        }\n",
    "        for a in alarm_list\n",
    "    ],\n",
    "    \"integration_tests\": {\n",
    "        \"pass_rate\":    test_results[\"pass_rate\"],\n",
    "        \"passed\":       test_results[\"passed\"],\n",
    "        \"failed\":       test_results[\"failed\"],\n",
    "    },\n",
    "    \"health_check\": {\n",
    "        \"success_rate\": health_summary[\"success_rate\"],\n",
    "        \"avg_latency_ms\": health_summary[\"avg_latency_ms\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Persist report locally and upload to S3\n",
    "report_filename = f\"monitoring_report_{datetime.now(timezone.utc):%Y%m%d_%H%M%S}.json\"\n",
    "with open(report_filename, \"w\") as fh:\n",
    "    json.dump(report, fh, indent=2, default=str)\n",
    "\n",
    "report_s3_uri = S3Uploader.upload(report_filename, reports_uri)\n",
    "print(f\"✓ Monitoring report saved to: {report_s3_uri}\")\n",
    "print(\"\\n=== Report Summary ===\")\n",
    "print(json.dumps(report, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nf5MWxqv64pO",
   "metadata": {
    "id": "Nf5MWxqv64pO"
   },
   "source": [
    "---\n",
    "\n",
    "### 8.8 Examine Execution Results\n",
    "\n",
    "The cells below should be run **after** the monitoring schedules have\n",
    "completed their first hourly execution (usually within ~20 minutes of the\n",
    "top of the next hour).  They download constraint-violation reports and\n",
    "analyse the CloudWatch metrics emitted by the model quality job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AAAjtfXQ64pO",
   "metadata": {
    "id": "AAAjtfXQ64pO"
   },
   "source": [
    "#### 8.8.1 Wait for a Model Quality Execution to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tyBqwMQ-64pO",
   "metadata": {
    "id": "tyBqwMQ-64pO"
   },
   "outputs": [],
   "source": [
    "# Poll for the first successful execution — waits up to 90 minutes\n",
    "deadline = time.time() + 90 * 60\n",
    "print(f\"Polling for model quality execution (schedule: {mq_schedule_name})…\")\n",
    "print(\"Note: first execution fires at the top of the next hour + up to 20 min.\")\n",
    "\n",
    "latest_execution = None\n",
    "while time.time() < deadline:\n",
    "    try:\n",
    "        desc = sagemaker_client.describe_monitoring_schedule(\n",
    "            MonitoringScheduleName=mq_schedule_name\n",
    "        )\n",
    "        print(f\"  Schedule status: {desc['MonitoringScheduleStatus']}\")\n",
    "        last_exec = desc.get(\"LastMonitoringExecutionSummary\")\n",
    "        if last_exec:\n",
    "            print(f\"  Last execution : {last_exec['MonitoringExecutionStatus']}\")\n",
    "            if last_exec[\"MonitoringExecutionStatus\"] in (\"Completed\", \"CompletedWithViolations\"):\n",
    "                print(\"✓ Execution completed!\")\n",
    "                # Retrieve the execution object for deeper inspection\n",
    "                execs = xgboost_model_quality_monitor.list_executions()\n",
    "                if execs:\n",
    "                    latest_execution = execs[-1]\n",
    "                break\n",
    "            elif last_exec[\"MonitoringExecutionStatus\"] == \"Failed\":\n",
    "                print(f\"✗ Execution failed: {last_exec.get('FailureReason', 'unknown')}\")\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"  Polling error: {e}\")\n",
    "    sleep(30)\n",
    "\n",
    "if time.time() >= deadline:\n",
    "    print(\"⏱ Timeout: no completed execution within 90 minutes.\")\n",
    "    print(\"  Re-run this cell later or check the SageMaker console.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J6iIgL3K64pO",
   "metadata": {
    "id": "J6iIgL3K64pO"
   },
   "source": [
    "#### 8.8.2 Review Constraint Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0Nu7Bld64pO",
   "metadata": {
    "id": "a0Nu7Bld64pO"
   },
   "outputs": [],
   "source": [
    "if latest_execution is not None:\n",
    "    try:\n",
    "        violations = latest_execution.constraint_violations().body_dict.get(\n",
    "            \"violations\", []\n",
    "        )\n",
    "        if violations:\n",
    "            import pandas as pd\n",
    "            pd.options.display.max_colwidth = None\n",
    "            print(f\"⚠ {len(violations)} constraint violation(s):\")\n",
    "            display(pd.json_normalize(violations).head(20))\n",
    "        else:\n",
    "            print(\"✓ No constraint violations in this execution\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve violations: {e}\")\n",
    "else:\n",
    "    print(\"No completed execution available yet — run Section 8.8.1 first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r5zRLHlf64pO",
   "metadata": {
    "id": "r5zRLHlf64pO"
   },
   "source": [
    "#### 8.8.3 Analyse Model Quality CloudWatch Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dX6LTSLg64pP",
   "metadata": {
    "id": "dX6LTSLg64pP"
   },
   "outputs": [],
   "source": [
    "# List all model-quality metrics emitted by the monitor for this schedule\n",
    "cw_namespace = \"aws/sagemaker/Endpoints/model-metrics\"\n",
    "cw_dims = [\n",
    "    {\"Name\": \"Endpoint\",           \"Value\": endpoint_name},\n",
    "    {\"Name\": \"MonitoringSchedule\", \"Value\": mq_schedule_name},\n",
    "]\n",
    "\n",
    "paginator = cloudwatch_client.get_paginator(\"list_metrics\")\n",
    "mq_metric_names = []\n",
    "for page in paginator.paginate(Dimensions=cw_dims, Namespace=cw_namespace):\n",
    "    for metric in page.get(\"Metrics\", []):\n",
    "        mq_metric_names.append(metric[\"MetricName\"])\n",
    "\n",
    "if mq_metric_names:\n",
    "    print(f\"Model quality metrics in CloudWatch ({len(mq_metric_names)} found):\")\n",
    "    for name in mq_metric_names:\n",
    "        print(f\"  {name}\")\n",
    "else:\n",
    "    print(\"No model quality metrics in CloudWatch yet.\")\n",
    "    print(\"They appear after the first successful monitoring execution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57942bc0-2f03-4f61-b44f-00f68b06a270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14f1b72a-174d-44de-be32-bc9a782d90fe",
   "metadata": {},
   "source": [
    "#### 8.9 Stop Monitors/Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa10e6-53e6-42d1-9f23-c2c8026a9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# List all running threads\n",
    "for t in threading.enumerate():\n",
    "    print(t.name, t.daemon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a515e29-3a1a-4647-9dea-f345db9bb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model_quality_monitor.stop_monitoring_schedule()\n",
    "data_quality_monitor.stop_monitoring_schedule()\n",
    "print(\"✓ Both monitoring schedules stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c8a57-2600-404f-90ec-6c045557346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review Logs/Errors\n",
    "capture_files = sorted(S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\"))\n",
    "print(f\"Found {len(capture_files)} capture files in new path\")\n",
    "\n",
    "if capture_files:\n",
    "    # Read the most recent file\n",
    "    raw = S3Downloader.read_file(capture_files[-1])\n",
    "    first_record = json.loads(raw.split(\"\\n\")[0])\n",
    "    print(\"\\nCapture record structure:\")\n",
    "    print(json.dumps(first_record, indent=2))\n",
    "    \n",
    "    # Show input and output data\n",
    "    print(\"\\n--- Input data (what was sent to endpoint) ---\")\n",
    "    print(first_record.get(\"captureData\", {}).get(\"endpointInput\", {}).get(\"data\", \"N/A\"))\n",
    "    print(\"\\n--- Output data (what endpoint returned) ---\")\n",
    "    print(first_record.get(\"captureData\", {}).get(\"endpointOutput\", {}).get(\"data\", \"N/A\"))\n",
    "    print(\"\\n--- Encoding ---\")\n",
    "    print(\"Input encoding: \", first_record.get(\"captureData\", {}).get(\"endpointInput\", {}).get(\"encoding\", \"N/A\"))\n",
    "    print(\"Output encoding:\", first_record.get(\"captureData\", {}).get(\"endpointOutput\", {}).get(\"encoding\", \"N/A\"))\n",
    "    print(\"\\n--- InferenceId ---\")\n",
    "    print(first_record.get(\"eventMetadata\", {}).get(\"inferenceId\", \"N/A\"))\n",
    "else:\n",
    "    print(\"No capture files found yet — send some traffic first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ca904-056a-4739-b8d6-a04d4e26b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review Cloudwatch logs\n",
    "logs_client = boto3.client(\"logs\", region_name=REGION)\n",
    "\n",
    "try:\n",
    "    # SageMaker model monitor logs go to this log group\n",
    "    log_group = f\"/aws/sagemaker/ProcessingJobs\"\n",
    "    \n",
    "    # Find the most recent model quality processing job\n",
    "    desc = sagemaker_client.describe_monitoring_schedule(\n",
    "        MonitoringScheduleName=mq_schedule_name\n",
    "    )\n",
    "    last_exec = desc.get(\"LastMonitoringExecutionSummary\", {})\n",
    "    job_arn = last_exec.get(\"ProcessingJobArn\", \"\")\n",
    "    job_name = job_arn.split(\"/\")[-1] if job_arn else None\n",
    "    \n",
    "    if job_name:\n",
    "        print(f\"Checking logs for job: {job_name}\")\n",
    "        streams = logs_client.describe_log_streams(\n",
    "            logGroupName=log_group,\n",
    "            logStreamNamePrefix=job_name,\n",
    "            orderBy=\"LastEventTime\",\n",
    "            descending=True,\n",
    "        )\n",
    "        if streams[\"logStreams\"]:\n",
    "            stream_name = streams[\"logStreams\"][0][\"logStreamName\"]\n",
    "            events = logs_client.get_log_events(\n",
    "                logGroupName=log_group,\n",
    "                logStreamName=stream_name,\n",
    "                limit=50,\n",
    "            )\n",
    "            print(f\"\\nLast 50 log lines from: {stream_name}\\n\")\n",
    "            for e in events[\"events\"]:\n",
    "                print(e[\"message\"])\n",
    "        else:\n",
    "            print(\"No log streams found for this job\")\n",
    "    else:\n",
    "        print(\"No processing job found in last execution\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching logs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1qtRWU0e7ENM",
   "metadata": {
    "id": "1qtRWU0e7ENM"
   },
   "source": [
    "## 9. CI/CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4Gqo2Mh67Evo",
   "metadata": {
    "id": "4Gqo2Mh67Evo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "C1CfBiL364pP",
   "metadata": {
    "id": "C1CfBiL364pP"
   },
   "source": [
    "## References\n",
    "\n",
    "- Amazon Web Services. (n.d.). *Amazon SageMaker developer guide*. https://docs.aws.amazon.com/sagemaker/\n",
    "- Amazon Web Services. (n.d.). *AWS SDK for Python (Boto3) documentation*. https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "- Anthropic. (2024). *Claude* (Version 4.5 Sonnet) [Large language model]. https://www.anthropic.com/claude\n",
    "- Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. *Proceedings of the 22nd ACM SIGKDD* (pp. 785–794). https://doi.org/10.1145/2939672.2939785\n",
    "- Harris, C. R., et al. (2020). Array programming with NumPy. *Nature, 585*(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "- Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. *Computing in Science & Engineering, 9*(3), 90–95. https://doi.org/10.1109/MCSE.2007.55\n",
    "- Huyen, C. (2022). *Designing machine learning systems: An iterative process for production-ready applications*. O'Reilly Media.\n",
    "- McKinney, W. (2010). Data structures for statistical computing in Python. *Proceedings of the 9th Python in Science Conference* (pp. 51–56).\n",
    "- Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. *JMLR, 12*, 2825–2830.\n",
    "- Waskom, M. L. (2021). seaborn: Statistical data visualization. *JOSS, 6*(60), 3021. https://doi.org/10.21105/joss.03021\n",
    "- Yelp. (n.d.). *Yelp Open Dataset*. https://www.yelp.com/dataset\n",
    "\n",
    "This project utilised Claude (Anthropic) and ChatGPT (OpenAI) for code debugging, documentation assistance, and technical guidance.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
