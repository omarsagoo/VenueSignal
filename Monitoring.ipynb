{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive ML System Monitoring\n",
    "## AAI-540 Final Project - Module 5: Monitoring & Observability\n",
    "\n",
    "**Project Team:** [Your Team Name]\n",
    "\n",
    "**Authors:** [Team Member Names]\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements comprehensive monitoring for our ML system including:\n",
    "1. **Model Monitoring** - Track prediction quality and bias\n",
    "2. **Data Monitoring** - Detect data quality issues and distribution drift\n",
    "3. **Infrastructure Monitoring** - Monitor endpoint performance and resource utilization\n",
    "4. **CloudWatch Dashboard** - Centralized visualization of all metrics\n",
    "5. **Automated Reporting** - Generate model and data quality reports\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Configuration](#setup)\n",
    "2. [Model Quality Monitoring](#model-quality)\n",
    "3. [Data Quality Monitoring](#data-quality)\n",
    "4. [Model Bias Monitoring](#model-bias)\n",
    "5. [Infrastructure Monitoring](#infrastructure)\n",
    "6. [CloudWatch Dashboard](#dashboard)\n",
    "7. [Generate Reports](#reports)\n",
    "8. [Cleanup](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install any missing packages (uncomment if needed)\n",
    "# !pip install --upgrade sagemaker boto3 -q\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from time import sleep\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer, JSONSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer, JSONDeserializer\n",
    "\n",
    "# Model Monitor imports\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    ModelQualityMonitor,\n",
    "    DataQualityDistributionConstraints,\n",
    "    CronExpressionGenerator\n",
    ")\n",
    "\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# Clarify imports for bias monitoring\n",
    "from sagemaker.clarify import (\n",
    "    BiasConfig,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    ModelPredictedLabelConfig,\n",
    "    SageMakerClarifyProcessor\n",
    ")\n",
    "from sagemaker.model_monitor import BiasAnalysisConfig, ModelBiasMonitor\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize Session and Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Initialize AWS clients\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "cw_client = boto3.client('cloudwatch', region_name=region)\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"SageMaker Session: {sagemaker_session}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configure S3 Paths and Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# TODO: Update this prefix to match your project\n",
    "project_name = \"team-project\"  # UPDATE THIS\n",
    "prefix = f\"sagemaker/{project_name}\"\n",
    "\n",
    "# Define S3 paths for monitoring\n",
    "data_capture_prefix = f\"{prefix}/monitoring/datacapture\"\n",
    "data_capture_uri = f\"s3://{bucket}/{data_capture_prefix}\"\n",
    "\n",
    "baseline_prefix = f\"{prefix}/monitoring/baseline\"\n",
    "baseline_data_uri = f\"s3://{bucket}/{baseline_prefix}/data\"\n",
    "baseline_results_uri = f\"s3://{bucket}/{baseline_prefix}/results\"\n",
    "\n",
    "model_quality_prefix = f\"{prefix}/monitoring/model-quality\"\n",
    "model_quality_baseline_uri = f\"s3://{bucket}/{model_quality_prefix}/baseline\"\n",
    "model_quality_results_uri = f\"s3://{bucket}/{model_quality_prefix}/results\"\n",
    "\n",
    "data_quality_prefix = f\"{prefix}/monitoring/data-quality\"\n",
    "data_quality_baseline_uri = f\"s3://{bucket}/{data_quality_prefix}/baseline\"\n",
    "data_quality_results_uri = f\"s3://{bucket}/{data_quality_prefix}/results\"\n",
    "\n",
    "bias_prefix = f\"{prefix}/monitoring/bias\"\n",
    "bias_baseline_uri = f\"s3://{bucket}/{bias_prefix}/baseline\"\n",
    "bias_results_uri = f\"s3://{bucket}/{bias_prefix}/results\"\n",
    "\n",
    "reports_uri = f\"s3://{bucket}/{prefix}/monitoring/reports\"\n",
    "\n",
    "print(\"\\n=== S3 Configuration ===\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Data Capture: {data_capture_uri}\")\n",
    "print(f\"Baseline: {baseline_results_uri}\")\n",
    "print(f\"Model Quality: {model_quality_results_uri}\")\n",
    "print(f\"Data Quality: {data_quality_results_uri}\")\n",
    "print(f\"Bias: {bias_results_uri}\")\n",
    "print(f\"Reports: {reports_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Specify Your Endpoint Name\n",
    "\n",
    "**Important:** Replace with your actual deployed endpoint name from Module 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update with your actual endpoint name\n",
    "endpoint_name = \"your-endpoint-name\"  # UPDATE THIS\n",
    "\n",
    "# Verify endpoint exists\n",
    "try:\n",
    "    response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"✓ Endpoint '{endpoint_name}' found\")\n",
    "    print(f\"  Status: {response['EndpointStatus']}\")\n",
    "    print(f\"  Instance Type: {response['ProductionVariants'][0]['InstanceType']}\")\n",
    "    print(f\"  Current Instance Count: {response['ProductionVariants'][0]['CurrentInstanceCount']}\")\nexcept Exception as e:\n",
    "    print(f\"✗ Error: Endpoint '{endpoint_name}' not found\")\n",
    "    print(f\"  {str(e)}\")\n",
    "    print(\"\\nPlease update the endpoint_name variable with your deployed endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Load Baseline Training Data\n",
    "\n",
    "**Important:** Update the path to your training data used in Module 3-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update with your actual training data path\n",
    "training_data_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"  # UPDATE THIS\n",
    "\n",
    "# TODO: Update with your actual validation/test data path for ground truth\n",
    "validation_data_uri = f\"s3://{bucket}/{prefix}/data/validation/validation.csv\"  # UPDATE THIS\n",
    "\n",
    "print(f\"Training data: {training_data_uri}\")\n",
    "print(f\"Validation data: {validation_data_uri}\")\n",
    "\n",
    "# Load a sample to verify\n",
    "try:\n",
    "    # Download a sample\n",
    "    local_training_file = S3Downloader.download(training_data_uri, \".\")\n",
    "    df_train_sample = pd.read_csv(local_training_file[0], nrows=5)\n",
    "    print(f\"\\n✓ Training data loaded successfully\")\n",
    "    print(f\"  Shape: {df_train_sample.shape}\")\n",
    "    print(f\"  Columns: {list(df_train_sample.columns)}\")\n",
    "    display(df_train_sample.head())\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading training data: {e}\")\n",
    "    print(\"Please update the training_data_uri variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Quality Monitoring <a id='model-quality'></a>\n",
    "\n",
    "Model Quality Monitoring tracks the prediction accuracy of your model over time by comparing predictions against ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Model Quality Baseline\n",
    "\n",
    "First, we need to establish a baseline for model quality metrics using our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify your problem type\n",
    "problem_type = \"BinaryClassification\"  # Options: \"BinaryClassification\", \"MulticlassClassification\", \"Regression\"\n",
    "\n",
    "# TODO: Specify inference and ground truth column names\n",
    "inference_attribute = \"prediction\"  # Column name for predictions in captured data\n",
    "probability_attribute = \"probability\"  # Column name for probability scores (if applicable)\n",
    "ground_truth_attribute = \"label\"  # Column name for actual labels in ground truth data\n",
    "\n",
    "print(f\"Problem Type: {problem_type}\")\n",
    "print(f\"Inference Attribute: {inference_attribute}\")\n",
    "print(f\"Ground Truth Attribute: {ground_truth_attribute}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize Model Quality Monitor\n",
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",  # Resource-efficient instance\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "print(\"✓ Model Quality Monitor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Suggest baseline for model quality\n",
    "try:\n",
    "    model_quality_monitor.suggest_baseline(\n",
    "        baseline_dataset=validation_data_uri,\n",
    "        dataset_format=DatasetFormat.csv(header=True),\n",
    "        output_s3_uri=model_quality_baseline_uri,\n",
    "        problem_type=problem_type,\n",
    "        inference_attribute=inference_attribute,\n",
    "        ground_truth_attribute=ground_truth_attribute,\n",
    "        wait=True,\n",
    "        logs=False\n",
    "    )\n",
    "    print(\"✓ Model quality baseline created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating baseline: {e}\")\n",
    "    print(\"This may happen if your validation data format doesn't match expectations.\")\n",
    "    print(\"Verify that your validation data has both predictions and ground truth labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Review Model Quality Baseline Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and display baseline statistics\n",
    "baseline_job = model_quality_monitor.latest_baselining_job\n",
    "baseline_stats_uri = baseline_job.baseline_statistics().statistics_s3_uri\n",
    "\n",
    "print(f\"Baseline Statistics URI: {baseline_stats_uri}\")\n",
    "\n",
    "# Download the baseline statistics\n",
    "local_stats_file = S3Downloader.download(baseline_stats_uri, \".\")\n",
    "with open(local_stats_file[0], 'r') as f:\n",
    "    baseline_stats = json.load(f)\n",
    "\n",
    "print(\"\\n=== Model Quality Baseline Metrics ===\")\n",
    "if 'binary_classification_metrics' in baseline_stats:\n",
    "    metrics = baseline_stats['binary_classification_metrics']\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value['value']:.4f}\")\n",
    "elif 'regression_metrics' in baseline_stats:\n",
    "    metrics = baseline_stats['regression_metrics']\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value['value']:.4f}\")\n",
    "else:\n",
    "    print(json.dumps(baseline_stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create Model Quality Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "model_quality_schedule_name = f\"{project_name}-model-quality-schedule-{datetime.now(timezone.utc):%Y%m%d-%H%M}\"\n",
    "\n",
    "try:\n",
    "    model_quality_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=model_quality_schedule_name,\n",
    "        endpoint_input=endpoint_name,\n",
    "        output_s3_uri=model_quality_results_uri,\n",
    "        problem_type=problem_type,\n",
    "        ground_truth_input=validation_data_uri,  # You'll need to update this with actual ground truth source\n",
    "        constraints=model_quality_monitor.suggested_constraints(),\n",
    "        schedule_cron_expression=CronExpressionGenerator.daily(),  # Run daily to save costs\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"✓ Model quality monitoring schedule created: {model_quality_schedule_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating schedule: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Monitoring <a id='data-quality'></a>\n",
    "\n",
    "Data Quality Monitoring detects anomalies and data drift in the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Enable Data Capture on Endpoint\n",
    "\n",
    "**Note:** If you already have data capture enabled from Module 4, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data capture is already enabled\n",
    "try:\n",
    "    endpoint_config = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    config_name = endpoint_config['EndpointConfigName']\n",
    "    config_details = sm_client.describe_endpoint_config(EndpointConfigName=config_name)\n",
    "    \n",
    "    if 'DataCaptureConfig' in config_details:\n",
    "        print(\"✓ Data capture is already enabled on this endpoint\")\n",
    "        capture_enabled = True\n",
    "    else:\n",
    "        print(\"✗ Data capture is not enabled\")\n",
    "        print(\"You need to update your endpoint config to enable data capture.\")\n",
    "        print(\"See the code below for reference.\")\n",
    "        capture_enabled = False\n",
    "except Exception as e:\n",
    "    print(f\"Error checking endpoint: {e}\")\n",
    "    capture_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data capture is not enabled, use this code to update your endpoint\n",
    "# Uncomment and run if needed\n",
    "\n",
    "# from sagemaker.model_monitor import DataCaptureConfig\n",
    "# \n",
    "# data_capture_config = DataCaptureConfig(\n",
    "#     enable_capture=True,\n",
    "#     sampling_percentage=100,  # Capture 100% of requests (reduce for high-traffic endpoints)\n",
    "#     destination_s3_uri=data_capture_uri,\n",
    "#     capture_options=[\"INPUT\", \"OUTPUT\"]  # Capture both input and output\n",
    "# )\n",
    "# \n",
    "# # You'll need to update your endpoint configuration\n",
    "# # This requires redeploying the endpoint with the new configuration\n",
    "# # See Module 4 deployment code for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Data Quality Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize Data Quality Monitor\n",
    "data_quality_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "print(\"✓ Data Quality Monitor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create baseline for data quality\n",
    "try:\n",
    "    data_quality_monitor.suggest_baseline(\n",
    "        baseline_dataset=training_data_uri,\n",
    "        dataset_format=DatasetFormat.csv(header=True),\n",
    "        output_s3_uri=data_quality_baseline_uri,\n",
    "        wait=True,\n",
    "        logs=False\n",
    "    )\n",
    "    print(\"✓ Data quality baseline created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating baseline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Review Data Quality Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and display baseline statistics\n",
    "baseline_job = data_quality_monitor.latest_baselining_job\n",
    "schema_uri = baseline_job.baseline_statistics().statistics_s3_uri\n",
    "constraints_uri = baseline_job.suggested_constraints().constraints_s3_uri\n",
    "\n",
    "print(f\"Schema URI: {schema_uri}\")\n",
    "print(f\"Constraints URI: {constraints_uri}\")\n",
    "\n",
    "# Download statistics\n",
    "local_stats_file = S3Downloader.download(schema_uri, \".\")\n",
    "with open(local_stats_file[0], 'r') as f:\n",
    "    data_stats = json.load(f)\n",
    "\n",
    "print(\"\\n=== Data Quality Statistics (Sample) ===\")\n",
    "if 'features' in data_stats:\n",
    "    for i, feature in enumerate(list(data_stats['features'])[:5]):  # Show first 5 features\n",
    "        print(f\"\\nFeature: {feature['name']}\")\n",
    "        print(f\"  Type: {feature.get('inferred_type', 'unknown')}\")\n",
    "        if 'numerical_statistics' in feature:\n",
    "            stats = feature['numerical_statistics']\n",
    "            print(f\"  Min: {stats.get('min', 'N/A')}\")\n",
    "            print(f\"  Max: {stats.get('max', 'N/A')}\")\n",
    "            print(f\"  Mean: {stats.get('mean', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create Data Quality Monitoring Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_quality_schedule_name = f\"{project_name}-data-quality-schedule-{datetime.now(timezone.utc):%Y%m%d-%H%M}\"\n",
    "\n",
    "try:\n",
    "    data_quality_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=data_quality_schedule_name,\n",
    "        endpoint_input=endpoint_name,\n",
    "        output_s3_uri=data_quality_results_uri,\n",
    "        statistics=data_quality_monitor.baseline_statistics(),\n",
    "        constraints=data_quality_monitor.suggested_constraints(),\n",
    "        schedule_cron_expression=CronExpressionGenerator.daily(),\n",
    "        enable_cloudwatch_metrics=True,\n",
    "    )\n",
    "    print(f\"✓ Data quality monitoring schedule created: {data_quality_schedule_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating schedule: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Bias Monitoring <a id='model-bias'></a>\n",
    "\n",
    "**Note:** Bias monitoring is optional but recommended if your model uses sensitive attributes. Skip this section if not applicable to your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Configure Bias Monitoring (Optional)\n",
    "\n",
    "Uncomment and configure if your dataset has sensitive attributes to monitor for bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Configure if applicable to your project\n",
    "# \n",
    "# # Specify the sensitive attribute (facet) to monitor\n",
    "# facet_name = \"age_group\"  # UPDATE THIS - e.g., \"gender\", \"age\", \"ethnicity\"\n",
    "# facet_values_or_threshold = [1]  # UPDATE THIS - sensitive group(s) to monitor\n",
    "# \n",
    "# # Label column name\n",
    "# label_name = \"label\"  # UPDATE THIS\n",
    "# \n",
    "# # Configure bias\n",
    "# bias_config = BiasConfig(\n",
    "#     label_values_or_threshold=[1],  # Positive class\n",
    "#     facet_name=facet_name,\n",
    "#     facet_values_or_threshold=facet_values_or_threshold,\n",
    "# )\n",
    "# \n",
    "# print(f\"Bias monitoring configured for facet: {facet_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Infrastructure Monitoring <a id='infrastructure'></a>\n",
    "\n",
    "Monitor endpoint performance, latency, and resource utilization using CloudWatch metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create CloudWatch Alarms for Endpoint Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alarm thresholds\n",
    "LATENCY_THRESHOLD_MS = 1000  # Alert if p99 latency > 1 second\n",
    "ERROR_RATE_THRESHOLD = 5  # Alert if error rate > 5%\n",
    "CPU_THRESHOLD = 80  # Alert if CPU > 80%\n",
    "MEMORY_THRESHOLD = 80  # Alert if memory > 80%\n",
    "\n",
    "print(\"Alarm Thresholds:\")\n",
    "print(f\"  Latency: {LATENCY_THRESHOLD_MS}ms\")\n",
    "print(f\"  Error Rate: {ERROR_RATE_THRESHOLD}%\")\n",
    "print(f\"  CPU: {CPU_THRESHOLD}%\")\n",
    "print(f\"  Memory: {MEMORY_THRESHOLD}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get endpoint variant name\n",
    "endpoint_desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "variant_name = endpoint_desc['ProductionVariants'][0]['VariantName']\n",
    "\n",
    "print(f\"Endpoint: {endpoint_name}\")\n",
    "print(f\"Variant: {variant_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Latency Alarm\n",
    "try:\n",
    "    cw_client.put_metric_alarm(\n",
    "        AlarmName=f\"{endpoint_name}-High-Latency\",\n",
    "        ComparisonOperator='GreaterThanThreshold',\n",
    "        EvaluationPeriods=2,\n",
    "        MetricName='ModelLatency',\n",
    "        Namespace='AWS/SageMaker',\n",
    "        Period=300,  # 5 minutes\n",
    "        Statistic='Average',\n",
    "        Threshold=LATENCY_THRESHOLD_MS * 1000,  # Convert to microseconds\n",
    "        ActionsEnabled=False,\n",
    "        AlarmDescription='Alert when model latency is too high',\n",
    "        Dimensions=[\n",
    "            {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "            {'Name': 'VariantName', 'Value': variant_name}\n",
    "        ]\n",
    "    )\n",
    "    print(\"✓ Created alarm: High Latency\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating latency alarm: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Invocation Error Rate Alarm\n",
    "try:\n",
    "    cw_client.put_metric_alarm(\n",
    "        AlarmName=f\"{endpoint_name}-High-Error-Rate\",\n",
    "        ComparisonOperator='GreaterThanThreshold',\n",
    "        EvaluationPeriods=2,\n",
    "        Metrics=[\n",
    "            {\n",
    "                'Id': 'error_rate',\n",
    "                'Expression': '(m2/m1)*100',\n",
    "                'Label': 'Error Rate (%)',\n",
    "            },\n",
    "            {\n",
    "                'Id': 'm1',\n",
    "                'MetricStat': {\n",
    "                    'Metric': {\n",
    "                        'Namespace': 'AWS/SageMaker',\n",
    "                        'MetricName': 'Invocations',\n",
    "                        'Dimensions': [\n",
    "                            {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "                            {'Name': 'VariantName', 'Value': variant_name}\n",
    "                        ]\n",
    "                    },\n",
    "                    'Period': 300,\n",
    "                    'Stat': 'Sum',\n",
    "                },\n",
    "                'ReturnData': False,\n",
    "            },\n",
    "            {\n",
    "                'Id': 'm2',\n",
    "                'MetricStat': {\n",
    "                    'Metric': {\n",
    "                        'Namespace': 'AWS/SageMaker',\n",
    "                        'MetricName': 'ModelInvocationErrors',\n",
    "                        'Dimensions': [\n",
    "                            {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "                            {'Name': 'VariantName', 'Value': variant_name}\n",
    "                        ]\n",
    "                    },\n",
    "                    'Period': 300,\n",
    "                    'Stat': 'Sum',\n",
    "                },\n",
    "                'ReturnData': False,\n",
    "            },\n",
    "        ],\n",
    "        Threshold=ERROR_RATE_THRESHOLD,\n",
    "        ActionsEnabled=False,\n",
    "        AlarmDescription='Alert when error rate exceeds threshold'\n",
    "    )\n",
    "    print(\"✓ Created alarm: High Error Rate\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating error rate alarm: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CPU Utilization Alarm\n",
    "try:\n",
    "    cw_client.put_metric_alarm(\n",
    "        AlarmName=f\"{endpoint_name}-High-CPU\",\n",
    "        ComparisonOperator='GreaterThanThreshold',\n",
    "        EvaluationPeriods=2,\n",
    "        MetricName='CPUUtilization',\n",
    "        Namespace='/aws/sagemaker/Endpoints',\n",
    "        Period=300,\n",
    "        Statistic='Average',\n",
    "        Threshold=CPU_THRESHOLD,\n",
    "        ActionsEnabled=False,\n",
    "        AlarmDescription='Alert when CPU utilization is high',\n",
    "        Dimensions=[\n",
    "            {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "            {'Name': 'VariantName', 'Value': variant_name}\n",
    "        ]\n",
    "    )\n",
    "    print(\"✓ Created alarm: High CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating CPU alarm: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Memory Utilization Alarm\n",
    "try:\n",
    "    cw_client.put_metric_alarm(\n",
    "        AlarmName=f\"{endpoint_name}-High-Memory\",\n",
    "        ComparisonOperator='GreaterThanThreshold',\n",
    "        EvaluationPeriods=2,\n",
    "        MetricName='MemoryUtilization',\n",
    "        Namespace='/aws/sagemaker/Endpoints',\n",
    "        Period=300,\n",
    "        Statistic='Average',\n",
    "        Threshold=MEMORY_THRESHOLD,\n",
    "        ActionsEnabled=False,\n",
    "        AlarmDescription='Alert when memory utilization is high',\n",
    "        Dimensions=[\n",
    "            {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "            {'Name': 'VariantName', 'Value': variant_name}\n",
    "        ]\n",
    "    )\n",
    "    print(\"✓ Created alarm: High Memory\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating memory alarm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 List All CloudWatch Alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all alarms for this endpoint\n",
    "alarms = cw_client.describe_alarms(\n",
    "    AlarmNamePrefix=endpoint_name\n",
    ")\n",
    "\n",
    "print(\"\\n=== CloudWatch Alarms ===\")\n",
    "for alarm in alarms['MetricAlarms']:\n",
    "    print(f\"\\n{alarm['AlarmName']}\")\n",
    "    print(f\"  State: {alarm['StateValue']}\")\n",
    "    print(f\"  Metric: {alarm.get('MetricName', 'Expression')}\")\n",
    "    print(f\"  Threshold: {alarm['Threshold']}\")\n",
    "    print(f\"  Description: {alarm.get('AlarmDescription', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CloudWatch Dashboard <a id='dashboard'></a>\n",
    "\n",
    "Create a centralized dashboard for visualizing all monitoring metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create Comprehensive Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_name = f\"{project_name}-monitoring-dashboard\"\n",
    "\n",
    "# Define dashboard body\n",
    "dashboard_body = {\n",
    "    \"widgets\": [\n",
    "        # Row 1: Endpoint Performance\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"properties\": {\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"ModelLatency\", {\"stat\": \"Average\", \"label\": \"Avg Latency\"}],\n",
    "                    [\"...\", {\"stat\": \"p99\", \"label\": \"P99 Latency\"}]\n",
    "                ],\n",
    "                \"period\": 300,\n",
    "                \"stat\": \"Average\",\n",
    "                \"region\": region,\n",
    "                \"title\": \"Model Latency (microseconds)\",\n",
    "                \"dimensions\": {\n",
    "                    \"EndpointName\": endpoint_name,\n",
    "                    \"VariantName\": variant_name\n",
    "                },\n",
    "                \"yAxis\": {\"left\": {\"min\": 0}}\n",
    "            },\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"x\": 0,\n",
    "            \"y\": 0\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"properties\": {\n",
    "                \"metrics\": [\n",
    "                    [\"AWS/SageMaker\", \"Invocations\", {\"stat\": \"Sum\", \"label\": \"Total Invocations\"}],\n",
    "                    [\".\", \"ModelInvocationErrors\", {\"stat\": \"Sum\", \"label\": \"Errors\"}]\n",
    "                ],\n",
    "                \"period\": 300,\n",
    "                \"stat\": \"Sum\",\n",
    "                \"region\": region,\n",
    "                \"title\": \"Invocations & Errors\",\n",
    "                \"dimensions\": {\n",
    "                    \"EndpointName\": endpoint_name,\n",
    "                    \"VariantName\": variant_name\n",
    "                },\n",
    "                \"yAxis\": {\"left\": {\"min\": 0}}\n",
    "            },\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"x\": 12,\n",
    "            \"y\": 0\n",
    "        },\n",
    "        \n",
    "        # Row 2: Resource Utilization\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"properties\": {\n",
    "                \"metrics\": [\n",
    "                    [\"/aws/sagemaker/Endpoints\", \"CPUUtilization\", {\"stat\": \"Average\"}]\n",
    "                ],\n",
    "                \"period\": 300,\n",
    "                \"stat\": \"Average\",\n",
    "                \"region\": region,\n",
    "                \"title\": \"CPU Utilization (%)\",\n",
    "                \"dimensions\": {\n",
    "                    \"EndpointName\": endpoint_name,\n",
    "                    \"VariantName\": variant_name\n",
    "                },\n",
    "                \"yAxis\": {\"left\": {\"min\": 0, \"max\": 100}}\n",
    "            },\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"x\": 0,\n",
    "            \"y\": 6\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"properties\": {\n",
    "                \"metrics\": [\n",
    "                    [\"/aws/sagemaker/Endpoints\", \"MemoryUtilization\", {\"stat\": \"Average\"}]\n",
    "                ],\n",
    "                \"period\": 300,\n",
    "                \"stat\": \"Average\",\n",
    "                \"region\": region,\n",
    "                \"title\": \"Memory Utilization (%)\",\n",
    "                \"dimensions\": {\n",
    "                    \"EndpointName\": endpoint_name,\n",
    "                    \"VariantName\": variant_name\n",
    "                },\n",
    "                \"yAxis\": {\"left\": {\"min\": 0, \"max\": 100}}\n",
    "            },\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"x\": 12,\n",
    "            \"y\": 6\n",
    "        },\n",
    "        \n",
    "        # Row 3: Disk Utilization\n",
    "        {\n",
    "            \"type\": \"metric\",\n",
    "            \"properties\": {\n",
    "                \"metrics\": [\n",
    "                    [\"/aws/sagemaker/Endpoints\", \"DiskUtilization\", {\"stat\": \"Average\"}]\n",
    "                ],\n",
    "                \"period\": 300,\n",
    "                \"stat\": \"Average\",\n",
    "                \"region\": region,\n",
    "                \"title\": \"Disk Utilization (%)\",\n",
    "                \"dimensions\": {\n",
    "                    \"EndpointName\": endpoint_name,\n",
    "                    \"VariantName\": variant_name\n",
    "                },\n",
    "                \"yAxis\": {\"left\": {\"min\": 0, \"max\": 100}}\n",
    "            },\n",
    "            \"width\": 12,\n",
    "            \"height\": 6,\n",
    "            \"x\": 0,\n",
    "            \"y\": 12\n",
    "        },\n",
    "        \n",
    "        # Row 4: Model Monitor Violations\n",
    "        {\n",
    "            \"type\": \"log\",\n",
    "            \"properties\": {\n",
    "                \"query\": f\"SOURCE '/aws/sagemaker/Endpoints/{endpoint_name}'\\n| fields @timestamp, @message\\n| filter @message like /violation/\\n| sort @timestamp desc\\n| limit 20\",\n",
    "                \"region\": region,\n",
    "                \"title\": \"Recent Monitoring Violations\",\n",
    "                \"stacked\": False\n",
    "            },\n",
    "            \"width\": 24,\n",
    "            \"height\": 6,\n",
    "            \"x\": 0,\n",
    "            \"y\": 18\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create dashboard\n",
    "try:\n",
    "    cw_client.put_dashboard(\n",
    "        DashboardName=dashboard_name,\n",
    "        DashboardBody=json.dumps(dashboard_body)\n",
    "    )\n",
    "    print(f\"✓ CloudWatch dashboard created: {dashboard_name}\")\n",
    "    print(f\"\\nView dashboard at:\")\n",
    "    print(f\"https://console.aws.amazon.com/cloudwatch/home?region={region}#dashboards:name={dashboard_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating dashboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Query Recent Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query recent endpoint metrics\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(hours=1)\n",
    "\n",
    "def get_metric_statistics(metric_name, namespace='AWS/SageMaker', statistic='Average'):\n",
    "    \"\"\"Helper function to get metric statistics\"\"\"\n",
    "    try:\n",
    "        response = cw_client.get_metric_statistics(\n",
    "            Namespace=namespace,\n",
    "            MetricName=metric_name,\n",
    "            Dimensions=[\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "                {'Name': 'VariantName', 'Value': variant_name}\n",
    "            ],\n",
    "            StartTime=start_time,\n",
    "            EndTime=end_time,\n",
    "            Period=300,\n",
    "            Statistics=[statistic]\n",
    "        )\n",
    "        \n",
    "        if response['Datapoints']:\n",
    "            latest = sorted(response['Datapoints'], key=lambda x: x['Timestamp'])[-1]\n",
    "            return latest[statistic]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"\\n=== Recent Endpoint Metrics (Last Hour) ===\")\n",
    "print(f\"\\nPerformance:\")\n",
    "latency = get_metric_statistics('ModelLatency')\n",
    "if latency:\n",
    "    print(f\"  Average Latency: {latency/1000:.2f}ms\")\n",
    "\n",
    "invocations = get_metric_statistics('Invocations', statistic='Sum')\n",
    "if invocations:\n",
    "    print(f\"  Total Invocations: {invocations:.0f}\")\n",
    "\n",
    "errors = get_metric_statistics('ModelInvocationErrors', statistic='Sum')\n",
    "if errors:\n",
    "    print(f\"  Errors: {errors:.0f}\")\n",
    "    if invocations and invocations > 0:\n",
    "        error_rate = (errors / invocations) * 100\n",
    "        print(f\"  Error Rate: {error_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\nResource Utilization:\")\n",
    "cpu = get_metric_statistics('CPUUtilization', namespace='/aws/sagemaker/Endpoints')\n",
    "if cpu:\n",
    "    print(f\"  CPU: {cpu:.2f}%\")\n",
    "\n",
    "memory = get_metric_statistics('MemoryUtilization', namespace='/aws/sagemaker/Endpoints')\n",
    "if memory:\n",
    "    print(f\"  Memory: {memory:.2f}%\")\n",
    "\n",
    "disk = get_metric_statistics('DiskUtilization', namespace='/aws/sagemaker/Endpoints')\n",
    "if disk:\n",
    "    print(f\"  Disk: {disk:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Monitoring Reports <a id='reports'></a>\n",
    "\n",
    "Generate comprehensive reports for model and data monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 List All Monitoring Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all monitoring schedules\n",
    "schedules = sm_client.list_monitoring_schedules(\n",
    "    EndpointName=endpoint_name,\n",
    "    MaxResults=100\n",
    ")\n",
    "\n",
    "print(\"\\n=== Active Monitoring Schedules ===\")\n",
    "for schedule in schedules['MonitoringScheduleSummaries']:\n",
    "    print(f\"\\nSchedule: {schedule['MonitoringScheduleName']}\")\n",
    "    print(f\"  Status: {schedule['MonitoringScheduleStatus']}\")\n",
    "    print(f\"  Type: {schedule.get('MonitoringType', 'DataQuality')}\")\n",
    "    print(f\"  Created: {schedule['CreationTime']}\")\n",
    "    print(f\"  Last Modified: {schedule['LastModifiedTime']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Check Latest Monitoring Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_execution(schedule_name):\n",
    "    \"\"\"Get the latest execution for a monitoring schedule\"\"\"\n",
    "    try:\n",
    "        executions = sm_client.list_monitoring_executions(\n",
    "            MonitoringScheduleName=schedule_name,\n",
    "            MaxResults=1,\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending'\n",
    "        )\n",
    "        \n",
    "        if executions['MonitoringExecutionSummaries']:\n",
    "            return executions['MonitoringExecutionSummaries'][0]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"\\n=== Latest Monitoring Executions ===\")\n",
    "for schedule in schedules['MonitoringScheduleSummaries']:\n",
    "    schedule_name = schedule['MonitoringScheduleName']\n",
    "    execution = get_latest_execution(schedule_name)\n",
    "    \n",
    "    print(f\"\\n{schedule_name}:\")\n",
    "    if execution:\n",
    "        print(f\"  Status: {execution['MonitoringExecutionStatus']}\")\n",
    "        print(f\"  Started: {execution.get('ScheduledTime', 'N/A')}\")\n",
    "        if 'ProcessingJobArn' in execution:\n",
    "            print(f\"  Processing Job: {execution['ProcessingJobArn'].split('/')[-1]}\")\n",
    "    else:\n",
    "        print(\"  No executions yet (schedule may not have run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Generate Model Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest model quality results if available\n",
    "try:\n",
    "    # List files in model quality results location\n",
    "    results_files = S3Downloader.list(model_quality_results_uri)\n",
    "    \n",
    "    if results_files:\n",
    "        print(f\"\\n=== Model Quality Monitoring Results ===\")\n",
    "        print(f\"Found {len(results_files)} result files\")\n",
    "        \n",
    "        # Download the most recent constraint violations report\n",
    "        for file in sorted(results_files, reverse=True):\n",
    "            if 'constraint_violations' in file:\n",
    "                local_file = S3Downloader.download(file, \".\")\n",
    "                with open(local_file[0], 'r') as f:\n",
    "                    violations = json.load(f)\n",
    "                \n",
    "                if violations.get('violations'):\n",
    "                    print(f\"\\n⚠️ Found {len(violations['violations'])} violations:\")\n",
    "                    for violation in violations['violations']:\n",
    "                        print(f\"  - {violation.get('description', 'Unknown violation')}\")\n",
    "                else:\n",
    "                    print(\"\\n✓ No violations detected\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"\\nNo monitoring results available yet.\")\n",
    "        print(\"Monitoring schedules need to run before results are generated.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCannot retrieve model quality results yet: {e}\")\n",
    "    print(\"This is expected if monitoring hasn't run yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Generate Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest data quality results if available\n",
    "try:\n",
    "    results_files = S3Downloader.list(data_quality_results_uri)\n",
    "    \n",
    "    if results_files:\n",
    "        print(f\"\\n=== Data Quality Monitoring Results ===\")\n",
    "        print(f\"Found {len(results_files)} result files\")\n",
    "        \n",
    "        # Download the most recent constraint violations report\n",
    "        for file in sorted(results_files, reverse=True):\n",
    "            if 'constraint_violations' in file:\n",
    "                local_file = S3Downloader.download(file, \".\")\n",
    "                with open(local_file[0], 'r') as f:\n",
    "                    violations = json.load(f)\n",
    "                \n",
    "                if violations.get('violations'):\n",
    "                    print(f\"\\n⚠️ Found {len(violations['violations'])} violations:\")\n",
    "                    for violation in violations['violations']:\n",
    "                        feature = violation.get('feature_name', 'Unknown')\n",
    "                        description = violation.get('description', 'Unknown violation')\n",
    "                        print(f\"  - Feature '{feature}': {description}\")\n",
    "                else:\n",
    "                    print(\"\\n✓ No violations detected\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"\\nNo monitoring results available yet.\")\n",
    "        print(\"Monitoring schedules need to run before results are generated.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCannot retrieve data quality results yet: {e}\")\n",
    "    print(\"This is expected if monitoring hasn't run yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Create Comprehensive Monitoring Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive monitoring report\n",
    "report = {\n",
    "    \"report_timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"endpoint_name\": endpoint_name,\n",
    "    \"monitoring_schedules\": [],\n",
    "    \"infrastructure_metrics\": {},\n",
    "    \"alarms\": [],\n",
    "    \"recent_violations\": []\n",
    "}\n",
    "\n",
    "# Add monitoring schedules\n",
    "for schedule in schedules['MonitoringScheduleSummaries']:\n",
    "    report[\"monitoring_schedules\"].append({\n",
    "        \"name\": schedule['MonitoringScheduleName'],\n",
    "        \"status\": schedule['MonitoringScheduleStatus'],\n",
    "        \"type\": schedule.get('MonitoringType', 'DataQuality')\n",
    "    })\n",
    "\n",
    "# Add infrastructure metrics\n",
    "report[\"infrastructure_metrics\"] = {\n",
    "    \"latency_ms\": latency/1000 if latency else None,\n",
    "    \"invocations\": invocations if invocations else 0,\n",
    "    \"errors\": errors if errors else 0,\n",
    "    \"cpu_utilization\": cpu if cpu else None,\n",
    "    \"memory_utilization\": memory if memory else None,\n",
    "    \"disk_utilization\": disk if disk else None\n",
    "}\n",
    "\n",
    "# Add alarms\n",
    "for alarm in alarms['MetricAlarms']:\n",
    "    report[\"alarms\"].append({\n",
    "        \"name\": alarm['AlarmName'],\n",
    "        \"state\": alarm['StateValue'],\n",
    "        \"metric\": alarm.get('MetricName', 'Expression'),\n",
    "        \"threshold\": alarm['Threshold']\n",
    "    })\n",
    "\n",
    "# Save report to S3\n",
    "report_filename = f\"monitoring_report_{datetime.now(timezone.utc):%Y%m%d_%H%M%S}.json\"\n",
    "with open(report_filename, 'w') as f:\n",
    "    json.dump(report, indent=2, fp=f)\n",
    "\n",
    "report_s3_uri = S3Uploader.upload(report_filename, reports_uri)\n",
    "\n",
    "print(f\"\\n✓ Monitoring report generated and saved to:\")\n",
    "print(f\"  {report_s3_uri}\")\n",
    "print(f\"\\n=== Report Summary ===\")\n",
    "print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing Monitoring with Sample Data <a id='testing'></a>\n",
    "\n",
    "Send test predictions to generate monitoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and adapt this to your model's input format\n",
    "\n",
    "# from sagemaker.predictor import Predictor\n",
    "# from sagemaker.serializers import CSVSerializer\n",
    "# from sagemaker.deserializers import JSONDeserializer\n",
    "# \n",
    "# # Create predictor\n",
    "# predictor = Predictor(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     sagemaker_session=sagemaker_session,\n",
    "#     serializer=CSVSerializer(),\n",
    "#     deserializer=JSONDeserializer()\n",
    "# )\n",
    "# \n",
    "# # Load test data\n",
    "# test_data = df_train_sample.drop(columns=['label'])  # UPDATE column name\n",
    "# \n",
    "# # Send test predictions\n",
    "# print(\"Sending test predictions...\")\n",
    "# for i, row in test_data.iterrows():\n",
    "#     prediction = predictor.predict(row.values.reshape(1, -1))\n",
    "#     print(f\"  Prediction {i+1}: {prediction}\")\n",
    "#     sleep(1)  # Small delay between requests\n",
    "# \n",
    "# print(\"\\n✓ Test predictions sent successfully\")\n",
    "# print(\"Data capture should now have recorded these predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup <a id='cleanup'></a>\n",
    "\n",
    "**Important:** Run these cells to delete resources and avoid charges.\n",
    "\n",
    "**⚠️ Warning:** This will delete all monitoring schedules, alarms, and the dashboard. Only run after you've completed your demonstration and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Delete Monitoring Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all monitoring schedules\n",
    "print(\"Deleting monitoring schedules...\")\n",
    "\n",
    "for schedule in schedules['MonitoringScheduleSummaries']:\n",
    "    schedule_name = schedule['MonitoringScheduleName']\n",
    "    try:\n",
    "        sm_client.delete_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "        print(f\"  ✓ Deleted: {schedule_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error deleting {schedule_name}: {e}\")\n",
    "\n",
    "print(\"\\nWaiting for deletions to complete...\")\n",
    "sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Delete CloudWatch Alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all CloudWatch alarms for this endpoint\n",
    "print(\"Deleting CloudWatch alarms...\")\n",
    "\n",
    "try:\n",
    "    alarm_names = [alarm['AlarmName'] for alarm in alarms['MetricAlarms']]\n",
    "    if alarm_names:\n",
    "        cw_client.delete_alarms(AlarmNames=alarm_names)\n",
    "        print(f\"  ✓ Deleted {len(alarm_names)} alarms\")\n",
    "    else:\n",
    "        print(\"  No alarms to delete\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error deleting alarms: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Delete CloudWatch Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete CloudWatch dashboard\n",
    "try:\n",
    "    cw_client.delete_dashboards(DashboardNames=[dashboard_name])\n",
    "    print(f\"✓ Deleted dashboard: {dashboard_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error deleting dashboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Optional: Delete S3 Monitoring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete S3 monitoring data\n",
    "# This will delete all captured data, baselines, and reports\n",
    "\n",
    "# print(\"Deleting S3 monitoring data...\")\n",
    "# \n",
    "# s3_resource = boto3.resource('s3')\n",
    "# bucket_resource = s3_resource.Bucket(bucket)\n",
    "# \n",
    "# # Delete monitoring prefix\n",
    "# bucket_resource.objects.filter(Prefix=f\"{prefix}/monitoring/\").delete()\n",
    "# \n",
    "# print(f\"✓ Deleted all S3 objects under s3://{bucket}/{prefix}/monitoring/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Keep Endpoint Running (Optional)\n",
    "\n",
    "**Note:** By default, we do NOT delete the endpoint as it may be needed for other project deliverables. \n",
    "If you want to delete the endpoint as well, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the endpoint\n",
    "# \n",
    "# print(\"Deleting endpoint...\")\n",
    "# predictor = Predictor(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     sagemaker_session=sagemaker_session\n",
    "# )\n",
    "# predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "# print(\"✓ Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook has implemented comprehensive monitoring for your ML system:\n",
    "\n",
    "✅ **Model Quality Monitoring** - Tracks prediction accuracy over time\n",
    "\n",
    "✅ **Data Quality Monitoring** - Detects data drift and quality issues\n",
    "\n",
    "✅ **Model Bias Monitoring** - Monitors for fairness issues (if applicable)\n",
    "\n",
    "✅ **Infrastructure Monitoring** - CloudWatch alarms for latency, errors, CPU, memory\n",
    "\n",
    "✅ **CloudWatch Dashboard** - Centralized visualization of all metrics\n",
    "\n",
    "✅ **Automated Reporting** - Comprehensive monitoring reports\n",
    "\n",
    "### Next Steps for Module 5:\n",
    "\n",
    "1. **Take Screenshots** - Capture dashboard, alarms, and monitoring schedules for your video demonstration\n",
    "\n",
    "2. **Update Design Document** - Document your monitoring approach in the ML Design Document\n",
    "\n",
    "3. **Wait for Executions** - Monitoring schedules run daily, so come back in 24-48 hours to see results\n",
    "\n",
    "4. **Generate Sample Violations** - (Optional) Intentionally send bad data to trigger violations for demonstration\n",
    "\n",
    "5. **Prepare for Module 6** - CI/CD will integrate with this monitoring infrastructure\n",
    "\n",
    "### Module 5 Deliverable Checklist:\n",
    "\n",
    "- [ ] Model monitors implemented\n",
    "- [ ] Data monitors implemented\n",
    "- [ ] Infrastructure monitors implemented\n",
    "- [ ] CloudWatch dashboard created\n",
    "- [ ] Model and data reports generated\n",
    "- [ ] Screenshots captured for documentation\n",
    "- [ ] Design document updated with monitoring details\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
